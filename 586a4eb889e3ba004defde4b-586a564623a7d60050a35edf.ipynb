{"metadata": {"signature": "sha256:7170f4f6aae1a2f3493e68716ed133efae7ccd2168ce4ca5c0cf34aef192cb29"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "A8FC178D0B034D1482B226F2466A9164", "metadata": {}, "source": "# TensorFlow \u7b14\u8bb02 \u53cc\u5411LSTM"}, {"cell_type": "markdown", "id": "4AA8F621E6E64ECD854A58969F8CE64F", "metadata": {}, "source": "### Basic LSTM"}, {"cell_type": "code", "collapsed": false, "id": "F806E78407B347CF8210C36807662951", "input": "%%time\nfrom __future__ import division\nfrom __future__ import print_function  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n%matplotlib inline\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\nfac = np.load('/home/big/Quotes/TensorFlow deal with Uqer/fac16.npy').astype(np.float32)\nret = np.load('/home/big/Quotes/TensorFlow deal with Uqer/ret16.npy').astype(np.float32)\n# \u6570\u636e\u683c\u5f0f \u65e5\u671f-\u591a\u56e0\u5b50 \u4f8b\u5982 \uff0809-01 Ab1 Ab2 Ab3 \uff09\uff0809-02 Ab1 Ab2 Ab3\uff09 ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "07F508DDE93D4871849EE651EEADB7FA", "input": "# Parameters\nlearning_rate = 0.001\nbatch_size = 1024\ntraining_iters = int(fac.shape[0]/batch_size)\ndisplay_step = 10\n\n# Network Parameters\nn_input = 17\nn_steps = 40\nn_hidden = 1024\nn_classes = 7\n\n# tf Graph input\nx = tf.placeholder('float',[None, n_steps, n_input])\ny = tf.placeholder('float',[None, n_classes])\n\n# Define weights\nweights = {\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\ndef BasicLSTM(x, weights, biases):\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1,n_input])\n    x = tf.split(0, n_steps, x)\n    # \u8fd9\u4e00\u6bb5\u4e0d\u7528\u6ce8\u610f\uff0c\u56e0\u4e3a\u4f7f\u7528CNN\u63d0\u53d6\u7684npy\u6570\u636e\u8fd9\u91cc\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u8f6c\u6362\u6210\u683c\u5f0f\u4e3a\n    # \u65e5\u671f-\u4e00\u6279\u6b21\u6570\u636e\uff08\u591a\u53ea\u80a1\u7968\uff09-\u591a\u56e0\u5b50\u6570\u636e\uff0c\u76f8\u5f53\u4e8e\u5c06\u591a\u53ea\u80a1\u7968\u7684\u591a\u56e0\u5b50\u6570\u636e\u4ee5\u65f6\u95f4\u5e8f\u5217\u4e00\u5929\u4e00\u5929\u5582\u7ed9RNN\u6a21\u578b\n    \n    Basicl_LSTM_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n    outputs, states = tf.nn.rnn(Basicl_LSTM_cell, x, dtype=tf.float32)\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\npred = BasicLSTM(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\ninit = tf.global_variables_initializer()   ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "7BA79E517A8D48B6A4C403F5C2858B98", "input": "# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    for step in range(1):\n        for i in range(int(len(fac)/batch_size)):\n            batch_x = fac[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_steps,n_input])\n            batch_y = ret[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_classes])\n            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})           \n            if i % display_step ==0:\n                print(i,'----',(int(len(fac)/batch_size)))\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,y: batch_y})\n        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.5f}\".format(acc))\n    print(\"Optimization Finished!\")   \n    # Calculate accuracy for 128 mnist test images\n    test_len = 1280\n    test_data = fac[:test_len].reshape([batch_size,n_steps,n_input])\n    test_label = ret[:test_len].reshape([batch_size,n_classes])\n\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    \n    sess.close()    ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "89D2EC1A07F24CC2B049362C0ACA0BB4", "metadata": {}, "source": "### \u53cc\u5411LSTM"}, {"cell_type": "code", "collapsed": false, "id": "9C7CA639D6614A1A9740B8289C42AD89", "input": "%%time\nfrom __future__ import division\nfrom __future__ import print_function  \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfac = np.load('/home/big/Quotes/TensorFlow deal with Uqer/fac16.npy').astype(np.float32)\nret = np.load('/home/big/Quotes/TensorFlow deal with Uqer/ret16.npy').astype(np.float32)\n# \u6570\u636e\u683c\u5f0f \u65e5\u671f-\u591a\u56e0\u5b50 \u4f8b\u5982 \uff0809-01 Ab1 Ab2 Ab3 \uff09\uff0809-02 Ab1 Ab2 Ab3\uff09 ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "059172B9591F4F7F82E00A709AD8F13C", "input": "# Parameters\nlearning_rate = 0.001\ntraining_iters = 100000\nbatch_size = 1280\ndisplay_step = 10\n\n# Network Parameters\nn_input = 17 # MNIST data input (img shape: 28*28)\nn_steps = 40 # timesteps\nn_hidden = 128 # hidden layer num of features\nn_classes = 7 # MNIST total classes (0-9 digits)\n\n\n# tf Graph input\nx = tf.placeholder(\"float\", [None, n_steps, n_input])\ny = tf.placeholder(\"float\", [None, n_classes])\n\n# Define weights\nweights = {\n    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\ndef BiRNN(x, weights, biases):\n\n    # Prepare data shape to match `bidirectional_rnn` function requirements\n    # Current data input shape: (batch_size, n_steps, n_input)\n    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n\n    # Permuting batch_size and n_steps\n    x = tf.transpose(x, [1, 0, 2])\n    # Reshape to (n_steps*batch_size, n_input)\n    x = tf.reshape(x, [-1, n_input])\n    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n    x = tf.split(0, n_steps, x)\n\n    # Define lstm cells with tensorflow\n    # Forward direction cell\n    lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n    # Backward direction cell\n    lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n\n    # Get lstm cell output\n    try:\n        outputs, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                              dtype=tf.float32)\n    except Exception: # Old TensorFlow version only returns outputs not states\n        outputs = tf.nn.bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                        dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\npred = BiRNN(x, weights, biases)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    for step in range(100):\n        for i in range(int(len(fac)/batch_size)):\n            batch_x = fac[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_steps,n_input])\n            batch_y = ret[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_classes])\n            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})           \n            if i % display_step ==0:\n                print(i,'----',(int(len(fac)/batch_size)))\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,y: batch_y})\n        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.5f}\".format(acc))\n    print(\"Optimization Finished!\")   \n    # Calculate accuracy for 128 mnist test images\n    test_len = 1280\n    test_data = fac[:test_len].reshape([batch_size,n_steps,n_input])\n    test_label = ret[:test_len].reshape([batch_size,n_classes])\n\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    \n    sess.close()    ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}