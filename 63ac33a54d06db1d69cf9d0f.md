# 基于PyTorch框架LSTM深度学习股票预测系统

传统的统计预测方法包括ARIMA (移动平均自回归) [2] 模型、GARCH (广义自回归条件异方差) [3] 模型、GM (灰色系统理论) [4] 模型等模型，该类模型更多地适用于平稳且线性的时间序列数据，因此不适用于股票价格预测。机器学习的方法包括决策树 [5]、支持向量机 [6]、逻辑回归 [7] 等，由于机器学习算法的自学能力非常强、对噪声数据的鲁棒性和容错性较强，适合处理非线性数据，因此该方法目前已广泛应用于图像识别、语音识别、情绪分类等领域，但是其在金融行业特别是股票预测方面的应用还不够深入，而深度学习作为机器学习的一个新的研究方向，更接近于人们对于人工智能的要求，被广泛应用于金融和经济领域，尤其在股票数据的预测中表现优异。陈可心和黄刚 [8] 提出由BiLSTM和CLSTM混合构建的CStock模型，该模型结合新闻和股价走势进行预测，不但利用了股票市场中的交易数据，同时考虑到财经以及政治新闻对于股票市场的影响。实验结果表明，CStock模型在一定程度上能够准确有效地对股票走势进行预测。

韩山杰和谈世哲 [9] 研究发现基于TensorFlow框架LSTM循环神经网络模型比BP神经网络模型在对苹果公司股价的预测上准确性更高，耗时更短，更高效。本文提出了基于PyTorch框架LSTM循环神经网络模型，不单单针对某支股票价格进行预测，而是选取创业300指数从开盘以来的交易数据，即2012年7月2日到2020年11月6日共计2032个日交易数据进行拟合预测。由于数据时间跨度比较大，运用传统的RNN模型可能出现梯度爆炸和梯度消失的现象，从而对网络的稳定性造成极大的影响，使得预测误差较大。而LSTM循环神经网络模型可以很好地解决上述问题，并且利用时间序列向前和向后两个时间方向上的上下文关系，使预测准确度得到较大地提升。同时，该模型引入了Dropout策略，在一定程度上解决了深层网络模型带来的训练难、收敛速度慢和过拟合等问题，对实际问题的预测具有更高的准确性，因此基于LSTM深度学习方法对股票价格预测有较强的现实意义。

2. 模型介绍

2.1. LSTM长短期循环神经网络

长短时记忆模型(Long Short-Term Memory, LSTM)是循环神经网络的变体。尽管在理论上，RNN可以处理任何长距离依赖问题，但实际上，其对于梯度消失、爆炸等问题很难实现，对此，LSTM通过引入门机制和记忆单元提供了解决方案，这里增加了遗忘门、更新门和输出门三个门机制。LSTM单元结构如图1所示。



Figure 1. LSTM neural network unit structure diagram

图1. LSTM神经网络单元结构图

从图1可以看到，LSTM模型的1个神经元包含了1个细胞状态(cell)和3个门(gate)机制。细胞状态(cell)是LSTM模型的关键所在，类似于存储器，是模型的记忆空间。细胞状态随着时间而变化，记录的信息由门机制决定和更新。门机制是让信息选择式通过的方法，通过sigmoid函数和点乘操作实现。sigmoid取值介于0~1之间，乘即点乘则决定了传送的信息量(每个部分有多少量可以通过)，当sigmoid取0时表示舍弃信息，取1时表示完全传输(即完全记住)。

LSTM拥有三个门，来保护和控制细胞状态：遗忘门(forget gate)、更新门(update gate)和输出门(output gate)。

LSTM中保存的历史信息由遗忘门、更新门和输出门控制，在t时刻，各门状态的数学表达式为：

c˜(t)=tanh(Wc[a(t−1),x(t)]+bc)(1)

Γf(t)=δ(Wf[a(t−1),x(t)]+bf)(2)

Γu(t)=δ(Wu[a(t−1),x(t)]+bu)(3)

Γo(t)=δ(Wo[a(t−1),x(t)]+bo)(4)

c(t)=Γf∗c(t−1)+Γu∗c˜(t)(5)

a(t)=Γo∗tanh(c(t))(6)

其中， x(t) 为t时刻的输入数据， a(t) 是t时刻LSTM单元的输出状态值， c˜(t) 是t时刻记忆单元的候选值， Γf(t) 是遗忘门t时刻的状态值， Γu(t) 是更新门t时刻的状态值， Γo(t) 是输出门t时刻的状态值。W为相应的权值，b是对应的偏置参数。记忆单元的状态值由更新门和遗忘门共同调节。

2.2. Dropout介绍

Dropout的数学形式表达式为：

y=f(W∗d(x))(7)

d(x)={mask∗x,训练阶段(1−p)x,其他(8)

其中，p为Dropout率，mask为以1 − p为概率的贝努力分布的二值向量。

从式(8)可以看出，Dropout与L1和L2范式正则化不同，Dropout并不会修改代价函数，而是修改深度网络本身。Dropout随机“删除”网络中的一些隐藏神经元，保持输入输出神经元不变。这样，对于一个网络而言，Dropout便是用相同的数据训练了多个不同的神经网络，产生了多个不同程度的拟合状态。但这些网络共用一个损失函数，相当于对神经网络本身进行了优化，求取了所有状态的平均值；同时，减少了神经单元之间的相互协同关系，增加了网络的鲁棒性。

2.3. 框架介绍

在开始深度学习项目之前，选择一个合适的框架非常重要，因为选择一个合适的框架能起到事半功倍的作用。众所周知，股票市场是一个受外界影响大，具有极强的不稳定性与随机性的系统，为投资者带来收益的同时，也可能会为投资者带来极大的亏损。因此，为了尽可能地扩大收益规避风险，对股票走势的准确预测变得尤为重要。

目前流行的深度学习主流框架有PyTorch和TensorFlow两大框架，针对本文创业300指数数据集，通过十折交叉验证结果发现，基于PyTorch框架的预测误差均小于TensorFlow框架，即基于PyTorch框架的预测准确性更高。与此同时，TensorFlow框架更多运用于工业研究，语言系统设计复杂，在迭代次数较大时，计算时间较长；而PyTorch框架更多运用于学术界，应用十分灵活，接口沿用Torch，契合用户思维，设计上更直观，追求尽量少的封装，建模过程更透明，便于多次调参和调整迭代次数进行最优模型的选择。综上所述，本文选择PyTorch框架构建LSTM循环神经网络模型对创业300指数收盘价进行预测。

2.4. 性能指标

为了衡量网络的稳定性与预测的准确性，本文根据股票的预测价格与实际价格，选取平均绝对百分比误差MAPE作为评估指标，计算公式如式(9)所示：

MAPE=1N∑i=1N∣∣∣yˆi−yiyi∣∣∣
其中， yˆi 为股票的预测价格， yi 为股票的实际价格，N为训练的天数。

3. 实证分析

3.1. 数据的介绍

选取创业300指数从2012年7月2号至2020年11月6号的日交易数据，特征变量为开盘价X1、最高价X2、最低价X3、成交量X4、总金额X5和涨跌幅X6，输出变量或预测变量为收盘价Y。

3.2. 数据预处理

因为数据的不同输入变量数值上差异明显，并且会对模型训练造成严重的影响，因此首先对数据进行归一化处理，消除量纲对模型造成的影响。

3.3. 算法实现

3.3.1. 模型建立

本文提出的LSTM循环神经网络模型的结构如图2所示，该模型由输入层、LSTM隐藏层、激活层和输出层四部分构成。



Figure 2. LSTM prediction model

图2. LSTM预测模型

3.3.2. 算法步骤

1) 数据预处理，将数据 X={X11,X12,⋯,X1n,X21,⋯,X2n,⋯,X6n} 归一化处理；

2) 初始化神经单元的细胞状态 c(0) 和 a(0)，将预处理后的数据输入第一层LSTM神经元；

3) 按照式(1)~(4)，计算出当前神经元记忆候选值 c˜(t)，更新门状态 Γu(t)，遗忘门状态 Γf(t) 和输出门状态 Γo(t) ；

4) 根据式(5)计算当前神经元的记忆状态值 c(t) ；

5) 根据式(6)计算当前神经元的输出值 a(t) ；

6) 保留 c(t) 、 a(t) 并将其运用到下一个时刻的LSTM神经元的计算中；

7) 重复3~6步骤，直到向前层、向后层的LSTM神经元均学习完全部的时间序列；

8) 重复以上步骤，直到最后一层LSTM输出非线性数据特征 yˆi。

3.4. 结果分析

3.4.1. 不同迭代次数的预测误差分析

Data1我们选择开盘价、最高价、最低价、成交量、成交额这五个变量作为输入变量，Data2加入涨跌幅变量即选择开盘价、最高价、最低价、成交量、成交额和涨跌幅这六个变量作为输入变量，以收盘价为输出变量建立LSTM模型进行预测。改变迭代次数观察不同输入变量个数下的平均绝对百分比误差。设置遗忘门偏置为1，LSTM单元数为2，表1为输出预测误差结果对比。

迭代次数

Data1 (输入维度 = 5， 遗忘门偏置= 1.0， LSTM单元数 = 2)

Data2 (输入维度 = 6， 遗忘门偏置 = 1.0， LSTM单元数 = 2)

10次

0.0472

0.0439

50次

0.0360

0.0322

100次

0.0303

0.0214

200次

0.0210

0.0189



Table 1. Comparison table of prediction errors of different iterations

表1. 不同迭代次数的预测误差对比表

由表1的结果可知，输入的维数越多，预测平均绝对百分比误差越小；证明涨跌幅这个变量的输入可以有效地降低预测误差，即引入涨跌幅变量具有实际意义。随着迭代次数的增加，预测误差在逐渐降低，而且从误差数据中可以看出在迭代100次时，网络已经比较稳定，但在迭代200次时，预测准确率达到最优。

3.4.2. 不同遗忘门偏置的预测误差分析

基于Data2的数据，设置迭代次数为200，LSTM单元数的值为2，设置遗忘门偏置的值依次为1、0.7、0.4进行实验，预测误差结果如表2所示。

迭代次数

遗忘门偏置 = 1，

LSTM单元数 = 2

遗忘门偏置 = 0.7，

LSTM单元数 = 2

遗忘门偏置 = 0.4，

LSTM单元数 = 2

200次

0.0189

0.0223

0.0174



Table 2. Prediction error table with different forget bias

表2. 不同遗忘门偏置的预测误差表

由表2的前两列可以看出，Data2的遗忘门偏置适当减小，即忘记部分信息，网络训练效果却有所下滑；从后两列可以看出，在Data2的遗忘门偏置变得更小，甚至忘记超过一半的信息时，网络训练效果却有所提高，并且高于完全保留信息时的网络，因此需进一步研究确定最优参数。

3.4.3. 不同LSTM单元数的预测误差分析

基于Data2的数据，设置迭代次数为200，在遗忘门偏置为1和0.4的条件下，依次设置LSTM的单元数为2、7、14建模进行预测，所得到的预测误差结果如表3所示。

迭代

次数

遗忘门偏置 = 1

遗忘门偏置 = 0.4

LSTM单元数 = 2

LSTM单元数 = 7

LSTM单元数 = 14

LSTM单元数 = 2

LSTM单元数 = 7

LSTM单元数 = 14

200次

0.0186

0.0239

0.0482

0.0109

0.0159

0.0385



Table 3. Prediction error table for different LSTM cell numbers

表3. 不同LSTM单元数的预测误差表

由表3的结果可知，在LSTM单元数增加的情况下，网络训练模型反而降低了，可以看出，其股票行情在7天内的关联程度比14天内的关联程度高，在2天内的关联程度比7天内的关联程度要高。在相同的单元数的情况下，遗忘门偏置应选择比较小的数，预测效果会比较好；在LSTM单元数较大的情况下，应该选择较小的遗忘门偏置，以免记忆太多的无效信息。

综上所述，得到创业300指数收盘价预测准确率最高的模型为输入变量为6维，迭代次数为200，遗忘门偏置为0.4，LSTM单元数为2的基于PyTorch框架LSTM循环神经网络模型。

3.4.4. 可视化结果

选取Data2，即选择开盘价、最高价、最低价、成交量、成交额和涨跌幅这六个变量作为输入变量，迭代次数为200次，分别输出LSTM模型一(遗忘门偏置 = 1、LSTM单元数 = 2)、LSTM模型二(遗忘门偏置 = 0.7、LSTM单元数 = 2)、LSTM模型三(遗忘门偏置 = 0.4、LSTM单元数 = 2)这三个模型的Loss值随迭代次数变化的折线图见图3~5与模型预测结果的可视化图见图6~8。



Figure 3. Loss function diagram (LSTM model 1)

图3. Loss函数图(LSTM模型一)



Figure 4. Loss function diagram (LSTM model 2)

图4. Loss函数图(LSTM模型二)



Figure 5. Loss function diagram (LSTM model 3)

图5. Loss函数图(LSTM模型三)



Figure 6. LSTM prediction of model 1

图6. 预测拟合图(LSTM模型一)



Figure 7. LSTM prediction of model 2

图7. 预测拟合图(LSTM模型二)



Figure 8. LSTM prediction of model 3

图8. 预测拟合图(LSTM模型三)

从以上可视化的结果可以看出，遗忘门偏置 = 1、LSTM单元数 = 2时，随着迭代次数的增加，损失值直线下降，但是在迭代次数大于100之后，损失值上下震荡明显，说明网络不稳定，从预测拟合图中可以看出，模型前期拟合效果较差，中期与后期相对较好，说明网络训练不稳定；遗忘门偏置 = 0.7、LSTM单元数 = 2时，Loss可视化的结果可以看出，25到100次迭代区间，损失值忽高忽低，100次之后，损失值上下震荡明显，说明网络极不稳定，从预测拟合图中可以看出，模型前期和后期的拟合效果较差，误差较大；遗忘门偏置 = 0.4、LSTM单元数 = 2时，随着迭代次数的增加，网络损失值逐渐收敛，在迭代100次之后，损失值趋于最小值，说明网络基本稳定，在迭代200次时，损失值达到最小，从预测拟合图中可以看出，模型前中期走势的拟合效果都比较好，只有后期拟合稍有误差，整体拟合效果优于前两个模型。因此，可视化结果与上文预测平均绝对百分比误差输出结果吻合，更进一步证明输入变量为6维，迭代次数为200，遗忘门偏置为0.4，LSTM单元数为2的基于PyTorch框架LSTM循环神经网络模型的预测准确率最高，拟合效果最好。

4. 结论

PyTorch框架具有设计直观，简易封装，集成度高等特点，使其能够灵活地调节参数，提高建模分析的效率。使用PyTorch框架设计并实现了用于股票预测的深度学习流程以及算法框架，得到了有较高准确率的预测模型。

通过实验验证了PyTorch框架构建LSTM循环神经网络模型的优势，分析了迭代次数、遗忘门偏置值以及LSTM神经元个数对网络模型的影响。首先，通过设置不同的迭代次数可知，迭代次数在100次后，网络输出误差有明显的降低，说明该网络是一个较轻量级的网络；为了预测的准确率，选择迭代200次进行后面的建模分析。其次，通过设置不同的遗忘门偏置可知，偏置值为1和0.4时预测效果优于偏置值为0.7时的模型。因此，在遗忘门偏置值为1和0.4时，分别设置不同的LSTM神经元个数建模可知，遗忘门偏置为0.4，LSTM单元数为2的基于PyTorch框架LSTM循环神经网络模型的预测误差最小，平均绝对百分比误差达到0.0109；最后，该结果通过Loss函数图与预测拟合图得到了进一步证明。为进一步使用PyTorch框架构建循环神经网络准确预测股价提供了依据，具有广阔的应用前景。