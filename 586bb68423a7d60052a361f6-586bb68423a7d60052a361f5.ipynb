{"metadata": {"signature": "sha256:452b44953d96b03be7d2208a9ff520246dfd32d3e3a19298fc3f52c8f8f96c28"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "486B046BAF2C4889A25B2C8297997D4D", "metadata": {}, "source": "# TensorFlow \u7b14\u8bb03 \u591a\u5c42LSTM"}, {"cell_type": "code", "collapsed": false, "id": "62CC247F10074997864DE64649A76DA9", "input": "%%time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n%matplotlib inline\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\nfac = np.load('F:/Quotes/fac16.npy').astype(np.float32)\nret = np.load('F:/Quotes/ret16.npy').astype(np.float32)\n# \u6570\u636e\u683c\u5f0f \u65e5\u671f-\u591a\u56e0\u5b50 \u4f8b\u5982 \uff0809-01 Ab1 Ab2 Ab3 \uff09\uff0809-02 Ab1 Ab2 Ab3\uff09 ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "9B3D22153E984EEF82435543022FE721", "metadata": {}, "source": "### MultiRNNCell\u200b \u591a\u5c42LSTM"}, {"cell_type": "code", "collapsed": false, "id": "6120413F06E84626B7ADC74AF5435FFA", "input": "# Parameters\nlearning_rate = 0.001\nbatch_size = 1024\ntraining_iters = int(fac.shape[0]/batch_size)\ndisplay_step = 10\n\n# Network Parameters\nn_input = 17\nn_steps = 40\nn_hidden = 1024\nn_classes = 7\n\n# tf Graph input\nx = tf.placeholder('float',[None, n_steps, n_input])\ny = tf.placeholder('float',[None, n_classes])\n\n# Define weights\nweights = {\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\ndef MultiLSTM(x, weights, biases):\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1,n_input])\n    x = tf.split(0, n_steps, x)\n    \n    Basicl_LSTM_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n    LSTM_cell = tf.nn.rnn_cell.DropoutWrapper(Basicl_LSTM_cell, output_keep_prob= 0.8)\n    Layers = tf.nn.rnn_cell.MultiRNNCell([LSTM_cell]*3, state_is_tuple= True)\n    \n    outputs, states = tf.nn.rnn(Layers, x, dtype=tf.float32)\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\npred = MultiLSTM(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\ninit = tf.global_variables_initializer()   ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "4CC8FBA941304DE6800A1A78330DFC21", "input": "# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n    for step in range(1):\n        for i in range(int(len(fac)/batch_size)):\n            batch_x = fac[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_steps,n_input])\n            batch_y = ret[i*batch_size:(i+1)*batch_size].reshape([batch_size,n_classes])\n            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})           \n            if i % display_step ==0:\n                print(i,'----',(int(len(fac)/batch_size)))\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,y: batch_y})\n        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.5f}\".format(acc))\n    print(\"Optimization Finished!\")   \n    # Calculate accuracy for 128 mnist test images\n    test_len = 1280\n    test_data = fac[:test_len].reshape([batch_size,n_steps,n_input])\n    test_label = ret[:test_len].reshape([batch_size,n_classes])\n\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))\n    \n    sess.close() ", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}