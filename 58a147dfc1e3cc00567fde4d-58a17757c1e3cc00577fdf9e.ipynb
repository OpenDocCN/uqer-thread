{"metadata": {"signature": "sha256:1d19c40182a56b372f8cacfb750a41bd645935bcf5166e3996f1cbdc4106379e"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "19264DE8EF6F4ECAA3E35A5242F1CE15", "metadata": {}, "source": "# tensorflow \u7b14\u8bb07 \u52a0\u901fSVM\u5206\u7c7b\u5668"}, {"cell_type": "markdown", "id": "66B1137C41D243448302454A795C6F13", "metadata": {}, "source": "\u6211\u5728\u5904\u740630W\u884c\u6570\u636e\u505a\u5206\u7c7b\u7684\u65f6\u5019\u53d1\u73b0sklearn\u5bf9\u591a\u6838\u5de5\u4f5c\u7ad9\u652f\u6301\u6548\u679c\u4e0d\u662f\u592a\u597d\uff0c\u6211\u4f7f\u7528\u4e00\u4e2a20\u6838E5\u5de5\u4f5c\u7ad9\u5c45\u7136\u8fd8\u6ca1\u6709\u6211\u7b14\u8bb0\u672c\u901f\u5ea6\u5feb\uff0c\u770b\u4e86\u4e00\u4e0b\u53d1\u73b0sklearn\u6ca1\u6709\u5145\u5206\u4f7f\u752864G\u5185\u5b58\u548cCPU\u7684\u591a\u6838\u3002\n\u8fd9\u91cc\u7f16\u5199\u4e86\u4e00\u4e2asklearn\u7684SVM\u7c7b\uff0c\u53ef\u4ee5\u901a\u8fc7\u5360\u636e\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u5bf9SVM\u52a0\u901f\u8fdb\u884c\u3002"}, {"cell_type": "markdown", "id": "9B2A54BDBA19475E8D64D813B725D3F5", "metadata": {}, "source": "### \u7528\u6cd5\n\u8bad\u7ec3 fit(trainX, trainY)\n\u9884\u6d4b pred(testX)\n\u5b66\u4e60\u6b65\u957f learning_rate = 0.001 \n\u8bad\u7ec3\u5468\u671f training_epoch = None,\n\u7ec8\u6b62\u8bef\u5dee error = 0.001,\nif training_epoch is not None then the error will not effect\n\u663e\u793a\u6b65\u957f display_step = 5"}, {"cell_type": "markdown", "id": "161C9B99045F4B528342C811B2579F5B", "metadata": {}, "source": "\u9ad8\u65af\u6838\u5b9a\u4e49\uff1agamma \n$$K(\\textbf{x})=exp\\left( -\\gamma \\textbf{x} \\cdot \\textbf{x}^{T} \\right)$$"}, {"cell_type": "markdown", "id": "8B11B8256AEF49158BFE74427FEE0182", "metadata": {}, "source": "### MultiSVC"}, {"cell_type": "code", "collapsed": false, "folded": false, "id": "1A8EA43B797C4DEF855D1F98E4E96A12", "input": "import tensorflow as tf\nimport functools\n\ndef lazy_property(function):\n    attribute = '_' + function.__name__\n\n    @property\n    @functools.wraps(function)\n    def wrapper(self):\n        if not hasattr(self, attribute):\n            setattr(self, attribute, function(self))\n        return getattr(self, attribute)\n    return wrapper\n\nclass MultiSVC(object):\n    \n    def __init__(self,\n                learning_rate = 0.001,\n                training_epoch = None,\n                error = 0.001,\n                display_step = 5):\n        self.learning_rate = learning_rate\n        self.training_epoch = training_epoch\n        self.display_step = display_step\n        self.error = error\n        \n    def __Preprocessing(self, trainX, trainY):\n        self.row = row = trainX.shape[0]\n        col = trainX.shape[1]\n        self.ycol = ycol = trainY.shape[1]\n        self.X = tf.placeholder(shape=[row, col], dtype= tf.float32)\n        self.Y = tf.placeholder(shape=[row, ycol], dtype= tf.float32)\n        self.test = tf.placeholder(shape=[None, col], dtype= tf.float32)\n        self.beta = tf.Variable(tf.truncated_normal(shape=[ycol, row], stddev=.1))\n    \n    def __dense_to_one_hot(self,labels_dense):\n        \"\"\"\u6807\u7b7e \u8f6c\u6362one hot \u7f16\u7801\n        \u8f93\u5165labels_dense \u5fc5\u987b\u4e3a\u975e\u8d1f\u6570\n        2016-11-21\n        \"\"\"\n        num_classes = len(np.unique(labels_dense)) # np.unique \u53bb\u6389\u91cd\u590d\u51fd\u6570\n        raws_labels = labels_dense.shape[0]\n        index_offset = np.arange(raws_labels) * num_classes\n        labels_one_hot = np.zeros((raws_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n        return labels_one_hot      \n    \n    @lazy_property \n    def Kernel_Train(self):\n        tmp_abs = tf.reshape(tensor=tf.reduce_sum(tf.square(self.X), axis=1), shape=[-1,1])\n        tmp_ = tf.add(tf.sub(tmp_abs, tf.mul(2., tf.matmul(self.X, tf.transpose(self.X)))), tf.transpose(tmp_abs))\n        return tf.exp(tf.mul(self.gamma, tf.abs(tmp_)))\n    \n    @lazy_property  \n    def Cost(self):\n        left = tf.reduce_sum(self.beta)\n        beta_square = tf.matmul(self.beta, self.beta, transpose_a=True)\n        tmp = tf.expand_dims(self.Y, axis= 0)\n        Y_square = tf.matmul(tf.transpose(tmp, perm=[2,1,0]), tf.transpose(tmp, perm=[2,0,1]))\n        right = tf.reduce_sum(tf.mul(self.Kernel_Train, tf.mul(beta_square, Y_square)), axis=[1,2])\n        return tf.reduce_sum(tf.neg(tf.sub(left, right)))\n    \n    @lazy_property  \n    def Kernel_Prediction(self):        \n        tmpA = tf.reshape(tf.reduce_sum(tf.square(self.X), 1),[-1,1])\n        tmpB = tf.reshape(tf.reduce_sum(tf.square(self.test), 1),[-1,1])\n        tmp = tf.add(tf.sub(tmpA, tf.mul(2.,tf.matmul(self.X, self.test, transpose_b=True))), tf.transpose(tmpB))\n        return tf.exp(tf.mul(self.gamma, tf.abs(tmp)))\n    \n    @lazy_property \n    def Prediction(self):\n        kernel_out = tf.matmul(tf.mul(tf.transpose(self.Y),self.beta), self.Kernel_Prediction)\n        return tf.arg_max(kernel_out - tf.expand_dims(tf.reduce_mean(kernel_out,1),1),0)\n    \n    @lazy_property \n    def Accuracy(self):\n        return tf.reduce_mean(tf.cast(tf.equal(self.Prediction, tf.argmax(self.Y,1)), tf.float32))\n    \n    def fit(self, trainX, trainY, gamma= 50.):\n        trainY = self.__dense_to_one_hot(trainY)\n        trainY = np.where(trainY, 1, -1)\n        self.sess = tf.InteractiveSession()\n        self.__Preprocessing(trainX, trainY)\n        self.gamma = tf.constant(value= -gamma, dtype=tf.float32)\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Cost)\n        self.sess.run(tf.global_variables_initializer())\n        \n        if self.training_epoch is not None:        \n            for ep in range(self.training_epoch):\n                self.sess.run(self.optimizer, feed_dict={self.X:trainX, self.Y:trainY})\n                if ep % self.display_step== 0:\n                    loss, acc = self.sess.run([self.Cost, self.Accuracy], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)\n        elif self.training_epoch is None:\n            acc = 0.1\n            ep = 0\n            while (acc< 1.- self.error):\n                acc,_ = self.sess.run([self.Accuracy, self.optimizer], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                ep += 1\n                if ep % self.display_step== 0: \n                    loss = self.sess.run(self.Cost, feed_dict={self.X:trainX, self.Y:trainY})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)    \n        print(\"Optimization Finished!\")      \n        self.trainX = trainX\n        self.trainY = trainY\n    \n    def pred(self,test):\n        output = self.sess.run(self.Prediction, feed_dict={self.X:self.trainX, self.Y:self.trainY, self.test:test})\n        return output", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "5865DFD8105B4293857E404073DD8894", "metadata": {}, "source": "### Gaussian RBF"}, {"cell_type": "code", "collapsed": false, "folded": false, "id": "5570A55FDDAE48F5816B01FD9BD0709F", "input": "import tensorflow as tf\nimport functools\n\ndef lazy_property(function):\n    attribute = '_' + function.__name__\n\n    @property\n    @functools.wraps(function)\n    def wrapper(self):\n        if not hasattr(self, attribute):\n            setattr(self, attribute, function(self))\n        return getattr(self, attribute)\n    return wrapper\n\nclass NonlinearSVC(object):\n    \n    def __init__(self,\n                learning_rate = 0.001,\n                training_epoch = None,\n                error = 0.001,\n                display_step = 5):\n        self.learning_rate = learning_rate\n        self.training_epoch = training_epoch\n        self.display_step = display_step\n        self.error = error\n        \n    def __Preprocessing(self, trainX):\n        row = trainX.shape[0]\n        col = trainX.shape[1]\n        self.X = tf.placeholder(shape=[row, col], dtype= tf.float32)\n        self.Y = tf.placeholder(shape=[row, 1], dtype= tf.float32)\n        self.test = tf.placeholder(shape=[None, col], dtype= tf.float32)\n        self.beta = tf.Variable(tf.truncated_normal(shape=[1, row], stddev=.1))\n        \n    @lazy_property \n    def Kernel_Train(self):\n        tmp_abs = tf.reshape(tensor=tf.reduce_sum(tf.square(self.X), axis=1), shape=[-1,1])\n        tmp_ = tf.add(tf.sub(tmp_abs, tf.mul(2., tf.matmul(self.X, tf.transpose(self.X)))), tf.transpose(tmp_abs))\n        return tf.exp(tf.mul(self.gamma, tf.abs(tmp_)))\n    \n    @lazy_property  \n    def Kernel_Prediction(self):        \n        tmpA = tf.reshape(tf.reduce_sum(tf.square(self.X), 1),[-1,1])\n        tmpB = tf.reshape(tf.reduce_sum(tf.square(self.test), 1),[-1,1])\n        tmp = tf.add(tf.sub(tmpA, tf.mul(2.,tf.matmul(self.X, self.test, transpose_b=True))), tf.transpose(tmpB))\n        return tf.exp(tf.mul(self.gamma, tf.abs(tmp)))\n    \n    @lazy_property  \n    def Cost(self):\n        left = tf.reduce_sum(self.beta)\n        beta_square = tf.matmul(self.beta, self.beta, transpose_a=True)\n        Y_square = tf.matmul(self.Y, self.Y, transpose_b= True)\n        right = tf.reduce_sum(tf.mul(self.Kernel_Train, tf.mul(beta_square, Y_square)))\n        return tf.neg(tf.sub(left, right))\n    \n    @lazy_property \n    def Prediction(self):\n        kernel_out = tf.matmul(tf.mul(tf.transpose(self.Y),self.beta), self.Kernel_Prediction)\n        return tf.sign(kernel_out - tf.reduce_mean(kernel_out))\n    \n    @lazy_property \n    def Accuracy(self):\n        return tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(self.Prediction), tf.squeeze(self.Y)), tf.float32))\n    \n    def fit(self, trainX, trainY, gamma= 50.):\n        self.sess = tf.InteractiveSession()\n        self.__Preprocessing(trainX)\n        self.gamma = tf.constant(value= -gamma, dtype=tf.float32)\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Cost)\n        self.sess.run(tf.global_variables_initializer())\n        \n        if self.training_epoch is not None:        \n            for ep in range(self.training_epoch):\n                self.sess.run(self.optimizer, feed_dict={self.X:trainX, self.Y:trainY})\n                if ep % self.display_step== 0:\n                    loss, acc = self.sess.run([self.Cost, self.Accuracy], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)\n        elif self.training_epoch is None:\n            acc = 0.1\n            ep = 0\n            while (acc< 1.- self.error):\n                acc,_ = self.sess.run([self.Accuracy, self.optimizer], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                ep += 1\n                if ep % self.display_step== 0: \n                    loss = self.sess.run(self.Cost, feed_dict={self.X:trainX, self.Y:trainY})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)     \n        print(\"Optimization Finished!\")      \n        self.trainX = trainX\n        self.trainY = trainY\n    \n    def pred(self,test):\n        output = self.sess.run(self.Prediction, feed_dict={self.X:self.trainX, self.Y:self.trainY, self.test:test})\n        return output", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "3546553742C9414FB7D8833DC444615E", "metadata": {}, "source": "### Linear SVM"}, {"cell_type": "code", "collapsed": false, "folded": false, "id": "FFBD96B0751F4F058A237F6D617C0888", "input": "import tensorflow as tf\nimport functools\n\ndef lazy_property(function):\n    attribute = '_' + function.__name__\n\n    @property\n    @functools.wraps(function)\n    def wrapper(self):\n        if not hasattr(self, attribute):\n            setattr(self, attribute, function(self))\n        return getattr(self, attribute)\n    return wrapper\n\nclass LinearSVC(object):\n    \n    def __init__(self,\n                learning_rate = 0.001,\n                training_epoch = None,\n                error = 0.001,\n                display_step = 5):\n        self.learning_rate = learning_rate\n        self.training_epoch = training_epoch\n        self.display_step = display_step\n        self.error = error\n        \n    def __Preprocessing(self, trainX):\n        row = trainX.shape[0]\n        col = trainX.shape[1]\n        self.X = tf.placeholder(shape=[row, col], dtype= tf.float32)\n        self.Y = tf.placeholder(shape=[row, 1], dtype= tf.float32)\n        self.test = tf.placeholder(shape=[None, col], dtype= tf.float32)\n        self.beta = tf.Variable(tf.truncated_normal(shape=[1, row], stddev=.1))\n        \n    @lazy_property \n    def Kernel_Train(self):\n        return tf.matmul(self.X, self.X, transpose_b=True)\n    \n    @lazy_property  \n    def Kernel_Prediction(self):  \n        return tf.matmul(self.X, self.test, transpose_b=True)\n    \n    @lazy_property  \n    def Cost(self):\n        left = tf.reduce_sum(self.beta)\n        beta_square = tf.matmul(self.beta, self.beta, transpose_a=True)\n        Y_square = tf.matmul(self.Y, self.Y, transpose_b= True)\n        right = tf.reduce_sum(tf.mul(self.Kernel_Train, tf.mul(beta_square, Y_square)))\n        return tf.neg(tf.sub(left, right))\n    \n    @lazy_property \n    def Prediction(self):\n        kernel_out = tf.matmul(tf.mul(tf.transpose(self.Y),self.beta), self.Kernel_Prediction)\n        return tf.sign(kernel_out - tf.reduce_mean(kernel_out))\n    \n    @lazy_property \n    def Accuracy(self):\n        return tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(self.Prediction), tf.squeeze(self.Y)), tf.float32))\n    \n    def fit(self, trainX, trainY, gamma= 50.):\n        self.sess = tf.InteractiveSession()\n        self.__Preprocessing(trainX)\n        self.gamma = tf.constant(value= -gamma, dtype=tf.float32)\n        #self.optimizer = tf.train.ProximalGradientDescentOptimizer(self.learning_rate).minimize(self.Cost)\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Cost)\n        self.sess.run(tf.global_variables_initializer())\n        \n        if self.training_epoch is not None:        \n            for ep in range(self.training_epoch):\n                self.sess.run(self.optimizer, feed_dict={self.X:trainX, self.Y:trainY})\n                if ep % self.display_step== 0:\n                    loss, acc = self.sess.run([self.Cost, self.Accuracy], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)\n        elif self.training_epoch is None:\n            acc = 0.1\n            ep = 0\n            while (acc< 1.- self.error):\n                acc,_ = self.sess.run([self.Accuracy, self.optimizer], feed_dict={self.X:trainX, self.Y:trainY, self.test:trainX})\n                ep += 1\n                if ep % self.display_step== 0: \n                    loss = self.sess.run(self.Cost, feed_dict={self.X:trainX, self.Y:trainY})\n                    print ('epoch=',ep,'loss= ',loss, 'accuracy= ', acc)        \n    \n        print(\"Optimization Finished!\")      \n        self.trainX = trainX\n        self.trainY = trainY\n    \n    def pred(self,test):\n        output = self.sess.run(self.Prediction, feed_dict={self.X:self.trainX, self.Y:self.trainY, self.test:test})\n        return output", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}