# 机器学习之“强化学习”

最基本的强化学习建立在马尔可夫决策过程（Markov Decision Process，MDP）上，当模型的动态特征已知时可以按照动态规划（Dynamic Programming，DP）进行迭代求解。

1988 年，时间差分算法（Temporal-Difference Method，TD）被应用于价值函数的迭代计算，它与类似的蒙特卡洛算法（Monte Carlo Method，MC）一样并不需要预先知道动态特征。本质上都被看作是 DP 的近似算法。

1989 年，经典的强化学习算法 Q-学习（Q-learning）被提出，它类似于 DP 中的价值迭代算法，但无需预先知道动态特征。

1994 年，SARSA 算法被提出，与 Q-learning 不同的是，SARSA 是同轨的（Onpolicy），即更新时下一步的动作依然按照原策略进行选取，它相对于 Q-learning 更加保守。

2013 年前后，深度学习与强化学习的结合，深度 Q 学习（Deep Q-learning）出现。以 Deep Q-learning 为代表的深度强化学习被广泛用于游戏、机器人、自动驾驶等各个领域。

2014 年左右，一些基于策略的（Policy-based）优化算法被提出。与之前的基于价值的（Value-based）算法不同，它整体评估一个策略，然后基于评估进行优化。它与深度学习联系紧密，被广泛应用于各个领域。

![![图片注释](http://storage-uqer.datayes.com/6245aa787bf0370166768fd0/45435222-407e-11ed-8ab2-0242ac140002)](http://storage-uqer.datayes.com/6245aa787bf0370166768fd0/41cf855c-407e-11ed-8ab2-0242ac140002)