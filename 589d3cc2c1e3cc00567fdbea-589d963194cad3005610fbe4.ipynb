{"metadata": {"signature": "sha256:0f7cb0f9f999fe50c976016018306dd2bfa0f25fc62f006b353b9d50b1c92bfb"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "289EC50978AD4D1697AAC0B371E9F2AA", "metadata": {}, "source": "# tensorflow\u7b14\u8bb06  RNN\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217"}, {"cell_type": "markdown", "id": "5C0D6B6EAC2F4E26840020D0865A880C", "metadata": {}, "source": "#### \u6700\u8fd1\u91cd\u770b\u4e86\u4e00\u904d\u5927\u4f5c\u624b\u56de\u5fc6\u5f55\uff0c\u5c31\u60f3\u80fd\u5426\u8ba9\u673a\u5668\u8bc6\u522b\u4e00\u4e0bM\u9876\u3001W\u5e95\u8fd9\u79cd\u76f8\u5f53\u4e3b\u89c2\u7684\u6280\u672f\u5206\u6790\u3002\n#### \u5047\u8bbe\u5404\u79cd\u6280\u672f\u5206\u6790\u56fe\u5f62\u53ef\u4ee5\u770b\u4f5c\u662f\u5404\u79cd\u6280\u672f\u5206\u6790\u56e0\u5b50\u5728\u4e00\u6bb5\u65f6\u95f4\u5185\u7684\u76f8\u5bf9\u6bd4\u4f8b\u53d8\u5316\uff0c\n#### \u8fd9\u6837\u53ef\u4ee5\u8ba4\u4e3a\u5c06\u6280\u672f\u5206\u6790\u56e0\u5b50\u8fdb\u884c\u6807\u51c6\u5316\u4e4b\u540e\u4e0d\u4f1a\u635f\u5931\u592a\u591a\u7684\u4fe1\u606f\u3002\n#### \u5c06\u56e0\u5b50\u968f\u4ea4\u6613\u65f6\u95f4\u63a8\u8fdb\u7684\u53d8\u6362\u770b\u4f5c\u65f6\u95f4\u5e8f\u5217 in_length\u8868\u793a\u65f6\u95f4\u5e8f\u5217\u7684\u957f\u5ea6\uff0c \n#### in_width\u770b\u4f5c\u65f6\u95f4\u5e8f\u5217\u7684\u5bbd\u5ea6\u4e5f\u5c31\u662f\u5e76\u5217\u7684\u591a\u79cd\u6280\u672f\u5206\u6790\u56e0\u5b50\u6570\u503c\u3002\n<br />\n\n\n#### \u4e0d\u8003\u8651\u65f6\u95f4\u63a8\u8fdb\u56e0\u7d20\u7684\u5f71\u54cd\u53ef\u4ee5\u4f7f\u7528CNN\u7b49\u975e\u65f6\u95f4\u5e8f\u5217DNN\u5904\u7406\u3002\n#### \u8003\u8651\u65f6\u95f4\u63a8\u8fdb\u5bf9\u56e0\u5b50\u53d8\u5316\u6709\u5f71\u54cd\uff0c\u6b64\u65f6\u523b\u56e0\u5b50\u53d8\u5316\u548c\u80a1\u4ef7\u6da8\u8dcc\u53d7\u5230\u524d\u9762\u72b6\u6001\u7684\u5f71\u54cd\u7684\u65f6\u95f4\u5e8f\u5217\u53ef\u4ee5\u4f7f\u7528HMM\u6216RNN\u5904\u7406\u3002\n#### HMM\u5e38\u89c1\u7684\u5047\u8bbe\u4e3a\u72b6\u6001i_t\u53ea\u53d7\u5230\u72b6\u6001i_{t-1}\u65f6\u523b\u7684\u5f71\u54cd\uff0cRNN\u5219\u6cdb\u6cdb\u8ba4\u4e3a\u53ef\u4ee5\u4f7f\u7528\u8bad\u7ec3\u96c6\u6765\u5b66\u4e60\u5230\u65f6\u95f4\u5e8f\u5217\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002\n#### \u672c\u5e16\u5bf9\u4e8eRNN\u7ed3\u6784\u8fdb\u884c\u7b80\u5355\u63a2\u7d22\u3002\n"}, {"cell_type": "markdown", "id": "69EBEE064C774D66925F18837E7F9BD3", "metadata": {}, "source": "### \u591a\u5c42RNN\u7ed3\u6784\u793a\u610f\u56fe\n-\u672c\u5e16\u6784\u5efa\u56db\u5c42RNN\u7f51\u7edc\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/e71cf6de-ef7b-11e6-aa07-0242ac140003)\n[\u56fe\u7247\u5f15\u7528](http://blog.csdn.net/heyongluoyao8/article/details/48636251)"}, {"cell_type": "markdown", "id": "4DD74B952F664891BF57B7ED53771673", "metadata": {}, "source": "### \u5b9a\u957f\u65f6\u95f4\u5e8f\u5217\n<br />\n\n\u8bad\u7ec3RNN\u6a21\u578b\u5b9a\u957f\u5e8f\u5217\uff0c\u5047\u8bbe\u5728T_i\u4ea4\u6613\u65e5\u53ef\u4ee5\u901a\u8fc7\u524dm\u4ea4\u6613\u65e5\u6280\u672f\u5206\u6790\u8d70\u52bf\u9884\u5224\u540e\u7b2cn\u4ea4\u6613\u65e5\u80a1\u4ef7\u6da8\u8dcc\u5e45\u5ea6\uff0c\u5e76\u4e14RNN\u6a21\u578b\u53ef\u4ee5\u81ea\u52a8\u4eceT_{i-m}\u5230T_i\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u5230\u8fd9\u79cd\u9884\u6d4b\u5173\u7cfb\u3002\n\n\u8f93\u5165\u6570\u636e\u683c\u5f0f[\u6279\u6b21\uff0c\u6b65\u957f\uff0c\u591a\u56e0\u5b50] \u5176\u4e2d\u6b65\u957f\u8868\u793a\u4eceT_{i-m}\u5230T_i\u65f6\u95f4\u5e8f\u5217\n\nclass.fit(trainX, trainY)\u8bad\u7ec3\u6a21\u578b   \n\nclf.pred_prob(trainX) \u9884\u6d4b\u8fd4\u56de\u6982\u7387\u77e9\u9635   \n\nclf.pred_signal(trainX) \u9884\u6d4b\u8fd4\u56de\u6807\u7b7e   \n\ntrainX \u8f93\u5165\u683c\u5f0f [row, in_length, in_width]   \n\ntrainY \u8f93\u5165\u683c\u5f0f [row]   \n\nbatch_size=128 \u5582\u5165\u6279\u6b21\u5927\u5c0f   \n\ndisplay_step=5 \u663e\u793a\u6b65\u957f   \n\nlayer_units_num=2000 \u9690\u85cf\u5c42\u5355\u5143\u6570\u76ee   \n\ntraining_epoch=100 \u8bad\u7ec3\u6b21\u6570    \n"}, {"cell_type": "code", "collapsed": false, "id": "E65A6CF5830F4D5095791F76925820E4", "input": "class test_1(object):\n    def __init__(self,\n                batch_size = 128,\n                learning_rate = 0.001,\n                training_epoch = 10,\n                display_step = 5,\n                layer_units_num = 100):\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.training_epoch = training_epoch\n        self.display_step = display_step\n        self.layer_units_num = layer_units_num\n        \n    def dense_to_one_hot(self,labels_dense):\n        \"\"\"\u6807\u7b7e \u8f6c\u6362one hot \u7f16\u7801\n        \u8f93\u5165labels_dense \u5fc5\u987b\u4e3a\u975e\u8d1f\u6570\n        2016-11-21\n        \"\"\"\n        num_classes = len(np.unique(labels_dense)) # np.unique \u53bb\u6389\u91cd\u590d\u51fd\u6570\n        raws_labels = labels_dense.shape[0]\n        index_offset = np.arange(raws_labels) * num_classes\n        labels_one_hot = np.zeros((raws_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n        return labels_one_hot  \n        \n    def Preprocessing(self, trainX, trainY, seed=False):\n        trainY = self.dense_to_one_hot(trainY)\n        self.in_length= in_length= trainX.shape[1]\n        self.in_width= in_width= trainX.shape[2]\n        self.out_classes= out_classes= trainY.shape[1]\n        \n        if seed:\n            tf.set_random_seed(20170204)\n        weights = {\n            'out': tf.Variable(tf.truncated_normal(shape=[self.layer_units_num, out_classes], \n                                                mean=0., stddev=1., seed=None, dtype=tf.float32),\n                               trainable=True, name='Weight_full_out')\n        }\n        \n        biases = {\n            'out': tf.Variable(tf.truncated_normal([out_classes]), trainable=True, name= 'Biases_full_out')\n        }    \n        \n        self.weights = weights\n        self.biases = biases        \n        \n        X = tf.placeholder(dtype=tf.float32, shape=[None, in_length, in_width], name='trainX') # \u6279\u6b21\uff0c\u65f6\u95f4\u5e8f\u5217\uff0c\u591a\u56e0\u5b50\n        Y = tf.placeholder(dtype= tf.float32, shape=[None, out_classes], name='trainY') \n        keep_prob = tf.placeholder(dtype= tf.float32)\n        self.X = X\n        self.Y = Y\n        self.keep_prob = keep_prob  \n        \n    def Network(self):\n        \n        with tf.name_scope('layer_1'):            \n            monolayer_1 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num, \n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)            \n            monolayer_1 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_1, output_keep_prob= keep_prob)\n\n        \n        with tf.name_scope('layer_2'):        \n            monolayer_2 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num,\n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n            monolayer_2 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_2, output_keep_prob= keep_prob)\n        \n        with tf.name_scope('layer_3'):                  \n            monolayer_3 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num,\n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n            monolayer_3 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_3, output_keep_prob= keep_prob)\n        \n        with tf.name_scope('layer_Final'):\n            monolayer_final = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.layer_units_num, \n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n        \n        with tf.name_scope('Layer_Structure_Combination'):\n            layer_units_num = self.layer_units_num\n            Layers = tf.nn.rnn_cell.MultiRNNCell(cells=[monolayer_1,monolayer_2,monolayer_3,monolayer_final],\n                                                state_is_tuple = True)    \n        self.Layers = Layers\n        return Layers\n        \n    def Model(self):\n        X = self.X\n        keep_prob = self.keep_prob\n        \n        X = tf.transpose(X, [1, 0, 2])\n        X = tf.reshape(X, [-1,self.in_width])\n        X = tf.split(split_dim=0, num_split=self.in_length, value=X)\n        \n        with tf.name_scope('layer_1'):            \n            monolayer_1 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num, \n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)            \n            monolayer_1 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_1, output_keep_prob= keep_prob)\n\n        \n        with tf.name_scope('layer_2'):        \n            monolayer_2 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num,\n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n            monolayer_2 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_2, output_keep_prob= keep_prob)\n        \n        with tf.name_scope('layer_3'):                  \n            monolayer_3 = tf.nn.rnn_cell.BasicLSTMCell(num_units= self.layer_units_num,\n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n            monolayer_3 = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer_3, output_keep_prob= keep_prob)\n        \n        with tf.name_scope('layer_Final'):\n            monolayer_final = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.layer_units_num, \n                                                       forget_bias=1., state_is_tuple=True, activation=tf.tanh)\n        \n        with tf.name_scope('Layer_Structure_Combination'):\n            layer_units_num = self.layer_units_num\n            Layers = tf.nn.rnn_cell.MultiRNNCell(cells=[monolayer_1,monolayer_2,monolayer_3,monolayer_final],\n                                                state_is_tuple = True)    \n        \n        outputs,_ = tf.nn.rnn(cell=monolayer_final, inputs=X, dtype=tf.float32)\n        output = outputs[-1]\n\n        return tf.nn.bias_add(value= tf.matmul(output, self.weights['out']), bias= self.biases['out'])  \n        \n    def train(self, trainX, trainY, seed=False):\n        self.sess = tf.InteractiveSession()\n        \n        self.Preprocessing(trainX, trainY, seed)\n        \n        tmp = self.Model()\n        \n        self.predict = tf.nn.softmax(tmp)\n        \n        self.cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(tmp, self.Y))\n        \n        optimizer = tf.train.AdamOptimizer(learning_rate= self.learning_rate) # 0 \u8bbe\u7f6e\u8bad\u7ec3\u5668\n        grads_and_vars = optimizer.compute_gradients(self.cost)\n        for i, (grid, var) in enumerate(grads_and_vars):\n            if grid != None:\n                grid = tf.clip_by_value(grid, -1., 1.)\n                grads_and_vars[i] = (grid, var)\n        optimizer = optimizer.apply_gradients(grads_and_vars)\n        self.optimizer = optimizer\n        \n        self.correct_pred = tf.equal(tf.argmax(tmp,1), tf.argmax(self.Y,1))\n        accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n        self.accuracy = accuracy\n        \n        #self.init = tf.global_variables_initializer()   \n        self.init = tf.initialize_all_variables()\n    def fit(self,trainX, trainY, dropout = 0.3, seed=True):\n        self.train(trainX, trainY, seed=True)\n        sess = self.sess\n        sess.run(self.init)\n        batch_size = self.batch_size\n        trainY = self.dense_to_one_hot(trainY)\n        for ep in range(self.training_epoch):\n            for i in range(int(len(trainX)/batch_size)+1):\n                if i < int(len(trainX)/batch_size)+1:\n                    batch_x = trainX[i*batch_size : (i+1)*batch_size]\n                    batch_y = trainY[i*batch_size : (i+1)*batch_size]\n                elif i== int(len(trainX)/batch_size)+1:\n                    batch_x = trainX[-batch_size:]\n                    batch_y = trainY[-batch_size:]\n                sess.run(self.optimizer, feed_dict={self.X:batch_x, self.Y:batch_y, self.keep_prob:(1.-dropout)})\n            if ep%self.display_step==0:                \n                loss, acc = sess.run([self.cost,self.accuracy], feed_dict={self.X:trainX, self.Y:trainY, self.keep_prob:1.})\n                print (str(ep)+\"th \"+'Epoch Loss = {:.5f}'.format(loss)+\" Training Accuracy={:.5f}\".format(acc))\n        self.sess= sess\n        print(\"Optimization Finished!\") \n    \n    def pred_prob(self, testX):\n        sess = self.sess\n        batch_size = self.batch_size\n        trainX = testX\n        predict_output = np.zeros([1,self.out_classes])\n        for i in range(int(len(trainX)/batch_size)+1):\n            if i < int(len(trainX)/batch_size)+1:\n                batch_x = trainX[i*batch_size : (i+1)*batch_size]\n                batch_y = trainY[i*batch_size : (i+1)*batch_size]\n            elif i== int(len(trainX)/batch_size)+1:\n                batch_x = trainX[-batch_size:]\n                batch_y = trainY[-batch_size:]\n            tp = sess.run(self.predict,feed_dict={self.X:batch_x, self.keep_prob:1.})\n            predict_output = np.row_stack([predict_output, tp])\n        predict_output = np.delete(predict_output, obj=0, axis=0)\n        return predict_output\n    \n    def pred(self, testX):\n        pred_prob = self.pred_prob(testX)\n        return np.argmax(pred_prob, axis=1)", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "542B521A1F034D918755813B51B4A4C9", "metadata": {}, "source": "#### \u53d8\u957f\u65f6\u95f4\u5e8f\u5217\n<br />\n\n\u8bad\u7ec3RNN\u6a21\u578b\u5b9a\u957f\u5e8f\u5217\uff0c\u5047\u8bbe\u5728T_i\u4ea4\u6613\u65e5\u53ef\u4ee5\u901a\u8fc7\u524dm\u4ea4\u6613\u65e5\u6280\u672f\u5206\u6790\u8d70\u52bf\u9884\u5224\u540e\u7b2cn\u4ea4\u6613\u65e5\u80a1\u4ef7\u6da8\u8dcc\u5e45\u5ea6\uff0cRNN\u6a21\u578b\u53ef\u4ee5\u4eceT_{i-m}\u5230T_i\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u5230\u8fd9\u79cd\u9884\u6d4b\u5173\u7cfb\uff0c\u4f46\u662f\u65e0\u6cd5\u8bad\u7ec3\u5f97\u5230\u51c6\u786e\u7684\u7a00\u758f\u6743\u91cd\u3002\u5bf9\u5e94T\u4ea4\u6613\u65e5\u8fdb\u884c\u9884\u5224\u7684\u65f6\u5019\u8003\u8651\u524d2\u5468\u8d70\u52bf\u6216\u8005\u524d2\u4e2a\u6708\u7684\u8d70\u52bf\uff0c\u8fd9\u4e2a\u65f6\u5019\u6a21\u578b\u65e0\u6cd5\u9690\u5f0f\u5b66\u4e60\uff0c\u9700\u8981\u6307\u5b9a\u8bad\u7ec3\u96c6\u4e3a\u53d8\u957f\u5e8f\u5217\u5bf9\u5e94\u6da8\u8dcc\u6807\u7b7e\u3002\n\n- \u8f93\u5165\n\n\u5bf9\u4e8e\u53d8\u957f\u65f6\u95f4\u5e8f\u5217 \u8bbe\u683c\u5f0f\u4e3a\n\n[batch_size, real_length, in_width]\n\n\u5176\u4e2dreal_length\u8868\u793a\u53d8\u957f\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u6b65\u957f\uff0c\u4e3a\u4e00\u4e2a\u53d8\u5316\u503c\uff0c\u4f7f\u7528LSTM\u8fdb\u884c\u9884\u6d4b\u7684\u65f6\u5019\u5c06\u8f93\u5165\u6570\u636e\u4fee\u6539\u683c\u5f0f\u4e3a\n\nbatch_size, in_length, in_width\n\n\u5176\u4e2din_length\u4f7f\u7528real_length\u4e2d\u7684\u6700\u5927\u6b65\u957f\u503c\uff0c\u7a7a\u4f59\u90e8\u5206\u4f7f\u75280\u586b\u5145\n\n\u4fee\u6539\u539f\u59cb\u7a20\u5bc6\u77e9\u9635\u4e3a\u7a00\u758f\u77e9\u9635\u7edf\u4e00\u683c\u5f0f\u8fdb\u884c\u8f93\u5165\u3002\n\n- \u8f93\u51fa\u88c1\u526a\n\n\u7531\u4e8e\u8f93\u5165tensor\u4e3a\u7a00\u758f\u77e9\u9635\uff0c\u5219\u5bf9\u5e94\u7684RNN\u7f51\u7edc\u8ba1\u7b97\u5f97\u5230\u7684\u77e9\u9635\u4e3a\u7a00\u758f\u77e9\u9635\uff08\u5bf9\u4e8e\u5168\u4e3a0\u7684\u586b\u5145\u7a00\u758f\u90e8\u5206\u8fdb\u884c\u6570\u503c\u4f18\u5316\u7684\u65f6\u5019\u68af\u5ea6\u4e3a0\u5b9e\u9645\u4e0d\u53d8\u5316\uff09\n\n\u5c06\u62d3\u6251\u7ed3\u6784\u56fe\u5f97\u5230\u7684\u77e9\u9635\u8fdb\u884c\u88c1\u526a\uff0c\u4f7f\u5f97\u8f93\u51fatensor\u683c\u5f0f\u4ece\n\n[batch_size, in_length, layer_units_num]\n\n\u8f6c\u6362\u4e3a\n\n[batch_size, 1, layer_units_num]\n\nsoftmax [batch_size, out_classes]\n\n\u8fd9\u91cc1\u8868\u793a\u8fd9\u91cc\u5bf9\u8f93\u5165\u6b65\u957f\u53d6\u5b9e\u9645\u8f93\u5165\u6b65\u957f\u6700\u540e\u4e00\u6b65\u3002\n"}, {"cell_type": "code", "collapsed": false, "id": "4E90017B0397418DBE3DF807B1188C7B", "input": "class test(object):\n    def __init__(self,\n                batch_size = 128,\n                learning_rate = 0.001,\n                error = .01,\n                display_step = 5,\n                layer_units_num = 200):\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.error = error\n        self.display_step = display_step\n        self.layer_units_num = layer_units_num\n        \n    def dense_to_one_hot(self,labels_dense):\n        \"\"\"\u6807\u7b7e \u8f6c\u6362one hot \u7f16\u7801\n        \u8f93\u5165labels_dense \u5fc5\u987b\u4e3a\u975e\u8d1f\u6570\n        2016-11-21\n        \"\"\"\n        num_classes = len(np.unique(labels_dense)) # np.unique \u53bb\u6389\u91cd\u590d\u51fd\u6570\n        raws_labels = labels_dense.shape[0]\n        index_offset = np.arange(raws_labels) * num_classes\n        labels_one_hot = np.zeros((raws_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n        return labels_one_hot  \n    \n    # \u83b7\u5f97\u6743\u91cd\u548c\u504f\u7f6e\n    @staticmethod\n    def _weight_and_bias(in_size, out_size):\n        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n        bias = tf.constant(0.1, shape=[out_size])\n        return tf.Variable(weight), tf.Variable(bias)\n    \n    @lazy_property\n    def length(self):\n        # \u8fd4\u56de\u8f93\u5165\u7a00\u758f\u5f20\u91cf\u8f6c\u53d8\u4e3a\u7a20\u5bc6\u5f20\u91cf\u7684\u5b9e\u9645\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\n        # \u5904\u7406\u8f93\u5165\u7684\u7a00\u758f\u77e9\u9635\uff0c\u8fd4\u56de\u4e00\u4e2a\u7cbe\u7b80\u8fc7\u7684\u7a20\u5bc6\u77e9\u9635\u63cf\u8ff0\n        # \u8f93\u5165\u7a00\u758f\u77e9\u9635 batch_size, in_lenth, in_width \u8f93\u51fa\u4e3abatch_size \u5bf9\u5e94\u53d8\u957f\u65f6\u95f4\u5e8f\u5217 in_lenth\u957f\u5ea6\n        # \u8f93\u5165\u683c\u5f0f\u4e3a batch_size, in_length, in_width \u5bf9in_width \u53d6\u7edd\u5bf9\u503c\u4e4b\u540e\u7684\u6700\u5927\u503c\uff0c\u5982\u679c\u4e3a\u7a7a\u7f6e\uff0c\u524d\u9762\u9ed8\u8ba4\u4e3a0\uff0czh,\u5219\u6700\u5927\u503c\u8fd4\u56de0,\n        # tf.sign \u8fd4\u56de\u683c\u5f0f [batch_size, in_length],\u5176\u4e2d\u586b\u5145\u6570\u503c\u4e3a0,1 \u7b26\u53f7\u5e8f\u5217\n        dense_sign = tf.sign(tf.reduce_max(tf.abs(self.X),axis=2))\n        length = tf.reduce_sum(input_tensor=dense_sign, axis=1)\n        # \u8fd4\u56dedense_sign \u7684 in_length\u957f\u5ea6\n        length = tf.cast(length, tf.int32)\n        return length\n    \n    @staticmethod\n    def _final_relevant(output, length):\n        # length \u8f93\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u5b9e\u9645\u957f\u5ea6\n        # in_length \u8868\u793a\u8f93\u5165\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\n        # max_length \u8868\u793a\u6700\u5927\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6,\u4e5f\u5c31\u662f\u7a00\u758f\u77e9\u9635 \u6700\u5927\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\n        batch_size = tf.shape(output)[0]\n        max_length = int(output.get_shape()[1])\n        output_size = int(output.get_shape()[2])\n        index = tf.range(start=0, limit=batch_size)*max_length + (length-1) # \u8fd9\u91cc\u4f7f\u7528max_length \u5f00\u521b\u95f4\u9694\uff0c\u4f7f\u7528length-1\u8868\u793a\u5b9e\u9645\u4f4d\u7f6e\uff0c\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\u7684\u4f4d\u7f6e\n        flat = tf.reshape(output, [-1,output_size]) # \u5c06\u8f93\u51fa\u5c55\u5e73\uff0cbatch_size*length in_width\n        relevant = tf.gather(flat, index) # \u6839\u636e\u5b9e\u9645\u957f\u5ea6\u9009\u51fa\u6700\u540e\u4e00\u4e2a\u8f93\u51faoutput\u72b6\u6001\u4f7f\u7528\n        return relevant  \n    \n    def Preprocessing(self, trainX, trainY):\n        self.in_length= in_length= trainX.shape[1]\n        self.in_width= in_width= trainX.shape[2]\n        self.out_classes= out_classes= trainY.shape[1]\n        \n        self.X = tf.placeholder(dtype=tf.float32, shape=[None, in_length, in_width], name='trainX') # \u6279\u6b21\uff0c\u65f6\u95f4\u5e8f\u5217\uff0c\u591a\u56e0\u5b50\n        self.Y = tf.placeholder(dtype= tf.float32, shape=[None, out_classes], name='trainY') \n        self.keep_prob = tf.placeholder(dtype= tf.float32)\n    \n    def str2float(self,s):  \n        def char2num(s):  \n            return {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}[s]  \n        n = s.index('.')  \n        return reduce(lambda x,y:x*10+y,map(char2num,s[:n]+s[n+1:]))/(10**n)  \n    \n    def Interface(self):        \n        # 4\u5c42GRU\u7ed3\u6784\u63cf\u8ff0\n        monolayer = tf.nn.rnn_cell.GRUCell(num_units= self.layer_units_num)\n        monolayer = tf.nn.rnn_cell.DropoutWrapper(cell=monolayer, output_keep_prob=self.keep_prob)\n        monolayer_final = tf.nn.rnn_cell.GRUCell(num_units= self.layer_units_num)\n        layers = tf.nn.rnn_cell.MultiRNNCell([monolayer]*3+[monolayer_final])\n        # \u6fc0\u6d3b \u6ce8\u610f in_length \u8868\u793a\u8f93\u5165\u5e8f\u5217\u6b65\u957f\uff0c length \u8868\u793a\u5b9e\u9645\u6b65\u957f\n        output,_ = tf.nn.dynamic_rnn(cell= layers, inputs= self.X, dtype= tf.float32, sequence_length= self.length)        \n        output = self._final_relevant(output, self.length)\n        \n        weights, biases = self._weight_and_bias(self.layer_units_num, self.out_classes)        \n        Prediction = tf.nn.bias_add(tf.matmul(output, weights),biases)\n        return Prediction\n    \n    def Graph(self, trainX, trainY):\n        try:\n            tf.InteractiveSession.close()\n        except:\n            pass\n        self.sess = tf.InteractiveSession()\n        tf.get_default_session()\n        self.Preprocessing(trainX, trainY)\n        tmp = self.Interface()\n        \n        self.pred = tf.nn.softmax(tmp)\n        self.cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(tmp, self.Y))\n        \n        optimizer = tf.train.AdamOptimizer(learning_rate= self.learning_rate) # 0 \u8bbe\u7f6e\u8bad\u7ec3\u5668\n        grads_and_vars = optimizer.compute_gradients(self.cost)\n        for i, (grid, var) in enumerate(grads_and_vars):\n            if grid != None:\n                grid = tf.clip_by_value(grid, -1., 1.)\n                grads_and_vars[i] = (grid, var)\n        optimizer = optimizer.apply_gradients(grads_and_vars)\n        self.optimizer = optimizer\n        \n        self.correct_pred = tf.equal(tf.argmax(tmp,1), tf.argmax(self.Y,1))\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n        self.init = tf.global_variables_initializer()\n        \n    def fit(self, trainX, trainY, dropout= 0.618):\n        # \u5bf9\u6807\u7b7e one_hot\u7f16\u7801\n        trainY = self.dense_to_one_hot(trainY)\n        \n        self.Graph(trainX, trainY)\n        self.sess.run(self.init)\n        batch_size = self.batch_size\n        loss =1000.\n        ep = 0\n        while (loss > self.error):\n            for i in range(int(len(trainX)/batch_size)+1):\n                if i < int(len(trainX)/batch_size)+1:\n                    batch_x = trainX[i*batch_size : (i+1)*batch_size]\n                    batch_y = trainY[i*batch_size : (i+1)*batch_size]\n                elif i== int(len(trainX)/batch_size)+1:\n                    batch_x = trainX[-batch_size:]\n                    batch_y = trainY[-batch_size:]\n                self.sess.run(self.optimizer,feed_dict={self.X:batch_x, self.Y:batch_y, self.keep_prob:(1.-dropout)})\n            loss = self.sess.run(self.cost, feed_dict={self.X:trainX, self.Y:trainY, self.keep_prob:1.})\n            if ep%self.display_step==0:                \n                acc = self.sess.run(self.accuracy, feed_dict={self.X:trainX, self.Y:trainY, self.keep_prob:1.})\n                print (str(ep)+\"th \"+'Epoch Loss = {:.5f}'.format(loss)+\" Training Accuracy={:.5f}\".format(acc))\n            ep += 1\n        print(\"Optimization Finished!\")                \n        \n    def pred_prob(self, testX):\n        batch_size = self.batch_size\n        trainX = testX\n        predict_output = np.zeros([1,self.out_classes])\n        for i in range(int(len(trainX)/batch_size)+1):\n            if i < int(len(trainX)/batch_size)+1:\n                batch_x = trainX[i*batch_size : (i+1)*batch_size]\n                batch_y = trainY[i*batch_size : (i+1)*batch_size]\n            elif i== int(len(trainX)/batch_size)+1:\n                batch_x = trainX[-batch_size:]\n                batch_y = trainY[-batch_size:]\n            tp = self.sess.run(self.pred, feed_dict={self.X:batch_x, self.keep_prob:1.})\n            predict_output = np.row_stack([predict_output, tp])\n        predict_output = np.delete(predict_output, obj=0, axis=0)\n        return predict_output\n    \n    def pred_signal(self, testX):\n        pred_prob = self.pred_prob(testX)\n        return np.argmax(pred_prob, axis=1)", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}