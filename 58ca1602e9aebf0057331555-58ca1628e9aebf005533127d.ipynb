{"metadata": {"signature": "sha256:f85c8deb018448efbdd7a3dfaea97951d2920ce040d68d158e4511721e4adf18"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "3029DC7063194E5AA4186A8FDACBDFCE", "metadata": {}, "source": "# tf_11NeuralGPU\u5377\u79ef\u4e0eRNN\u7ed3\u5408\u8d85\u7ebf\u6027\u8ba1\u7b97\u65f6\u95f4\u591a\u56e0\u5b50\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b"}, {"cell_type": "markdown", "id": "010880773F5D4A319379CC755FDC1E6F", "metadata": {}, "source": "# NeuralGPU\u5377\u79ef\u4e0eRNN\u7ed3\u5408\u8d85\u7ebf\u6027\u8ba1\u7b97\u65f6\u95f4\u591a\u56e0\u5b50\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b"}, {"cell_type": "markdown", "id": "4303678E9E224F74AEBADCD5F76B767E", "metadata": {}, "source": "\u770bNTM blog\u7684\u65f6\u5019\u53d1\u73b0\u7684NeuralGPU\u7684paper\uff0c\u611f\u89c9\u8fd9\u4e2a\u4e1c\u897f\u8981\u662f\u80fd\u591f\u6253\u7834RNN\u67b6\u6784\u6216\u8005\u662f\u66f4\u52a0\u5de7\u5999\u7684\u5229\u7528\u5377\u79ef\u5c06\u4f1a\u662f\u4e00\u4e2a\u4ee4\u4eba\u60ca\u53f9\u7684\u8bbe\u8ba1\uff0c\u4f46\u662f\u51e0\u7bc7\u8bba\u6587\u91cc\u9762\u90fd\u8fd8\u662f\u5728\u7cc5\u5408CNN\u548cRNN\uff0c\u4e2a\u4eba\u611f\u89c9\u8fd9\u4e2a\u8bbe\u8ba1\u5e94\u8be5\u8fd8\u662f\u5904\u4e8e\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u3002\n\u66f4\u65b0\uff1a\n\nNeuralGPU\u7b80\u5355\u4ecb\u7ecd---\u5728\u6211\u524d\u9762\u7684\u77ed\u6587\u91cc\u9762\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5e38\u89c1\u7684\u4f7f\u7528\u5377\u79efCNN\u7f51\u7edc\u5904\u7406\u591a\u56e0\u5b50\u80a1\u7968\u6536\u76ca\u7387\u5e8f\u5217\u7684\u73a9\u6cd5\uff0c\u5728\u5904\u7406\u591a\u56e0\u5b50\u6570\u636e\u7684\u65f6\u5019\uff0c\u5c06\u591a\u56e0\u5b50\u6570\u636e\u5408\u6210\u4e00\u526f\u56fe\u7247\uff0c\u7136\u540e\u4f7f\u7528CNN\u5904\u7406\u8fd9\u4e2a\u56fe\u7247\uff0c\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u91ccNeuralGPU\u7684\u73a9\u6cd5\u5927\u81f4\u7c7b\u4f3c\uff0c\u6709\u4e00\u70b9\u533a\u522b\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u6a21\u578b\u5c06\u5377\u79ef\u6838\u91cc\u9762\u7684\u901a\u9053\uff08\u6bcf\u5c42\u5377\u79ef\u6ee4\u6ce2\u5668\u7684\u6570\u91cf\uff09\u6269\u5c55\u6210\u4e00\u4e2a\u8f93\u5165\u591a\u56e0\u5b50\u6570\u636e\u7684\u7ef4\u5ea6\uff0c\u540c\u65f6\u5c06\u539f\u6765\u76842D\u56fe\u7247\u7684\u9ad8\u5ea6\u5f53\u4f5c\u662f\u65f6\u95f4\u5e8f\u5217\u6b65\uff0c\u8fd9\u6837\u5728\u7b2c\u4e00\u526f\u56fe\u7247\u8f93\u5165\u7684\u65f6\u5019\uff08\u539f\u59cb\u6570\u636e\u5d4c\u5165\uff09in_channels\u8fdb\u5165\u901a\u9053\u6570\u91cf\u4ece\u539f\u6765\u7684\u8868\u793a3\u5e45\u7ea2\u7eff\u84dd\u4e09\u57fa\u8272\u7684\u56fe\u7247\u53d8\u6210\u4e86\u591a\u56e0\u5b50\u6570\u636e\u3002\n\n\u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\uff0c\u53ef\u4ee5\u53c2\u7167\u4e0b\u9762\u7684\u7acb\u65b9\u4f53\u56fe\u7247\uff0c\u524d\u8868\u9762\u5c31\u662f\u5e38\u89c1\u7684\u5377\u79ef\u56fe\u7247\uff0c\u7acb\u65b9\u4f53\u7684\u539a\u5ea6\u4e5f\u5c31\u662f\u901a\u5e38\u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u7684\u6570\u91cf\uff0c\u8fd9\u91cc\u5c06\u524d\u8868\u9762\u7684\u4e0a\u7b2c\u4e00\u5217\u7684\u6570\u636e\u70b9\u5f53\u4f5c\u65f6\u523b\u6807\u5ea6\uff0c\u7136\u540e\u6bcf\u4e2a\u6570\u636e\u70b9\u5bf9\u5e94\u7684\u539a\u5ea6\u4e5f\u5c31\u662f\u4e00\u4e2a\u8d85\u957f\u7684\u6570\u636e\u5217\u5d4c\u5165\u591a\u56e0\u5b50\u5e8f\u5217\u3002\u8fd9\u4e2a\u65f6\u5019\u8fd9\u4e2a\u7acb\u65b9\u4f53\u5c31\u5d4c\u5165\u4e00\u5f20\u56fe\u7247\uff0c\u5728yoz\u8f74\u7ebf\u4e0a\uff0c\u8fd9\u4e2a\u7acb\u65b9\u4f53\u5176\u4ed6\u5730\u65b9\u521d\u59cb\u7f6e0\u5c31\u5b8c\u6210\u521d\u59cb\u6570\u636e\u5d4c\u5165\u4e86\u3002\n\n\u66f4\u65b0\uff1a\n\n\u5728\u540e\u7eed\u7684paper\u91cc\u9762\u4efb\u52a1\u539f\u59cb\u7684\u6a21\u578b\u8bbe\u8ba1\u5bf9NLP\u5904\u7406\u6548\u679c\u6b20\u4f73\uff0c\u5e76\u63a2\u7d22\u4e86\u9a6c\u5c14\u79d1\u592b\u6269\u5c55\u548c\u5377\u79ef\u6269\u5c55\uff0c\u8fd9\u91cc\u7531\u4e8e\u4f7f\u7528many to one\u7684\u6570\u636e\u5904\u7406\uff0c\u7a0d\u4f5c\u8c03\u6574\uff0c\u5728\u6700\u540e\u4e00\u6b65\u76f4\u63a5\u4f7f\u7528tf.matmul\u5904\u7406\u53d6\u4ee3\u5377\u79ef\u64cd\u4f5c\u3002\n\n\u6570\u636e\u6269\u5c55\u5982\u4e0b\u56fe1\uff0c\u5728\u5bf9\u6a21\u578b\u5faa\u73afn\u6b21\u4e4b\u540e\uff0c\u4f7f\u7528\u53e6\u5916\u7684n\u6b21\u5faa\u73af\u6bcf\u6b21\u4f9d\u6b21\u83b7\u5f97\u4e00\u4e2a\u65f6\u523b\u8f93\u51fa\uff0c\u5e76\u7d2f\u79ef\u65f6\u523b\u8f93\u51fa\u8fdb\u5165\u8fed\u4ee3\u516c\u5f0f\uff0c\u663e\u5f0f\u8fbe\u5230\u524d\u4e00\u4e2a\u65f6\u523b\u8f93\u51fa\u5f71\u54cd\u540e\u4e00\u65f6\u523b\u7684\u6548\u679c\uff08\u8fd9\u91cc\u53c8\u4ece\u65b0\u56de\u5230\u4e86RNN\u6a21\u578b\u601d\u8def\u4e0a\u4e86\uff09\uff0c\u8bbe\u7f6etape\u7f13\u5b58\uff0c\u521d\u59cb\u7f6e0\uff0c\u7136\u540e\u6ca1\u5faa\u73af\u4e00\u6b21\u586b\u5145\u4e00\u4e2at\u65f6\u523b\u7684\u8f93\u51fa\u7ed3\u679c\u5e76\u8fdb\u5165\u4e0b\u4e00\u6b21\u5faa\u73af\u4e2d\u3002\n\n\u5bf9\u4e8e\u4e00\u4e2a\u5b9e\u4f8b\u6765\u5b66\u4e60\u7b97\u6cd5\uff0cNeuralGPU\u6700\u5927\u7684\u4e00\u4e2a\u4f18\u70b9\u662f\u5b9e\u73b0\u8d85\u7ebf\u6027\u8ba1\u7b97\u65f6\u95f4\u7684\u7b97\u6cd5\u5b66\u4e60\uff0c\u8fd9\u4e2a\u662f\u76ee\u524d\u4ec5\u6709\u7684\u4e00\u4e2a\u652f\u6301\u8d85\u7ebf\u6027\u8ba1\u7b97\u65f6\u95f4\u7684RNN\u53d8\u79cd[2]\u3002\u5728\u5bf9NeuralGPU\u8fdb\u884c\u6269\u5c55\u4e4b\u540e\u53d6\u5f97\u4e86\u76f8\u5f53\u4e0d\u9519\u7684\u6548\u679c\u5728NLP\u4e0a\u9762[1]\u3002\n\n![](https://pic1.zhimg.com/v2-1364f92f7732a22a6d5661320db15190_b.png)\nNeuralGPU\u7684\u4f7f\u7528\u65b9\u6cd5\u53ef\u4ee5\u5047\u60f3\u4e0b\u9762\u7684\u7acb\u65b9\u4f53\u3002\n\n\u4f7f\u7528tf.nn.conv2d\u91cc\u9762\u7684in_channels\u8868\u793a\u591a\u56e0\u5b50\u6570\u636e\uff0c\u5bf9\u5e94\u5377\u79ef\u64cd\u4f5c\u4e3a\uff0c2D\u5377\u79ef\u4e2d\u56fe\u7247\u7684\u9ad8\u5ea6\u4e3aRNN\u4e2d\u7684\u65f6\u95f4\u6b65\uff0c\u5bbd\u5ea6\u4e3a\u521d\u59cb\u7f6e0\uff0c\u591a\u901a\u9053\u7684\u4e3a\u591a\u56e0\u5b50\u6570\u636e\u3002\n\n\u5982\u4e0a\u56fe\uff0c\u5bf9\u5e94\u4e00\u4e2a2d\u5377\u79ef\u6838\u64cd\u4f5c\uff0c\u7b2c\u4e00\u5217\uff08\u9634\u5f71\u5217\uff09\u4e3a\u65f6\u95f4\u6b65\uff0c\u6bcf\u4e2a\u50cf\u7d20\u70b9\u5bf9\u5e94\u4e00\u4e2aRNN\u7684\u65f6\u523bk\u7684\u8f93\u5165\uff0c\u4e5f\u5c31\u662f\u56fe\u7247\u7684\u9ad8\u5ea6\u8f93\u5165\u65f6\u95f4\u6b65\uff0c\u5bbd\u5ea6\u975e\u7b2c\u4e00\u5217\u521d\u59cb\u7f6e0. \u5c06\u591a\u901a\u9053\uff08\u591a\u6ee4\u6ce2\u5668\uff09\u3002\n\n\u8f93\u5165\u6570\u636e\u683c\u5f0f\u4e3a[batch, in_length, in_width] batch x \u65f6\u523b x \u591a\u56e0\u5b50\u6570\u636e\n\n\u8c03\u6574\u4e3a[batch, in_length, w, in_width]\u6570\u636e\u683c\u5f0f\uff0c\u5176\u4e2dw\u7b2c\u4e00\u5217\u4e3a\u8f93\u5165\u6570\u636e\uff0c\u5176\u4f59\u7f6e0\u5728\u521d\u59cb\u72b6\u6001\u3002\u591a\u56e0\u5b50\u6570\u636e\u5bf9\u5e942D\u5377\u79ef\u7684\u901a\u9053\u64cd\u4f5c\u3002\n\n![](https://pic2.zhimg.com/v2-fbdcd4629696a9ea29eed77fb6d6d035_b.jpg)\n\n\u7acb\u65b9\u4f53 in_length*w \u9762\u4e3a\u8bba\u6587\u91cc\u9762\u7684mental image\uff0c\u6df1\u5ea6\uff08in_width,\u591a\u56e0\u5b50\u6570\u636e\uff09\u5bf9\u5e94tf.nn.conv2d\u91cc\u9762\u7684channels\uff0c\u8fd9\u4e2a\u5730\u65b9\u662f\u4e00\u4e2a\u6bd4\u8f83\u5bb9\u6613\u9519\u610f\u7684\u5730\u65b9\u3002\u603b\u7684\u6765\u8bf4\u867d\u7136\u8fd9\u4e2a\u6a21\u578b\u5f15\u5165\u7684\u5377\u79ef\uff0c\u5e76\u4e14\u62e5\u6709\u4e00\u4e2a\u8d85\u7ebf\u6027\u7684\u8ba1\u7b97\u65f6\u95f4\uff0c\u4f46\u662f\u7ecf\u8fc7\u4ed4\u7ec6\u7814\u7a76\uff0c\u6211\u8ba4\u4e3a\u8fd9\u4f9d\u7136\u662f\u4e00\u4e2a\u5e7f\u4e49\u7ebf\u6027\u73a9\u6cd5\uff0c\u8fd9\u6837\u8fdb\u884c\u5efa\u6a21\u5bf9\u5e94\u591a\u56e0\u5b50\u5e8f\u5217\u9884\u6d4b\u4f1a\u63a2\u7d22\u591a\u56e0\u5b50\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u4f46\u662f\u5e76\u4e0d\u4f1a\u6539\u53d8\u591a\u56e0\u5b50\u56e0\u5b50\u7684\u9636\u6b21\uff0c\u6240\u4ee5\u8be5\u6a21\u578b\u5efa\u7acb\u4e4b\u540e\uff0c\u7ecf\u8fc7\u5b66\u4e60\u5bf9\u5e94\u7684APT\u6a21\u578b\u4f9d\u7136\u662fi=1\u7684\u60c5\u51b5\u3002\n\n$$r_p = \\alpha + \\beta_1 x^n_1+...+ \\beta_i x^n_i $$\n\u8fd9\u4e2a\u6a21\u578b\u6211\u572814~16\u5e74HS300\u6210\u4efd\u80a1\u6a2a\u9762\u6570\u636e\u5904\u7406\u65b9\u5f0f\u8fdb\u884c\u591a\u56e0\u5b50\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65f6\u5019\uff0c\u6cdb\u5316\u6548\u679c\u76f8\u5f53\u4e0d\u9519\uff0c\u52a0\u4e0a\u4e24\u5c42dropout\u4e4b\u540e\u6cdb\u5316\u6548\u679c\u76f8\u5f53\u597d\uff0c\u4f46\u662f\u6a21\u578b\u6784\u5efa\u6c42\u89e3\u7279\u522b\u9ebb\u70e6\u3002"}, {"cell_type": "markdown", "id": "43F0A55516644CF985940CFBA134F626", "metadata": {}, "source": "\u53c2\u8003\uff1a\n\nCan Active Memory Replace Attention?\n\nExtensions and Limitations of the Neural GPU\n\nNeural GPUs Learn Algorithms\n\npython\u4ee3\u7801\uff1a\n\n\u5377\u79efRNN\u51fd\u6570\u90e8\u5206"}, {"cell_type": "code", "collapsed": false, "id": "3C0A97BAE21247A4811667D3B82A4AE6", "input": "import tensorflow as tf\n\ndef CGRU(state, parameter, prefix):\n    kh = parameter[0]\n    kw = parameter[1]\n    c_in = parameter[2]\n    c_out = parameter[3]\n    \n    with tf.variable_scope(prefix):\n        # reset\n        U_1 = tf.get_variable(name='U1_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_1 = tf.get_variable(name='B1_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())    \n        reset = tf.sigmoid(tf.add(x=tf.nn.conv2d(input=state, filter=U_1, strides=[1,1,1,1], padding='SAME'), y=B_1))\n        #reset = sigmoid_cutoff(tf.add(x=tf.nn.conv2d(input=state, filter=U_1, strides=[1,1,1,1], padding='SAME'), y=B_1))\n        \n        \n        # update\n        U_2 = tf.get_variable(name='U2_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_2 = tf.get_variable(name='B2_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())  \n        update = tf.sigmoid(tf.add(x=tf.nn.conv2d(input=state, filter=U_2, strides=[1,1,1,1], padding='SAME'), y=B_2))\n        \n        # CGRU(s)\n        U_0 = tf.get_variable(name='U0_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_0 = tf.get_variable(name='B0_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())  \n        cgru_tmp = tf.add(tf.nn.conv2d(input=tf.multiply(reset,state), filter=U_0, strides=[1,1,1,1], padding='SAME'), B_0)\n        return tf.add(tf.multiply(update, state), tf.multiply(tf.subtract(1.,update),tf.tanh(cgru_tmp)))\n    \ndef Multi_CGRUs(state, parameter, layers_num, prefix):\n    for i in range(layers_num):\n        state = CGRU(state, parameter, prefix+\"multi_%d\"%i)\n    return state\n\ndef CGRU_d(state, tape, prameter, prefix):\n    kh = parameter[0]\n    kw = parameter[1]\n    c_in = parameter[2]\n    c_out = parameter[3]\n    \n    with tf.variable_scope(prefix):\n        # reset\n        U_1 = tf.get_variable(name='U1_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        W_1 = tf.get_variable(name='W1', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_1 = tf.get_variable(name='B1_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())    \n        r1 = tf.nn.conv2d(input=state, filter=U_1, strides=[1,1,1,1], padding='SAME')\n        r2 = tf.nn.conv2d(input=tape, filter=W_1, strides=[1,1,1,1], padding='SAME')\n        reset = tf.sigmoid(tf.add(tf.add(r1,r2),B_1))\n        \n        # update\n        U_2 = tf.get_variable(name='U2_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        W_2 = tf.get_variable(name='W2', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_2 = tf.get_variable(name='B2_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())\n        u1 = tf.nn.conv2d(input=state, filter=U_2, strides=[1,1,1,1], padding='SAME')\n        u2 = tf.nn.conv2d(input=tape, filter=W_2, strides=[1,1,1,1], padding='SAME')\n        update = tf.sigmoid(tf.add(tf.add(u1,u2),B_2))\n        \n        # CGRU(s)\n        U_0 = tf.get_variable(name='U0_kernel_bank', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        W_0 = tf.get_variable(name='W0', shape=[kh, kw, c_in, c_out], initializer=tf.truncated_normal_initializer())\n        B_0 = tf.get_variable(name='B0_bias_vectors', shape=[c_out], initializer=tf.truncated_normal_initializer())  \n        c1 = tf.nn.conv2d(input=tf.multiply(reset,state), filter=U_0, strides=[1,1,1,1], padding='SAME')\n        c2 = tf.nn.conv2d(input=tape, filter=W_0, strides=[1,1,1,1], padding='SAME')\n        c3 = tf.tanh(tf.add(tf.add(c1,c2),B_0))\n        return tf.add(tf.multiply(update, state), tf.multiply(tf.subtract(1.,update),c3))    \n\ndef Multi_CGRUDs(state, tape, parameter, layers_num, prefix):\n    for i in range(layers_num):\n        state = CGRU_d(state, tape, parameter, prefix+\"multi_%d\"%i)\n    return state", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "\u6a21\u5757:tensorflow\u4e0d\u652f\u6301, \u5982\u679c\u60a8\u9700\u8981\u6dfb\u52a0, \u8bf7\u8054\u7cfb\u6211\u4eec."}, {"output_type": "stream", "stream": "stdout", "text": "\n"}], "trusted": true}, {"cell_type": "markdown", "id": "E395A8D1645E4BAFA1C499ED6ACCFE4D", "metadata": {}, "source": "\u539f\u59cbNerual_GPU\u7684 \u62d3\u6251\u7ed3\u6784\u6784\u5efa\uff0c\u4f7f\u7528tf.whlie_loop\u7684swap\u8282\u7701\u663e\u5b58\uff0c\u8fd9\u4e00\u90e8\u5206\u4e3b\u8981\u662f\u4e00\u4e2a\u5143\u7ec6\u80de\u7ed3\u6784\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u5b66\u4e60n\u591a\u8fdb\u5236\u4f4d\u201c\u56db\u5219\u8fd0\u7b97\u201d\u7684\u90a3\u4e2a\u521d\u59cb\u6a21\u578b\u3002"}, {"cell_type": "code", "collapsed": false, "id": "8C612762E1DF4E2B8C8CE402D0476F10", "input": "with tf.variable_scope('previous') as scope_previous:\n    tmp = Multi_CGRUs(X, parameter, layers, prefix='previous_part_for_n_steps_')\n    \n    def cond(tp, tmp):\n        return tf.less(tp, in_length)\n\n    def body(tp, tmp):\n        scope_previous.reuse_variables()\n        tmp = Multi_CGRUs(tmp, parameter, layers, prefix='previous_part_for_n_steps_')\n        tp = tp +1\n        return tp, tmp\n\n    ftp, prev = tf.while_loop(cond, body, [1, tmp]) \nPrevious_Part = prev   \nlen(tf.trainable_variables())\n", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "B90246B5758D4B3B81AB24B68643D6BA", "metadata": {}, "source": "NerualGPU \u6269\u5c55\u8f93\u51fa\u90e8\u5206\uff0c\u8fd9\u662f\u6269\u5c55\u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884cn\u6b65\u626b\u63cf\u6216\u8005\u5377\u79ef\u5904\u7406NLP\u7684\u6269\u5c55\u6a21\u578b\uff0c\u672c\u6587\u6240\u8bf4\u7684\u591a\u56e0\u5b50\u65f6\u95f4\u5e8f\u5217\u662f\u4f7f\u7528\u5982\u4e0b\u7684\u62d3\u6251\u7ed3\u6784\u6784\u5efa\u7684\u3002\u8fd9\u4e00\u90e8\u5206\u6211\u6682\u65f6\u6ca1\u6709\u627e\u5230\u597d\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u7b28\u6cd5\u5b50\uff0c\u663e\u5f0f\u914d\u7f6e\u591aGPU\u663e\u5b58\u8dd1\u5230512\u6ee4\u6ce2\u5668\u3002"}, {"cell_type": "code", "collapsed": false, "id": "C8DA04D6AD4F4551865667E19795BED7", "input": "for tp in range(in_length):\n    with tf.variable_scope('previous') as scope_previous:\n        if tp == 0:\n            tmp = Multi_CGRUs(X, parameter, layers, prefix='previous_part_for_n_steps_')\n            #print(tp)\n        else:\n            scope_previous.reuse_variables()\n            tmp = Multi_CGRUs(tmp, parameter, layers, prefix='previous_part_for_n_steps_')\n            #print (tp)\nPrevious_Part = tmp", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "3E19D115F5B943D49B178DB758B4CE73", "input": "tape = tf.Variable(tf.zeros(shape=[batch_size, in_length, w, in_width], dtype=tf.float32), trainable=False)\n\nfor tl in range(in_length):\n    with tf.variable_scope('latter') as scope_latter:\n\n        if tl == 0:\n            state_l = Multi_CGRUDs(Previous_Part, tape, parameter, layers, prefix='latter_part_for_n_steps_')\n            tmp_1 = tf.split(value=state_l, num_or_size_splits=in_length, axis=1)[0]\n            tmp_1 = tf.split(value=tmp_1, num_or_size_splits=w, axis=2)[0]\n            tmp_1 = tf.concat(values=[tmp_1]+[tf.zeros_like(tmp_1)]*(w-1), axis=2)\n            tmp_2 = tf.split(value=tape, num_or_size_splits=in_length, axis=1)\n            tmp_2[tl] = tmp_1\n            tape = tf.concat(values=tmp_2, axis=1)\n            #print (tl)\n        \n        else:\n            scope_latter.reuse_variables()\n            state_l = Multi_CGRUDs(state_l, tape, parameter, layers, prefix='latter_part_for_n_steps_')\n            tmp_1 = tf.split(value=state_l, num_or_size_splits=in_length, axis=1)[tl]\n            tmp_1 = tf.split(value=tmp_1, num_or_size_splits=w, axis=2)[0]\n            tmp_1 = tf.concat(values=[tmp_1]+[tf.zeros_like(tmp_1)]*(w-1), axis=2)\n            tmp_2 = tf.split(value=tape, num_or_size_splits=in_length, axis=1)\n            tmp_2[tl] = tmp_1\n            tape = tf.concat(values=tmp_2, axis=1)\n            #print (tl)\nlen(tf.trainable_variables())", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}