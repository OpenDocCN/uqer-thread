{"metadata": {"signature": "sha256:2f1c7ed33c928bc683be702861f43ee403b1ac3d0476251c1367dc79c76a0be0"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "E23BDF017EF2409F8E66567C28BC378F", "metadata": {}, "source": "# tensorflow\u7b14\u8bb04 CNN+SVC"}, {"cell_type": "markdown", "id": "9CF9D71180F74CB08B30CC78A5A7387F", "metadata": {}, "source": "\u7ed3\u6784 5\u5c42\u5377\u79ef - 3\u5c42\u5168\u8fde\u63a5 \u4f7f\u7528SVM \u53d6\u4ee3softmax\u8fdb\u884c\u9884\u6d4b\uff1b\n\u8ba1\u7b97\u91cf\u6709\u70b9\u5927\uff0c\u5927\u5bb6\u770b\u770b\u5373\u53ef\u3002\n\u5377\u79ef\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u53c2\u8003AlexNet\n\u8bdd\u8bf4\u6211\u662f\u5148\u5199\u5b8c\u4ee3\u7801\u624d\u770b\u7684AlexNet\u7684\u8bba\u6587\u548c\u535a\u5ba2\uff0c\u8fd9\u662f\u6076\u4e60\uff0c\u8981\u6539\uff01\uff01\n[\u5411\u91cf\u673a\u4e0e\u903b\u8f91\u56de\u5f52\u6bd4\u8f83\u53c2\u89c1](https://uqer.datayes.com/community/share/58562a9f6a5e6d0052291ebe)"}, {"cell_type": "markdown", "id": "5BC094E64F4441B58C10691E6D6B1E72", "metadata": {}, "source": "\u94fe\u63a5\uff1ahttp://pan.baidu.com/s/1eSsUTDS \u5bc6\u7801\uff1ajw81\nfac\u6570\u636e\u4e3a\u4f18\u77ff\u65e5\u7ebf\u591a\u56e0\u5b50\u6570\u636e"}, {"cell_type": "code", "collapsed": false, "id": "90338DA6AAAE43CA89601722365CE7B3", "input": "%%time\nimport numpy as np\nimport matplotlib.pylab as plt\n%matplotlib inline\nimport tensorflow as tf\nfrom sklearn.cross_validation import train_test_split\n\nfac = np.load('F:/Quotes/fac16.npy').astype(np.float32)\nret = np.load('F:/Quotes/ret16.npy').astype(np.float32)\n\ntrain_X, test_X, train_Y, test_Y = train_test_split(fac, ret, test_size= 0.4)\nprint ('\u8bad\u7ec3\u96c6/\u603b\u6570\u636e\u96c6 %.3f'%(len(train_X)/len(fac)))", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "FBB76519A33549FA8AF1580090D8AA07", "input": "# Parameters\nlearning_rate = 0.001 # \u5b66\u4e60\u901f\u7387\uff0c\ntraining_iters = 20 # \u8bad\u7ec3\u6b21\u6570\nbatch_size = 1024 # \u6bcf\u6b21\u8ba1\u7b97\u6570\u91cf \u6279\u6b21\u5927\u5c0f\ndisplay_step = 10 # \u663e\u793a\u6b65\u957f\n# Network Parameters\nn_input = 40*17 # 40\u5929\u00d717\u591a\u56e0\u5b50\nn_classes = 7 # \u6839\u636e\u6da8\u8dcc\u5e45\u5ea6\u5206\u62107\u7c7b\u522b\n# \u8fd9\u91cc\u6ce8\u610f\u8981\u4f7f\u7528 one-hot\u683c\u5f0f\uff0c\u4e5f\u5c31\u662f\u5982\u679c\u5206\u7c7b\u59823\u7c7b -1,0,1 \u5219\u9700\u89813\u5217\u6765\u8868\u8fbe\u8fd9\u4e2a\u5206\u7c7b\u7ed3\u679c\uff0c3\u7c7b\u662f-1 0 1 \u7136\u540e\u662f\u54ea\u7c7b\uff0c\u54ea\u7c7b\u90a3\u4e00\u884c\u4e3a1 \u5426\u5219\u4e3a0\ndropout = 0.5# Dropout, probability to keep units\n# tensorflow \u56fe Graph \u8f93\u5165 input\uff0c\u8fd9\u91cc\u7684\u5360\u4f4d\u7b26\u5747\u4e3a\u8f93\u5165\nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n# 2 \u5c42 CNN \u63d0\u53d6\u7279\u5f81\u5411\u91cf\ndef CNN_Net_two(x,weights,biases,dropout=0.8,m=1):\n    # layer hidden 1\n    x = tf.reshape(x, shape=[-1,40,17,1])\n    x = tf.nn.conv2d(x, weights['wc1'], strides=[1,m,m,1],padding='SAME')\n    x = tf.nn.bias_add(x,biases['bc1'])\n    x = tf.nn.relu(x)\n    x = tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.001/9.0)\n    x = tf.nn.dropout(x,0.3)\n    \n    # layer hidden 2\n    x = tf.nn.conv2d(x, weights['wc2'], strides=[1,m,m,1],padding='SAME')\n    x = tf.nn.bias_add(x,biases['bc2'])\n    x = tf.nn.relu(x)\n    x = tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.001/9.0)\n    x = tf.nn.dropout(x,0.3)\n    \n    # layer hidden 3\n    x = tf.nn.conv2d(x, weights['wc3'], strides=[1,m,m,1],padding='SAME')\n    x = tf.nn.bias_add(x,biases['bc3'])\n    x = tf.nn.relu(x)\n    x = tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.001/9.0)\n    x = tf.nn.dropout(x,0.3)\n    \n    # layer hidden 4\n    x = tf.nn.conv2d(x, weights['wc4'], strides=[1,m,m,1],padding='SAME')\n    x = tf.nn.bias_add(x,biases['bc4'])\n    x = tf.nn.relu(x)\n    x = tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.001/9.0)\n    x = tf.nn.dropout(x,0.3)\n    \n    # layer hidden 5\n    x = tf.nn.conv2d(x, weights['wc5'], strides=[1,m,m,1],padding='SAME')\n    x = tf.nn.bias_add(x,biases['bc5'])\n    x = tf.nn.relu(x)\n    x = tf.nn.local_response_normalization(x, depth_radius=5, bias=1.0, alpha=0.001/9.0)\n    x = tf.nn.dropout(x,0.3)\n    #print (x.get_shape().as_list())\n    \n    # \u5168\u8fde\u63a5\u5c421\n    x = tf.reshape(x,[-1,weights['wd1'].get_shape().as_list()[0]])\n    x = tf.add(tf.matmul(x,weights['wd1']),biases['bd1'])\n    x = tf.nn.relu(x)\n    x = tf.nn.dropout(x,dropout)\n    #print (x.get_shape().as_list())\n    \n    # \u5168\u8fde\u63a5\u5c422\n    x = tf.reshape(x,[-1,weights['wd2'].get_shape().as_list()[0]])\n    x = tf.add(tf.matmul(x,weights['wd2']),biases['bd2'])\n    x = tf.nn.relu(x)\n    x = tf.nn.dropout(x,dropout)\n    #print (x.get_shape().as_list())\n    \n    # \u5168\u8fde\u63a5\u5c423\n    x = tf.reshape(x,[-1,weights['wd3'].get_shape().as_list()[0]])\n    x = tf.add(tf.matmul(x,weights['wd3']),biases['bd3'])\n    x = tf.nn.relu(x)\n    x = tf.nn.dropout(x,dropout)\n    #print (x.get_shape().as_list())\n    \n    \n    t = tf.add(tf.matmul(x,weights['out']),biases['out'])\n    #print (t.get_shape().as_list())\n    # \u8fd4\u56de\u4e24\u4e2a\u6570\u503c\uff0ct\u7528\u4e8esoftmax\u5206\u7c7b\uff0cx\u7528\u4e8e\u63d0\u53d6CNN\u5904\u7406\u7684\u6570\u636e\uff0c\u4e5f\u5c31\u662f\u7ecf\u8fc7\u5377\u79ef\u5904\u7406\u7684\u7279\u5f81\u5411\u91cf\u3002\n    return t,x\n\n# Store layers weight & bias\nweights = {\n    'wc1': tf.Variable(tf.random_normal([10, 5, 1, 64])),\n    'wc2': tf.Variable(tf.random_normal([10, 5, 64, 128])),\n    'wc3': tf.Variable(tf.random_normal([10, 5, 128, 256])),\n    'wc4': tf.Variable(tf.random_normal([10, 5, 256, 512])),\n    'wc5': tf.Variable(tf.random_normal([10, 5, 512, 1024])),\n    'wd1': tf.Variable(tf.random_normal([40*17*1024, 1024])),\n    'wd2': tf.Variable(tf.random_normal([1024, 256])),\n    'wd3': tf.Variable(tf.random_normal([256, 32])),\n    'out': tf.Variable(tf.random_normal([32, n_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([64])),\n    'bc2': tf.Variable(tf.random_normal([128])),\n    'bc3': tf.Variable(tf.random_normal([256])),\n    'bc4': tf.Variable(tf.random_normal([512])),\n    'bc5': tf.Variable(tf.random_normal([1024])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'bd2': tf.Variable(tf.random_normal([256])),\n    'bd3': tf.Variable(tf.random_normal([32])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n# \u6a21\u578b\u4f18\u5316\npred,tmp = CNN_Net_two(x,weights,biases,dropout=keep_prob)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(pred,1),tf.arg_max(y,1))\n# tf.argmax(input,axis=None) \u7531\u4e8e\u6807\u7b7e\u7684\u6570\u636e\u683c\u5f0f\u662f -1 0 1 3\u5217\uff0c\u8be5\u8bed\u53e5\u662f\u8868\u793a\u8fd4\u56de\u503c\u6700\u5927\u4e5f\u5c31\u662f1\u7684\u7d22\u5f15\uff0c\u4e24\u4e2a\u7d22\u5f15\u76f8\u540c\u5219\u662f\u9884\u6d4b\u6b63\u786e\u3002\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n# \u66f4\u6539\u6570\u636e\u683c\u5f0f\uff0c\u964d\u4f4e\u5747\u503c\ninit = tf.global_variables_initializer()", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "E720ED4AD8D6442E9CFEB5EE3FA3E66F", "metadata": {}, "source": "### \u8ba1\u7b97\u4fdd\u5b58\u6a21\u578b"}, {"cell_type": "code", "collapsed": false, "id": "FACA382668DF49FA809DC02FFB567567", "input": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(init)\n    # for step in range(300):\n    for step in range(1):\n        trl=int(len(train_X)/batch_size)\n        for i in range(trl):\n            print (i,'--',trl)\n            batch_x = train_X[i*batch_size:(i+1)*batch_size]\n            batch_y = train_Y[i*batch_size:(i+1)*batch_size]\n            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y,keep_prob:0.5})\n        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,y: batch_y,keep_prob: 1.})\n        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.5f}\".format(acc))\n    save_path = saver.save(sess,'F:/Quotes/test_var.ckpt')\n    print ('\u4fdd\u6301\u53d8\u91cf')\n    print(\"Optimization Finished!\")   \n    sess.close()", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "39FF0C1DA11543578F4A06CF5575CD1B", "metadata": {}, "source": "### \u8bfb\u53d6\u6a21\u578b\uff0c\u8fdb\u884c\u9884\u6d4b"}, {"cell_type": "code", "collapsed": false, "id": "FAD77A9A8390490F921A21BAF7B22672", "input": "saver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(init)\n    saver.restore(sess,'F:/Quotes/test_var.ckpt')\n    trainX_Convolution = sess.run(tmp, feed_dict={x:train_X, keep_prob:1.})\n    # \u7ecf\u8fc7\u5377\u79ef\u5904\u7406\u7684\u7279\u5f81\u5411\u91cf\n    nn_score = sess.run(accuracy,feed_dict={x:train_X, keep_prob:1.})\n    nn_score1 = sess.run(accuracy,feed_dict={x:test_X, keep_prob:1.})\n    print(nn_score,'---',nn_score1)\n    sess.close()", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "C775FC7EE3854A3B821DA24700924A43", "metadata": {}, "source": "### one-hot\u5411\u91cf\u8f6c\u6362\u4e3a\u5217\u5411\u91cf\u3002"}, {"cell_type": "code", "collapsed": false, "id": "ED42F590DA054E0DBC85986459BFB6A7", "input": "# train_Y \nol_train_Y = []\nfor i in range(len(train_Y)):\n    t = train_Y[i]\n    arg = np.argmax(t)\n    ol_train_Y.append(arg)\n    \n# softmax_pred \nol_softmax_pred = []\nfor i in range(len(softmax_pred)):\n    t = softmax_pred [i]\n    arg = np.argmax(t)\n    ol_softmax_pred.append(arg)", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "0FF7AFC46E36487B86A48321ABC10BC0", "metadata": {}, "source": "### SVM \u9884\u6d4b"}, {"cell_type": "markdown", "id": "C48EF82B407F42CD8F905841569F883B", "metadata": {}, "source": "from sklearn.svm import SVC\nclf = SVC(C=0.9,gamma=1.0,decision_function_shape='ovo')\nclf.fit(trainX_Convolution, ol_train_Y)\nc = clf.predict(trainX_Convolution)\nprint ('CNN\u9884\u6d4b',(np.corrcoef(a,c)[0][1]))"}], "metadata": {}}]}