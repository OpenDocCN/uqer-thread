{"metadata": {"signature": "sha256:3d8e119798ffc780525e9fee847d233cd3148a96c742d9e2e91788f2bd5dafaf"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "18F843ED8A00412CA300D6DA39A64B2F", "metadata": {}, "source": "# "}, {"cell_type": "markdown", "id": "8F8A897B04BF4AD38CD63EDE97DB123F", "metadata": {}, "source": "\u795e\u7ecf\u7f51\u7edc"}, {"cell_type": "code", "collapsed": false, "id": "73F02368EFE449C19E7CDD3685CEE283", "input": "from keras.models import Sequential, Model\nfrom keras.layers import convolutional, merge, Input, BatchNormalization\nfrom keras.layers.core import Activation, Flatten\nfrom AlphaGo.util import flatten_idx\nfrom AlphaGo.models.nn_util import Bias, NeuralNetBase, neuralnet\nimport numpy as np\n\n\n@neuralnet\nclass CNNPolicy(NeuralNetBase):\n\t\"\"\"uses a convolutional neural network to evaluate the state of the game\n\tand compute a probability distribution over the next action\n\t\"\"\"\n\n\tdef _select_moves_and_normalize(self, nn_output, moves, size):\n\t\t\"\"\"helper function to normalize a distribution over the given list of moves\n\t\tand return a list of (move, prob) tuples\n\t\t\"\"\"\n\t\tif len(moves) == 0:\n\t\t\treturn []\n\t\tmove_indices = [flatten_idx(m, size) for m in moves]\n\t\t# get network activations at legal move locations\n\t\tdistribution = nn_output[move_indices]\n\t\tdistribution = distribution / distribution.sum()\n\t\treturn zip(moves, distribution)\n\n\tdef batch_eval_state(self, states, moves_lists=None):\n\t\t\"\"\"Given a list of states, evaluates them all at once to make best use of GPU\n\t\tbatching capabilities.\n\n\t\tAnalogous to [eval_state(s) for s in states]\n\n\t\tReturns: a parallel list of move distributions as in eval_state\n\t\t\"\"\"\n\t\tn_states = len(states)\n\t\tif n_states == 0:\n\t\t\treturn []\n\t\tstate_size = states[0].size\n\t\tif not all([st.size == state_size for st in states]):\n\t\t\traise ValueError(\"all states must have the same size\")\n\t\t# concatenate together all one-hot encoded states along the 'batch' dimension\n\t\tnn_input = np.concatenate([self.preprocessor.state_to_tensor(s) for s in states], axis=0)\n\t\t# pass all input through the network at once (backend makes use of batches if len(states) is large)\n\t\tnetwork_output = self.forward(nn_input)\n\t\t# default move lists to all legal moves\n\t\tmoves_lists = moves_lists or [st.get_legal_moves() for st in states]\n\t\tresults = [None] * n_states\n\t\tfor i in range(n_states):\n\t\t\tresults[i] = self._select_moves_and_normalize(network_output[i], moves_lists[i], state_size)\n\t\treturn results\n\n\tdef eval_state(self, state, moves=None):\n\t\t\"\"\"Given a GameState object, returns a list of (action, probability) pairs\n\t\taccording to the network outputs\n\n\t\tIf a list of moves is specified, only those moves are kept in the distribution\n\t\t\"\"\"\n\t\ttensor = self.preprocessor.state_to_tensor(state)\n\t\t# run the tensor through the network\n\t\tnetwork_output = self.forward(tensor)\n\t\tmoves = moves or state.get_legal_moves()\n\t\treturn self._select_moves_and_normalize(network_output[0], moves, state.size)\n\n\t@staticmethod\n\tdef create_network(**kwargs):\n\t\t\"\"\"construct a convolutional neural network.\n\n\t\tKeword Arguments:\n\t\t- input_dim:         \tdepth of features to be processed by first layer (no default)\n\t\t- board:             \twidth of the go board to be processed (default 19)\n\t\t- filters_per_layer: \tnumber of filters used on every layer (default 128)\n\t\t- layers:            \tnumber of convolutional steps (default 12)\n\t\t- filter_width_K:    \t(where K is between 1 and <layers>) width of filter on\n\t\t\t\t\t\t\t\tlayer K (default 3 except 1st layer which defaults to 5).\n\t\t\t\t\t\t\t\tMust be odd.\n\t\t\"\"\"\n\t\tdefaults = {\n\t\t\t\"board\": 19,\n\t\t\t\"filters_per_layer\": 128,\n\t\t\t\"layers\": 12,\n\t\t\t\"filter_width_1\": 5\n\t\t}\n\t\t# copy defaults, but override with anything in kwargs\n\t\tparams = defaults\n\t\tparams.update(kwargs)\n\n\t\t# create the network:\n\t\t# a series of zero-paddings followed by convolutions\n\t\t# such that the output dimensions are also board x board\n\t\tnetwork = Sequential()\n\n\t\t# create first layer\n\t\tnetwork.add(convolutional.Convolution2D(\n\t\t\tinput_shape=(params[\"input_dim\"], params[\"board\"], params[\"board\"]),\n\t\t\tnb_filter=params[\"filters_per_layer\"],\n\t\t\tnb_row=params[\"filter_width_1\"],\n\t\t\tnb_col=params[\"filter_width_1\"],\n\t\t\tinit='uniform',\n\t\t\tactivation='relu',\n\t\t\tborder_mode='same'))\n\n\t\t# create all other layers\n\t\tfor i in range(2, params[\"layers\"] + 1):\n\t\t\t# use filter_width_K if it is there, otherwise use 3\n\t\t\tfilter_key = \"filter_width_%d\" % i\n\t\t\tfilter_width = params.get(filter_key, 3)\n\t\t\tnetwork.add(convolutional.Convolution2D(\n\t\t\t\tnb_filter=params[\"filters_per_layer\"],\n\t\t\t\tnb_row=filter_width,\n\t\t\t\tnb_col=filter_width,\n\t\t\t\tinit='uniform',\n\t\t\t\tactivation='relu',\n\t\t\t\tborder_mode='same'))\n\n\t\t# the last layer maps each <filters_per_layer> feature to a number\n\t\tnetwork.add(convolutional.Convolution2D(\n\t\t\tnb_filter=1,\n\t\t\tnb_row=1,\n\t\t\tnb_col=1,\n\t\t\tinit='uniform',\n\t\t\tborder_mode='same'))\n\t\t# reshape output to be board x board\n\t\tnetwork.add(Flatten())\n\t\t# add a bias to each board location\n\t\tnetwork.add(Bias())\n\t\t# softmax makes it into a probability distribution\n\t\tnetwork.add(Activation('softmax'))\n\n\t\treturn network\n\n\n@neuralnet\nclass ResnetPolicy(CNNPolicy):\n\t\"\"\"Residual network architecture as per He at al. 2015\n\t\"\"\"\n\t@staticmethod\n\tdef create_network(**kwargs):\n\t\t\"\"\"construct a convolutional neural network with Resnet-style skip connections.\n\t\tArguments are the same as with the default CNNPolicy network, except the default\n\t\tnumber of layers is 20 plus a new n_skip parameter\n\n\t\tKeword Arguments:\n\t\t- input_dim:         \tdepth of features to be processed by first layer (no default)\n\t\t- board:             \twidth of the go board to be processed (default 19)\n\t\t- filters_per_layer: \tnumber of filters used on every layer (default 128)\n\t\t- layers:            \tnumber of convolutional steps (default 20)\n\t\t- filter_width_K:    \t(where K is between 1 and <layers>) width of filter on\n\t\t\t\t\t\t\t\tlayer K (default 3 except 1st layer which defaults to 5).\n\t\t\t\t\t\t\t\tMust be odd.\n\t\t- n_skip_K:             (where K is as in filter_width_K) number of convolutional\n\t\t\t\t\t\t\t\tlayers to skip with the linear path starting at K. Only valid\n\t\t\t\t\t\t\t\tat K >= 1. (Each layer defaults to 1)\n\n\t\tNote that n_skip_1=s means that the next valid value of n_skip_* is 3\n\n\t\tA diagram may help explain (numbers indicate layer):\n\n\t\t\t1             2              3                   4              5              6\n\t\tI--C -- B -- R -- C -- B -- R -- C -- M -- B -- R -- C -- B -- R -- C -- B -- R -- C -- M  ...  M  -- R -- F -- O\n\t\t\t\\___________________________/ \\____________________________________________________/ \\ ... /\n\t\t\t\t\t[n_skip_1 = 2]                             [n_skip_3 = 3]\n\n\t\tI - input\n\t\tB - BatchNormalization\n\t\tR - ReLU\n\t\tC - Conv2D\n\t\tF - Flatten\n\t\tO - output\n\t\tM - merge\n\n\t\tThe input is always passed through a Conv2D layer, the output of which layer is counted as '1'.\n\t\tEach subsequent [R -- C] block is counted as one 'layer'. The 'merge' layer isn't counted; hence\n\t\tif n_skip_1 is 2, the next valid skip parameter is n_skip_3, which will start at the output\n\t\tof the merge\n\t\t\"\"\"\n\t\tdefaults = {\n\t\t\t\"board\": 19,\n\t\t\t\"filters_per_layer\": 128,\n\t\t\t\"layers\": 20,\n\t\t\t\"filter_width_1\": 5\n\t\t}\n\t\t# copy defaults, but override with anything in kwargs\n\t\tparams = defaults\n\t\tparams.update(kwargs)\n\n\t\t# create the network using Keras' functional API,\n\t\t# since this isn't 'Sequential'\n\t\tmodel_input = Input(shape=(params[\"input_dim\"], params[\"board\"], params[\"board\"]))\n\n\t\t# create first layer\n\t\tconvolution_path = convolutional.Convolution2D(\n\t\t\tinput_shape=(),\n\t\t\tnb_filter=params[\"filters_per_layer\"],\n\t\t\tnb_row=params[\"filter_width_1\"],\n\t\t\tnb_col=params[\"filter_width_1\"],\n\t\t\tinit='uniform',\n\t\t\tactivation='linear',  # relu activations done inside resnet modules\n\t\t\tborder_mode='same')(model_input)\n\n\t\tdef add_resnet_unit(path, K, **params):\n\t\t\t\"\"\"Add a resnet unit to path starting at layer 'K',\n\t\t\tadding as many (ReLU + Conv2D) modules as specified by n_skip_K\n\n\t\t\tReturns new path and next layer index, i.e. K + n_skip_K, in a tuple\n\t\t\t\"\"\"\n\t\t\t# loosely based on https://github.com/keunwoochoi/residual_block_keras\n\t\t\t# (see also keras docs here: http://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers)\n\n\t\t\tblock_input = path\n\t\t\t# use n_skip_K if it is there, default to 1\n\t\t\tskip_key = \"n_skip_%d\" % K\n\t\t\tn_skip = params.get(skip_key, 1)\n\t\t\tfor i in range(n_skip):\n\t\t\t\tlayer = K + i\n\t\t\t\t# add BatchNorm\n\t\t\t\tpath = BatchNormalization()(path)\n\t\t\t\t# add ReLU\n\t\t\t\tpath = Activation('relu')(path)\n\t\t\t\t# use filter_width_K if it is there, otherwise use 3\n\t\t\t\tfilter_key = \"filter_width_%d\" % layer\n\t\t\t\tfilter_width = params.get(filter_key, 3)\n\t\t\t\t# add Conv2D\n\t\t\t\tpath = convolutional.Convolution2D(\n\t\t\t\t\tnb_filter=params[\"filters_per_layer\"],\n\t\t\t\t\tnb_row=filter_width,\n\t\t\t\t\tnb_col=filter_width,\n\t\t\t\t\tinit='uniform',\n\t\t\t\t\tactivation='linear',\n\t\t\t\t\tborder_mode='same')(path)\n\t\t\t# Merge 'input layer' with the path\n\t\t\tpath = merge([block_input, path], mode='sum')\n\t\t\treturn path, K + n_skip\n\n\t\t# create all other layers\n\t\tlayer = 1\n\t\twhile layer < params['layers']:\n\t\t\tconvolution_path, layer = add_resnet_unit(convolution_path, layer, **params)\n\t\tif layer > params['layers']:\n\t\t\tprint \"Due to skipping, ended with {} layers instead of {}\".format(layer, params['layers'])\n\n\t\t# since each layer's activation was linear, need one more ReLu\n\t\tconvolution_path = Activation('relu')(convolution_path)\n\n\t\t# the last layer maps each <filters_per_layer> featuer to a number\n\t\tconvolution_path = convolutional.Convolution2D(\n\t\t\tnb_filter=1,\n\t\t\tnb_row=1,\n\t\t\tnb_col=1,\n\t\t\tinit='uniform',\n\t\t\tborder_mode='same')(convolution_path)\n\t\t# flatten output\n\t\tnetwork_output = Flatten()(convolution_path)\n\t\t# add a bias to each board location\n\t\tnetwork_output = Bias()(network_output)\n\t\t# softmax makes it into a probability distribution\n\t\tnetwork_output = Activation('softmax')(network_output)\n\n\t\treturn Model(input=[model_input], output=[network_output])\n", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "\u6a21\u5757:AlphaGo\u4e0d\u652f\u6301, \u5982\u679c\u60a8\u9700\u8981\u6dfb\u52a0, \u8bf7\u8054\u7cfb\u6211\u4eec.\n\u6a21\u5757:keras\u4e0d\u652f\u6301, \u5982\u679c\u60a8\u9700\u8981\u6dfb\u52a0, \u8bf7\u8054\u7cfb\u6211\u4eec."}, {"output_type": "stream", "stream": "stdout", "text": "\n"}], "trusted": true}], "metadata": {}}]}