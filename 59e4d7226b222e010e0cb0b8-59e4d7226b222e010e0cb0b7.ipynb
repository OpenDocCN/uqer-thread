{"metadata": {"signature": "sha256:fcb3adba9fa0ad20cf2672f5ff9d0a837c520a81882fa2c9a0e0d09c05733981"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "C85E2A678BB74D36932328AC2D1FBB34", "metadata": {}, "source": "# Notebook"}, {"cell_type": "markdown", "id": "1C8389A4A4FD49EC9B593972A72BD36F", "metadata": {}, "source": "![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/daa4805c-b28a-11e7-9497-0242ac140002)"}, {"cell_type": "markdown", "id": "9F9A973A6379479F8724FF9FA571545E", "metadata": {}, "source": "#### \u4e00\u4e2a\u7b80\u5355\u7684DQN\u4e0e\u4f2a\u65e5\u5185\u7b56\u7565\u6846\u67b6\n[github](https://github.com/AlphaSmartDog/DeepLearningNotes/tree/master/Note-5%20Agent%20Thinker)\n- \u672c\u6587\u4f7f\u7528HS300\u6307\u6570\u6570\u636e\n- \u7b80\u5355\u6784\u5efa\u4e00\u4e2a\u73af\u5883\uff0c\u4e00\u4e2a\u865a\u62df\u7684\u4ea4\u6613\u8d26\u6237\uff0c\u53ef\u4ee5\u662f\u80a1\u6307\u671f\u8d27\u6216\u8005\u5546\u54c1\u671f\u8d27\uff0c\u8fdb\u884c\u65e5\u7ebf\u7ea7\u522b\u4ea4\u6613\uff0c\u6bcf\u4ea4\u6613\u65e5\u5f00\u76d8\u4ef7\u4ea4\u5272\uff0c\u6536\u76d8\u4ef7\u8bc4\u4f30\u3002\n- \u8bbe\u8ba1\u51b3\u7b56\u8005\uff0c\u5373\u865a\u62df\u4ea4\u6613\u5458\uff0c\u6bcf\u65e5\u5f00\u76d8\u524d\u9884\u5224\u4eca\u65e5\u8d70\u52bf\uff0c\u5982\u679c\u4eca\u65e5\u6536\u76d8\u4ef7\u9ad8\u4e8e\u5f00\u76d8\u4ef7\u505a\u591a\uff0c\u6536\u76d8\u4ef7\u4f4e\u4e8e\u5f00\u76d8\u4ef7\u505a\u7a7a\uff0c\u5e76\u4e14\u8bbe\u7f6e\u6469\u64e6\u6210\u672c\uff0c\u5f00\u76d8\u6536\u76d8\u4ef7\u5dee\u5c0f\u4e8e\u4e00\u5b9a\u503c\u7a7a\u4ed3\u3002"}, {"cell_type": "markdown", "id": "483CCC5B20414456A711F439BE3DBAD3", "metadata": {}, "source": "\u4e0d\u540c\u4e8e\u6b64\u524d\u535a\u5ba2\u4ecb\u7ecd\u7684RNN\u7528\u4e8e\u80a1\u7968\u7684\u9884\u6d4b\uff0cRL\u5728\u6bcf\u4e2astate\uff08\u6216\u5e27\uff09\u9884\u6d4b\u7684\u662f\u6298\u6263\u672a\u6765\u9884\u671f\u6536\u76ca\uff08\u6b64\u5904\u53ef\u4ee5\u7406\u89e3\u4e3a\u8fdc\u671f\u6807\u6298\u73b0\uff09\u3002\u5bf9\u6bd4RNN\u6a21\u578b\uff0cRL\u6a21\u578b\u5728\u5bf9\u80a1\u7968\u5e02\u573a\u672a\u6765\u8d70\u52bf\u7684\u9884\u6d4b\u65b9\u9762\u66f4\u52a0\u5bbd\u677e\u6216\u66f4\u9c81\u68d2\u3002RL\u5728$s_t$\u4ea7\u751f\u7684\u9884\u6d4b\u7531\u672a\u6765\u9884\u671f\u6536\u76ca\u7531\u771f\u5b9e\u5956\u52b1$R_t$ \u548c\u903c\u8fd1\u51fd\u6570\u6a21\u578b\u4f30\u8ba1\u672a\u6765\u4e00\u6bb5\u65f6\u95f4\u6298\u6263\u6536\u76ca\u7ec4\u6210\u3002\u8fd9\u76f8\u6bd4\u4e8eRNN\u66f4\u52a0\u5bbd\u677e\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u80fd\u671f\u671b\u80a1\u7968\u5e02\u573a\u7684\u6a21\u5f0f\u6216\u8005\u72b6\u6001\u4e0e\u672a\u6765\u80a1\u4ef7\u8d70\u52bf\u5b58\u5728\u4e25\u683c\u7684\u51fd\u6570\u5173\u7cfb\uff0c\u4e5f\u5c31\u662f\u4e0d\u5e94\u8be5\u671f\u671b\u5728$t$ \u65f6\u523b\u7684\u4e8b\u4ef6\u7cbe\u51c6\u7684\u53cd\u9988\u5728$t+k$ \u65f6\u523b\u7684\u80a1\u4ef7\u4e0a\u9762\u3002\u6b64\u5916\u901a\u5e38\u5728\u5b9e\u9645\u4f7f\u7528RL\u6a21\u578b\u7684\u65f6\u5019\u4e0d\u9700\u8981\u7279\u522b\u7cbe\u786e\u7684\u6536\u655b\uff0c\u901a\u5e38\u53ea\u8981\u8fbe\u5230\u4e00\u5b9a\u7684\u51c6\u786e\u7a0b\u5ea6\u6216\u4e00\u5b9a\u7684\u6536\u655b\u8303\u56f4\u5373\u53ef\u3002\n\nRL\u5728\u72b6\u6001state($s_t$) \u9884\u6d4b\u662f\u5e9e\u5927\u4f46\u6709\u9650\u9884\u671f\u6536\u76ca\u6298\u73b0\u503c\u3002\n\n$predict_t= G_t \\doteq \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\\\=R_t+ \\sum_{k=1}^{\\infty} \\gamma^k R_{t+k+1} $\n\nRNN\u5728\u72b6\u6001$s_t$ \u9884\u6d4b\u7684\u662f\u672a\u6765\u67d0\u4e2a\u8282\u70b9\u6216\u8005\u8282\u70b9\u5f88\u5c0f\u6ce2\u52a8\u8303\u56f4\u5185\u7684\u9884\u671f\u6536\u76ca\u3002\n\n$predict_t= R_{t+k} + Var(R_{t+k}) $\n\n\u7136\u800c\u9488\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u8bbe\u8ba1\u7684RL\u7cfb\u7edf\uff0c\u5927\u591a\u6570\u5f3a\u5236\u5047\u8bbe\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(Markov Decision Processes)\uff0c\u5f53\u7136\u8fd9\u5728\u5de5\u7a0b\u5e94\u7528\u4e2d\u5177\u6709\u6781\u597d\u7684\u6548\u679c\uff0c\u901a\u5e38\u975e\u9a6c\u5c14\u79d1\u592b\u5c5e\u6027\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u901a\u8fc7MDP\u6709\u6548\u7684\u8fd1\u4f3c\u3002\u7136\u800c\u4e0d\u540c\u4e8e\u5de5\u4e1a\u5e94\u7528\u573a\u666f\uff0c\u865a\u62df\u4ea4\u6613\u5458(Agent)\u5728A\u80a1\u5e02\u573a\u9762\u5bf9\u7684\u662f\u4e00\u4e2a\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\uff0c\u4e0d\u540c\u4e8e\u524d\u9762\u7684\u5de5\u4e1a\u5e94\u7528\uff0cA\u80a1\u5e02\u573aEnvironment \u7684\u89c4\u5219\u662f\u52a8\u6001\u53d8\u52a8\u7684\uff0c\u5e76\u4e14\u8fd9\u79cd\u53d8\u52a8\u901a\u5e38\u65e0\u6cd5\u4ece\u76d8\u9762\u6570\u636e\u9884\u6d4b\u3002\u6b64\u5916\uff0cRL\u7cfb\u7edf\u4e00\u822c\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9action\u6765\u63a7\u5236\u6216\u5f71\u54cd\u60c5\u666f\u53d1\u5c55\uff0c\u5982AlphaGo\u3001DQN\u7b49\u5728\u56f4\u68cb\u3001\u6e38\u620f\u4e0a\u9762\u7684\u52a8\u4f5c\u53ef\u4ee5\u5207\u5b9e\u5f71\u54cdRL\u51b3\u7b56\u7cfb\u7edf\u672a\u6765\u63a5\u53d7\u7684state\uff0c \u4f46\u662fAgent\u7684\u884c\u4e3a\u96be\u4ee5\u5f71\u54cdA\u80a1\u5e02\u573a\uff0c\u6216\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u884c\u4e3a\u5f71\u54cd\u3002\u4e5f\u5c31\u662f\uff0cAgent\u5bf9action\u7684\u51b3\u7b56\u53ea\u80fd\u5f71\u54cd\u5230\u672a\u6765\u83b7\u5f97\u5956\u52b1\uff0c\u800c\u65e0\u6cd5\u5bf9\u4ea4\u4e92\u73af\u5883\u4ea7\u751f\u5f71\u54cd\u3002\n\nAgent for game\n\n$action \\to reward\\ and \\ next\\ state$\n\nAgent for A\u80a1\n\n$action \\to reward$\n"}, {"cell_type": "markdown", "id": "9623AD87610B4F43956992D8B926124A", "metadata": {}, "source": "## 1 \u4ea4\u6613\u8d26\u6237-\u865a\u62df\u73af\u5883\u6784\u5efa"}, {"cell_type": "code", "collapsed": false, "id": "8BD6E96AEBBE41178BA04F3C7705886C", "input": "import numpy as np\nimport pandas as pd\nimport talib \n\ndef fix_data(path):\n    tmp = pd.read_csv(path, encoding=\"gbk\", engine='python')\n    tmp.rename(columns={'Unnamed: 0':'trading_time'}, inplace=True)\n    tmp['trading_point'] = pd.to_datetime(tmp.trading_time)\n    del tmp['trading_time']\n    tmp.set_index(tmp.trading_point, inplace=True)\n    return tmp\n\ndef High_2_Low(tmp, freq):\n    \"\"\"\u5904\u7406\u4eceRiceQuant\u4e0b\u8f7d\u7684\u5206\u949f\u7ebf\u6570\u636e\uff0c\n    \u4ece\u5206\u949f\u7ebf\u6570\u636e\u5408\u6210\u4f4e\u9891\u6570\u636e\n    2017-08-11    \n    \"\"\"\n    # \u5206\u522b\u5904\u7406bar\u6570\u636e\n    tmp_open = tmp['open'].resample(freq).ohlc()\n    tmp_open = tmp_open['open'].dropna()\n\n    tmp_high = tmp['high'].resample(freq).ohlc()\n    tmp_high = tmp_high['high'].dropna()\n\n    tmp_low = tmp['low'].resample(freq).ohlc()\n    tmp_low = tmp_low['low'].dropna()\n\n    tmp_close = tmp['close'].resample(freq).ohlc()\n    tmp_close = tmp_close['close'].dropna()\n\n    tmp_price = pd.concat([tmp_open, tmp_high, tmp_low, tmp_close], axis=1)\n    \n    # \u5904\u7406\u6210\u4ea4\u91cf\n    tmp_volume = tmp['volume'].resample(freq).sum()\n    tmp_volume.dropna(inplace=True)\n    \n    return pd.concat([tmp_price, tmp_volume], axis=1)\n\ndef get_factors(index, \n                Open, \n                Close, \n                High, \n                Low, \n                Volume,\n                rolling = 26,\n                drop=False, \n                normalization=True):\n    \n    tmp = pd.DataFrame()\n    tmp['tradeTime'] = index\n    \n    #\u7d2f\u79ef/\u6d3e\u53d1\u7ebf\uff08Accumulation / Distribution Line\uff0c\u8be5\u6307\u6807\u5c06\u6bcf\u65e5\u7684\u6210\u4ea4\u91cf\u901a\u8fc7\u4ef7\u683c\u52a0\u6743\u7d2f\u8ba1\uff0c\n    #\u7528\u4ee5\u8ba1\u7b97\u6210\u4ea4\u91cf\u7684\u52a8\u91cf\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['AD'] = talib.AD(High, Low, Close, Volume)\n\n    # \u4f73\u5e86\u6307\u6807\uff08Chaikin Oscillator\uff09\uff0c\u8be5\u6307\u6807\u57fa\u4e8eAD\u66f2\u7ebf\u7684\u6307\u6570\u79fb\u52a8\u5747\u7ebf\u800c\u8ba1\u7b97\u5f97\u5230\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['ADOSC'] = talib.ADOSC(High, Low, Close, Volume, fastperiod=3, slowperiod=10)\n\n    # \u5e73\u5747\u52a8\u5411\u6307\u6570\uff0cDMI\u56e0\u5b50\u7684\u6784\u6210\u90e8\u5206\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['ADX'] = talib.ADX(High, Low, Close,timeperiod=14)\n\n    # \u76f8\u5bf9\u5e73\u5747\u52a8\u5411\u6307\u6570\uff0cDMI\u56e0\u5b50\u7684\u6784\u6210\u90e8\u5206\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['ADXR'] = talib.ADXR(High, Low, Close,timeperiod=14)\n\n    # \u7edd\u5bf9\u4ef7\u683c\u632f\u8361\u6307\u6570\n    tmp['APO'] = talib.APO(Close, fastperiod=12, slowperiod=26)\n\n    # Aroon\u901a\u8fc7\u8ba1\u7b97\u81ea\u4ef7\u683c\u8fbe\u5230\u8fd1\u671f\u6700\u9ad8\u503c\u548c\u6700\u4f4e\u503c\u4ee5\u6765\u6240\u7ecf\u8fc7\u7684\u671f\u95f4\u6570\uff0c\u5e2e\u52a9\u6295\u8d44\u8005\u9884\u6d4b\u8bc1\u5238\u4ef7\u683c\u4ece\u8d8b\u52bf\u5230\u533a\u57df\u533a\u57df\u6216\u53cd\u8f6c\u7684\u53d8\u5316\uff0c\n    #Aroon\u6307\u6807\u5206\u4e3aAroon\u3001AroonUp\u548cAroonDown3\u4e2a\u5177\u4f53\u6307\u6807\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['AROONDown'], tmp['AROONUp'] = talib.AROON(High, Low,timeperiod=14)\n    tmp['AROONOSC'] = talib.AROONOSC(High, Low,timeperiod=14)\n\n    # \u5747\u5e45\u6307\u6807\uff08Average TRUE Ranger\uff09\uff0c\u53d6\u4e00\u5b9a\u65f6\u95f4\u5468\u671f\u5185\u7684\u80a1\u4ef7\u6ce2\u52a8\u5e45\u5ea6\u7684\u79fb\u52a8\u5e73\u5747\u503c\uff0c\n    #\u662f\u663e\u793a\u5e02\u573a\u53d8\u5316\u7387\u7684\u6307\u6807\uff0c\u4e3b\u8981\u7528\u4e8e\u7814\u5224\u4e70\u5356\u65f6\u673a\u3002\u5c5e\u4e8e\u8d85\u4e70\u8d85\u5356\u578b\u56e0\u5b50\u3002\n    tmp['ATR14']= talib.ATR(High, Low, Close, timeperiod=14)\n    tmp['ATR6']= talib.ATR(High, Low, Close, timeperiod=6)\n\n    # \u5e03\u6797\u5e26\n    tmp['Boll_Up'],tmp['Boll_Mid'],tmp['Boll_Down']= talib.BBANDS(Close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n\n    # \u5747\u52bf\u6307\u6807\n    tmp['BOP'] = talib.BOP(Open, High, Low, Close)\n\n    #5\u65e5\u987a\u52bf\u6307\u6807\uff08Commodity Channel Index\uff09\uff0c\u4e13\u95e8\u6d4b\u91cf\u80a1\u4ef7\u662f\u5426\u5df2\u8d85\u51fa\u5e38\u6001\u5206\u5e03\u8303\u56f4\u3002\u5c5e\u4e8e\u8d85\u4e70\u8d85\u5356\u578b\u56e0\u5b50\u3002\n    tmp['CCI5'] = talib.CCI(High, Low, Close, timeperiod=5)\n    tmp['CCI10'] = talib.CCI(High, Low, Close, timeperiod=10)\n    tmp['CCI20'] = talib.CCI(High, Low, Close, timeperiod=20)\n    tmp['CCI88'] = talib.CCI(High, Low, Close, timeperiod=88)\n\n    # \u94b1\u5fb7\u52a8\u91cf\u6446\u52a8\u6307\u6807\uff08Chande Momentum Osciliator\uff09\uff0c\u4e0e\u5176\u4ed6\u52a8\u91cf\u6307\u6807\u6446\u52a8\u6307\u6807\u5982\u76f8\u5bf9\u5f3a\u5f31\u6307\u6807\uff08RSI\uff09\u548c\u968f\u673a\u6307\u6807\uff08KDJ\uff09\u4e0d\u540c\uff0c\n    # \u94b1\u5fb7\u52a8\u91cf\u6307\u6807\u5728\u8ba1\u7b97\u516c\u5f0f\u7684\u5206\u5b50\u4e2d\u91c7\u7528\u4e0a\u6da8\u65e5\u548c\u4e0b\u8dcc\u65e5\u7684\u6570\u636e\u3002\u5c5e\u4e8e\u8d85\u4e70\u8d85\u5356\u578b\u56e0\u5b50\n    tmp['CMO_Close'] = talib.CMO(Close,timeperiod=14)\n    tmp['CMO_Open'] = talib.CMO(Close,timeperiod=14)\n\n    # DEMA\u53cc\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\n    tmp['DEMA6'] = talib.DEMA(Close, timeperiod=6)\n    tmp['DEMA12'] = talib.DEMA(Close, timeperiod=12)\n    tmp['DEMA26'] = talib.DEMA(Close, timeperiod=26)\n\n    # DX \u52a8\u5411\u6307\u6570\n    tmp['DX'] = talib.DX(High, Low, Close,timeperiod=14)\n\n    # EMA \u6307\u6570\u79fb\u52a8\u5e73\u5747\u7ebf\n    tmp['EMA6'] = talib.EMA(Close, timeperiod=6)\n    tmp['EMA12'] = talib.EMA(Close, timeperiod=12)\n    tmp['EMA26'] = talib.EMA(Close, timeperiod=26)\n\n    # KAMA \u9002\u5e94\u6027\u79fb\u52a8\u5e73\u5747\u7ebf\n    tmp['KAMA'] = talib.KAMA(Close, timeperiod=30)\n\n    # MACD\n    tmp['MACD_DIF'],tmp['MACD_DEA'],tmp['MACD_bar'] = talib.MACD(Close, fastperiod=12, slowperiod=24, signalperiod=9)\n\n    # \u4e2d\u4f4d\u6570\u4ef7\u683c \u4e0d\u77e5\u9053\u662f\u4ec0\u4e48\u610f\u601d\n    tmp['MEDPRICE'] = talib.MEDPRICE(High, Low)\n\n    # \u8d1f\u5411\u6307\u6807 \u8d1f\u5411\u8fd0\u52a8\n    tmp['MiNUS_DI'] = talib.MINUS_DI(High, Low, Close,timeperiod=14)\n    tmp['MiNUS_DM'] = talib.MINUS_DM(High, Low,timeperiod=14)\n\n    # \u52a8\u91cf\u6307\u6807\uff08Momentom Index\uff09\uff0c\u52a8\u91cf\u6307\u6570\u4ee5\u5206\u6790\u80a1\u4ef7\u6ce2\u52a8\u7684\u901f\u5ea6\u4e3a\u76ee\u7684\uff0c\u7814\u7a76\u80a1\u4ef7\u5728\u6ce2\u52a8\u8fc7\u7a0b\u4e2d\u5404\u79cd\u52a0\u901f\uff0c\n    #\u51cf\u901f\uff0c\u60ef\u6027\u4f5c\u7528\u4ee5\u53ca\u80a1\u4ef7\u7531\u9759\u5230\u52a8\u6216\u7531\u52a8\u8f6c\u9759\u7684\u73b0\u8c61\u3002\u5c5e\u4e8e\u8d8b\u52bf\u578b\u56e0\u5b50\n    tmp['MOM'] = talib.MOM(Close, timeperiod=10)\n\n    # \u5f52\u4e00\u5316\u5e73\u5747\u503c\u8303\u56f4\n    tmp['NATR'] = talib.NATR(High, Low, Close,timeperiod=14)\n\n    # OBV \t\u80fd\u91cf\u6f6e\u6307\u6807\uff08On Balance Volume\uff0cOBV\uff09\uff0c\u4ee5\u80a1\u5e02\u7684\u6210\u4ea4\u91cf\u53d8\u5316\u6765\u8861\u91cf\u80a1\u5e02\u7684\u63a8\u52a8\u529b\uff0c\n    #\u4ece\u800c\u7814\u5224\u80a1\u4ef7\u7684\u8d70\u52bf\u3002\u5c5e\u4e8e\u6210\u4ea4\u91cf\u578b\u56e0\u5b50\n    tmp['OBV'] = talib.OBV(Close, Volume)\n\n    # PLUS_DI \u66f4\u5411\u6307\u793a\u5668\n    tmp['PLUS_DI'] = talib.PLUS_DI(High, Low, Close,timeperiod=14)\n    tmp['PLUS_DM'] = talib.PLUS_DM(High, Low, timeperiod=14)\n\n    # PPO \u4ef7\u683c\u632f\u8361\u767e\u5206\u6bd4\n    tmp['PPO'] = talib.PPO(Close, fastperiod=6, slowperiod= 26, matype=0)\n\n    # ROC 6\u65e5\u53d8\u52a8\u901f\u7387\uff08Price Rate of Change\uff09\uff0c\u4ee5\u5f53\u65e5\u7684\u6536\u76d8\u4ef7\u548cN\u5929\u524d\u7684\u6536\u76d8\u4ef7\u6bd4\u8f83\uff0c\n    #\u901a\u8fc7\u8ba1\u7b97\u80a1\u4ef7\u67d0\u4e00\u6bb5\u65f6\u95f4\u5185\u6536\u76d8\u4ef7\u53d8\u52a8\u7684\u6bd4\u4f8b\uff0c\u5e94\u7528\u4ef7\u683c\u7684\u79fb\u52a8\u6bd4\u8f83\u6765\u6d4b\u91cf\u4ef7\u4f4d\u52a8\u91cf\u3002\u5c5e\u4e8e\u8d85\u4e70\u8d85\u5356\u578b\u56e0\u5b50\u3002\n    tmp['ROC6'] = talib.ROC(Close, timeperiod=6)\n    tmp['ROC20'] = talib.ROC(Close, timeperiod=20)\n    #12\u65e5\u91cf\u53d8\u52a8\u901f\u7387\u6307\u6807\uff08Volume Rate of Change\uff09\uff0c\u4ee5\u4eca\u5929\u7684\u6210\u4ea4\u91cf\u548cN\u5929\u524d\u7684\u6210\u4ea4\u91cf\u6bd4\u8f83\uff0c\n    #\u901a\u8fc7\u8ba1\u7b97\u67d0\u4e00\u6bb5\u65f6\u95f4\u5185\u6210\u4ea4\u91cf\u53d8\u52a8\u7684\u5e45\u5ea6\uff0c\u5e94\u7528\u6210\u4ea4\u91cf\u7684\u79fb\u52a8\u6bd4\u8f83\u6765\u6d4b\u91cf\u6210\u4ea4\u91cf\u8fd0\u52a8\u8d8b\u5411\uff0c\n    #\u8fbe\u5230\u4e8b\u5148\u63a2\u6d4b\u6210\u4ea4\u91cf\u4f9b\u9700\u7684\u5f3a\u5f31\uff0c\u8fdb\u800c\u5206\u6790\u6210\u4ea4\u91cf\u7684\u53d1\u5c55\u8d8b\u52bf\u53ca\u5176\u5c06\u6765\u662f\u5426\u6709\u8f6c\u52bf\u7684\u610f\u613f\uff0c\n    #\u5c5e\u4e8e\u6210\u4ea4\u91cf\u7684\u53cd\u8d8b\u5411\u6307\u6807\u3002\u5c5e\u4e8e\u6210\u4ea4\u91cf\u578b\u56e0\u5b50\n    tmp['VROC6'] = talib.ROC(Volume, timeperiod=6)\n    tmp['VROC20'] = talib.ROC(Volume, timeperiod=20)\n\n    # ROC 6\u65e5\u53d8\u52a8\u901f\u7387\uff08Price Rate of Change\uff09\uff0c\u4ee5\u5f53\u65e5\u7684\u6536\u76d8\u4ef7\u548cN\u5929\u524d\u7684\u6536\u76d8\u4ef7\u6bd4\u8f83\uff0c\n    #\u901a\u8fc7\u8ba1\u7b97\u80a1\u4ef7\u67d0\u4e00\u6bb5\u65f6\u95f4\u5185\u6536\u76d8\u4ef7\u53d8\u52a8\u7684\u6bd4\u4f8b\uff0c\u5e94\u7528\u4ef7\u683c\u7684\u79fb\u52a8\u6bd4\u8f83\u6765\u6d4b\u91cf\u4ef7\u4f4d\u52a8\u91cf\u3002\u5c5e\u4e8e\u8d85\u4e70\u8d85\u5356\u578b\u56e0\u5b50\u3002\n    tmp['ROCP6'] = talib.ROCP(Close, timeperiod=6)\n    tmp['ROCP20'] = talib.ROCP(Close, timeperiod=20)\n    #12\u65e5\u91cf\u53d8\u52a8\u901f\u7387\u6307\u6807\uff08Volume Rate of Change\uff09\uff0c\u4ee5\u4eca\u5929\u7684\u6210\u4ea4\u91cf\u548cN\u5929\u524d\u7684\u6210\u4ea4\u91cf\u6bd4\u8f83\uff0c\n    #\u901a\u8fc7\u8ba1\u7b97\u67d0\u4e00\u6bb5\u65f6\u95f4\u5185\u6210\u4ea4\u91cf\u53d8\u52a8\u7684\u5e45\u5ea6\uff0c\u5e94\u7528\u6210\u4ea4\u91cf\u7684\u79fb\u52a8\u6bd4\u8f83\u6765\u6d4b\u91cf\u6210\u4ea4\u91cf\u8fd0\u52a8\u8d8b\u5411\uff0c\n    #\u8fbe\u5230\u4e8b\u5148\u63a2\u6d4b\u6210\u4ea4\u91cf\u4f9b\u9700\u7684\u5f3a\u5f31\uff0c\u8fdb\u800c\u5206\u6790\u6210\u4ea4\u91cf\u7684\u53d1\u5c55\u8d8b\u52bf\u53ca\u5176\u5c06\u6765\u662f\u5426\u6709\u8f6c\u52bf\u7684\u610f\u613f\uff0c\n    #\u5c5e\u4e8e\u6210\u4ea4\u91cf\u7684\u53cd\u8d8b\u5411\u6307\u6807\u3002\u5c5e\u4e8e\u6210\u4ea4\u91cf\u578b\u56e0\u5b50\n    tmp['VROCP6'] = talib.ROCP(Volume, timeperiod=6)\n    tmp['VROCP20'] = talib.ROCP(Volume, timeperiod=20)\n\n    # RSI\n    tmp['RSI'] = talib.RSI(Close, timeperiod=14)\n\n    # SAR \u629b\u7269\u7ebf\u8f6c\u5411\n    tmp['SAR'] = talib.SAR(High, Low, acceleration=0.02, maximum=0.2)\n\n    # TEMA\n    tmp['TEMA6'] = talib.TEMA(Close, timeperiod=6)\n    tmp['TEMA12'] = talib.TEMA(Close, timeperiod=12)\n    tmp['TEMA26'] = talib.TEMA(Close, timeperiod=26)\n\n    # TRANGE \u771f\u5b9e\u8303\u56f4\n    tmp['TRANGE'] = talib.TRANGE(High, Low, Close)\n\n    # TYPPRICE \u5178\u578b\u4ef7\u683c\n    tmp['TYPPRICE'] = talib.TYPPRICE(High, Low, Close)\n\n    # TSF \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\n    tmp['TSF'] = talib.TSF(Close, timeperiod=14)\n\n    # ULTOSC \u6781\u9650\u632f\u5b50\n    tmp['ULTOSC'] = talib.ULTOSC(High, Low, Close, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n\n    # \u5a01\u5ec9\u6307\u6807\n    tmp['WILLR'] = talib.WILLR(High, Low, Close, timeperiod=14)\n    \n    # \u6807\u51c6\u5316\n    if normalization:\n        factors_list = tmp.columns.tolist()[1:]\n\n        if rolling >= 26:\n            for i in factors_list:\n                tmp[i] = (tmp[i] - tmp[i].rolling(window=rolling, center=False).mean())\\\n                /tmp[i].rolling(window=rolling, center=False).std()\n        elif rolling < 26 & rolling > 0:\n            print ('Recommended rolling range greater than 26')\n        elif rolling <=0:\n            for i in factors_list:\n                tmp[i] = (tmp[i] - tmp[i].mean())/tmp[i].std()\n            \n    if drop:\n        tmp.dropna(inplace=True)\n        \n    tmp.set_index('tradeTime', inplace=True)\n    \n    return tmp", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "6392AA0A5D5F4AF99FC9F92273CED19A", "input": "tmp = fix_data('HS300.csv')\ntmp = High_2_Low(tmp, '5min')\nDtmp = High_2_Low(tmp, '1d')\n\nIndex = tmp.index\nHigh = tmp.high.values\nLow = tmp.low.values\nClose = tmp.close.values\nOpen = tmp.open.values\nVolume = tmp.volume.values\nfactors = get_factors(Index, Open, Close, High, Low, Volume, rolling = 188, drop=True)\n\nDtmp['returns'] = np.log(Dtmp['close'].shift(-1)/Dtmp['close'])\nDtmp.dropna(inplace=True)\n\nstart_date = pd.to_datetime('2011-01-12')\nend_date = pd.to_datetime('2016-12-29')\nDtmp = Dtmp.loc[start_date:end_date]\nDtmp = Dtmp.iloc[5:]\nfactors = factors.loc[start_date:end_date]\n\nflist = []\nfor i in range(len(Dtmp)):\n    s = i * 50\n    e = (i + 5) * 50\n    f = np.array(factors.iloc[s:e])\n    flist.append(np.expand_dims(f, axis=0))\n\nfac_array = np.concatenate(flist, axis=0)\nshape = [fac_array.shape[0], 5, 50, fac_array.shape[2]]\nfac_array = fac_array.reshape(shape)\nfac_array = np.transpose(fac_array, [0,2,3,1])\n\ndata_quotes = Dtmp\ndata_fac = fac_array", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "\u4e0d\u652f\u6301\u5c5e\u6027:open."}, {"output_type": "stream", "stream": "stdout", "text": "\n"}], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "3D6EBF6ACF7D438D8AF994CEED2DD7A6", "input": "class Account(object):\n    \n    def __init__(self, data_quotes, data_fac):\n        self.data_close = data_quotes['close']\n        self.data_open = data_quotes['open']\n        self.data_observation = data_fac\n        self.action_space = ['long', 'short', 'close']\n        self.free = 1e-4\n        self.reset()\n\n    def reset(self):\n        self.step_counter = 0\n        self.cash = 1e5\n        self.position = 0\n        self.total_value = self.cash + self.position\n        self.flags = 0\n        \n    def get_initial_state(self):\n        return np.expand_dims(self.data_observation[0],axis=0)\n    \n    def get_action_space(self):\n        return self.action_space\n\n    def long(self):\n        self.flags = 1\n        quotes = self.data_open[self.step_counter] * 10\n        self.cash -= quotes * (1 + self.free)\n        self.position = quotes \n        \n    def short(self):\n        self.flags = -1\n        quotes = self.data_open[self.step_counter] * 10\n        self.cash += quotes * (1 - self.free)\n        self.position = - quotes        \n        \n    def keep(self):\n        quotes = self.data_open[self.step_counter] * 10\n        self.position = quotes * self.flags\n        \n    def close_long(self): \n        self.flags = 0\n        quotes = self.data_open[self.step_counter] * 10\n        self.cash += quotes * (1 - self.free) \n        self.position = 0  \n        \n    def close_short(self):\n        self.flags = 0\n        quotes = self.data_open[self.step_counter] * 10\n        self.cash -= quotes * (1 + self.free) \n        self.position = 0  \n\n    def step_op(self, action):\n        \n        if action == 'long':\n            if self.flags == 0:\n                self.long()\n            elif self.flags == -1:\n                self.close_short()\n                self.long()\n            else:\n                self.keep() \n        \n        elif action == 'close':\n            if self.flags == 1:\n                self.close_long()\n            elif self.flags == -1:\n                self.close_short()\n            else:\n                pass\n                \n        elif action == 'short':\n            if self.flags == 0:\n                self.short()\n            elif self.flags == 1:\n                self.close_long()\n                self.short()\n            else:\n                self.keep()\n        else:\n            raise ValueError(\"action should be elements of ['long', 'short', 'close']\")\n            \n        position = self.data_close[self.step_counter] * 10 * self.flags\n        reward = self.cash + position - self.total_value\n        self.step_counter += 1\n        self.total_value = position + self.cash\n        next_observation = self.data_observation[self.step_counter]\n        \n        done = False\n        if self.total_value < 4000:\n            done = True\n        if self.step_counter > 600:\n            done = True\n            \n        return reward, np.expand_dims(next_observation, axis=0), done\n    \n    def step(self, action):\n        if action == 0:\n            return self.step_op('long')\n        elif action == 1:\n            return self.step_op('short')\n        elif action == 2:\n            return self.step_op('close')\n        else:\n            raise ValueError(\"action should be one of [0,1,2]\")\n    \n    def get_next_state(self):\n        self.step_counter += 1\n        next_observation = self.data_observation[self.step_counter]\n        return np.expand_dims(next_observation, axis=0)", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": true, "id": "BC53435EAA2046098F64CF3B08DBD578", "input": "env = Account(data_quotes, data_fac)", "language": "python", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'data_quotes' is not defined", "output_type": "pyerr", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[1;32m<mercury-input-4-BC53435EAA2046098F64CF3B08DBD578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAccount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_quotes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_fac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[1;31mNameError\u001b[0m: name 'data_quotes' is not defined"]}], "trusted": true}, {"cell_type": "markdown", "id": "A56731A444A84B6691D53B2534BC0946", "metadata": {}, "source": "### 2 \u865a\u62df\u51b3\u7b56\u8005\u6784\u5efa"}, {"cell_type": "code", "collapsed": false, "id": "63CAC7C489EF48A7BDC9EF07491BA0A2", "input": "import random\nimport numpy as np\nimport tensorflow as tf\nfrom collections import deque", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "0805BA68AF8C453B814C351D2003CE72", "input": "class DQNCore(object):\n    def __init__(self, observation, num_actions, learning_rate=1e-3, memory_size=1024, batch_size=32, gamma=.9, name='DNCore'):\n        self.num_actions = num_actions\n        self.memory_size = memory_size\n        self.gamma = gamma # discount factor for excepted returns \n        self.batch_size = 32\n        \n        # placeholder for samples replay experience\n        shape = [None] + list(observation.shape [1:])\n        self.inputs = tf.placeholder(tf.float32, shape, 'inputs')\n        self.targets = tf.placeholder(tf.float32, [None], 'targets') # y_j\n        self.actions = tf.placeholder(tf.int32, [None], 'actions')\n        self.rewards = tf.placeholder(tf.float32, [None], 'rewards')\n        self.Q = self._build_QNetwork('Qeval', True) # state Q\n        self.next_Q = self._build_QNetwork('next_eval',False) # next state Q\n        \n        # actions selection corresponding one hot matrix column\n        one_hot = tf.one_hot(self.actions, self.num_actions, 1., 0.)\n        Qmax = tf.reduce_sum(self.Q * one_hot, axis=1)\n        self._loss = tf.reduce_mean(tf.squared_difference(Qmax, self.targets))\n        self._train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self._loss)\n        \n        # session\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())   \n        \n    def init(self):\n        self.step_counter = 0       \n        self.cache = deque(maxlen=self.memory_size) # replay experience\n\n    def _build_QNetwork(self, name, trainable):\n        with tf.variable_scope(name):\n            # input layer\n            network = tf.layers.conv2d(self.inputs, 16, [8,8], [4,4], 'same', \n                                       activation=tf.nn.relu, trainable=trainable, name='input_layer')\n            # hidden layer\n            network = tf.layers.conv2d(network, 32, [4,4], [2,2], 'same', \n                                       activation=tf.nn.relu, trainable=trainable, name='hidden_layer')\n            # final layer\n            network = tf.contrib.layers.flatten(network)\n            network = tf.layers.dense(network, 64, tf.nn.relu, \n                                      trainable=trainable, name='final_layer')\n            # output layer\n            network = tf.layers.dense(network, self.num_actions, None, \n                                      trainable=trainable, name='output_layer')\n            return network\n\n    def update_nextQ_network(self): \n        next_params = tf.get_collection(\n            tf.GraphKeys.GLOBAL_VARIABLES, \n            scope='next_eval')\n        Q_params = tf.get_collection(\n            tf.GraphKeys.GLOBAL_VARIABLES, \n            scope='Qeval')\n        # zip \u957f\u5ea6\u4e0d\u7b49\u65f6\uff0c\u53d6\u957f\u5ea6\u7684\u6700\u5c0f\u7684\n        self.sess.run([tf.assign(n,q) for n,q in zip(next_params, Q_params)])\n\n    def update_cache(self, state, action, reward, next_state, done):\n        # update replay experience pool\n        self.cache.append((state, action, reward, next_state, done))\n\n    def _get_minibatch(self):\n        # get samples from replay experience pool\n        minibatch = random.sample(self.cache, self.batch_size) \n        state = np.vstack([i[0] for i in minibatch])\n        action = np.squeeze(np.vstack([i[1] for i in minibatch]))\n        reward = np.squeeze(np.vstack([i[2] for i in minibatch]))\n        next_state = np.vstack([i[3] for i in minibatch])\n        done = [i[4] for i in minibatch]\n        return state, action, reward, next_state, done\n\n    def step_learning(self):\n        # samples from repaly experience pool\n        state, action, reward, next_state, done = self._get_minibatch()\n        next_Q = self.sess.run(self.next_Q, feed_dict={self.inputs:next_state})\n        # done mask True 1 False 0\n        mask = np.array(done).astype('float')\n        target = mask * reward + (1 - mask) * \\\n        (reward + self.gamma * np.max(next_Q, axis=1))\n        \n        # op gradient descent step \n        self.sess.run(self._train_op, \n                      feed_dict={self.inputs:state, \n                                 self.actions:action, \n                                 self.targets:target})    \n        \n    def greedy_policy(self, observation):\n        # \u6ce8\uff1a\u53ea\u5728\u4f18\u5316\u903c\u8fd1\u51fd\u6570\u53c2\u6570\u8fc7\u7a0b\u4f7f\u7528 varepsilon greedy policy\n        action_value = self.sess.run(\n            self.Q, feed_dict={self.inputs:observation})\n        return np.argmax(action_value, axis=1)[0]\n    \n    def varepsilon_greedy_policy(self, observation, varepsilon=0.9):\n        if np.random.uniform() < varepsilon:\n            action = self.greedy_policy(observation)\n        else:\n            action = np.random.randint(self.num_actions)\n        return action", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "B482A2BE658248759F2E962C4927098C", "metadata": {}, "source": "### 3 \u8bad\u7ec3\u4ea4\u6613\u5458 DQN"}, {"cell_type": "code", "collapsed": false, "id": "E0C4C5A93CB049A2AD223C881ECE7972", "input": "import pandas as pd\nimport seaborn as sns\n%matplotlib inline", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "1AEC58D43555457A962FAF4E8D11F9DF", "input": "init_state = env.get_initial_state()\naction_space = env.get_action_space()\nagent = DQNCore(init_state, len(action_space))", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "EF27FBBE7773440689270B016F03243C", "input": "# \u8fd9\u91cc\u53ea\u662f\u7b80\u5355\u7684\u4ee3\u7801\u8fd0\u884c\u793a\u4f8b\uff0c\u53ea\u8fdb\u884c\u768410\u6b21\u8bad\u7ec3\uff0c\u8fd9\u4ec5\u4ec5\u662f\u51fa\u4e8e\u793a\u4f8b\u8003\u91cf\uff0c\u4e0d\u4ee3\u8868DQN\u4ec5\u4ec5\u8bad\u7ec310\u6b21\u5c31\u8fd1\u4f3c\u6570\u503c\u7a33\u5b9a\u6536\u655b\u3002\u3002\u3002\u3002\nMAX_EPISODE = 10\n\nglobal_counter = 0\nvarepsilon = 0\n\nfor episode in range(MAX_EPISODE):\n    print (episode)\n    step_counter = 0\n    env.reset()\n    state = env.get_initial_state()\n    agent.init()\n    while True:\n        global_counter += 1\n        step_counter += 1\n        if global_counter % 500 == 0:\n            varepsilon += 5e-5\n        \n        action = agent.varepsilon_greedy_policy(state, varepsilon)\n        reward, next_state, done = env.step(action)\n        agent.update_cache(state, action, reward, next_state, done)\n        state = next_state\n        \n        if global_counter > 500 and step_counter > 32:\n            agent.step_learning()\n        if global_counter % 500 ==0:\n            agent.update_nextQ_network()     \n        \n        if done:\n            break", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "B93C0704E8FE47DE853F75D45EB205E0", "metadata": {}, "source": "### 4 \u6d4b\u8bd5\u865a\u62df\u4ea4\u6613\u5458\u6c34\u5e73"}, {"cell_type": "code", "collapsed": false, "id": "D7D6F4A3B10E4A168E05D246918BD233", "input": "reward_list= []\nvalue_list = []\nenv.reset()\nstate = env.get_initial_state()\nfor i in range(900):\n    action = agent.greedy_policy(state)\n    reward, next_state, done = env.step(action)\n    state = next_state\n    reward_list.append(reward)\n    value_list.append(env.total_value)", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "A4FE632727DD4FBD834E60B48CA17FE0", "metadata": {}, "source": "# \u51c0\u503c\u8d70\u52bf\u56fe\uff0c 600\u4e4b\u540e\u4e3a\u6837\u672c\u5916\u6d4b\u8bd5\n- \u8fd9\u4e2a\u662f\u6211\u53e6\u5916\u56e0\u5b50\u548c\u8bad\u7ec3\u8fc7\u7a0b\u8dd1\u51fa\u7684\u6cdb\u5316\u6d4b\u8bd5"}, {"cell_type": "markdown", "id": "04FE9162E6374CF987044670CF2EB8DA", "metadata": {}, "source": "![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/db7ced3e-b28a-11e7-9497-0242ac140002)"}, {"cell_type": "code", "collapsed": false, "id": "B52D52540BD345FA8FE1579D94A48B40", "input": "", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}