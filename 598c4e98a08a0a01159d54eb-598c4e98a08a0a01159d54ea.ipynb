{"metadata": {"signature": "sha256:e2a7b399f7e4db5070a4966c2a4229ebbc28bc8a3712d62811a9f580aa7fb1b0"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "53B7D4A34AB94A4B918E0E4DA8B886C7", "metadata": {}, "source": "# PonderDNC\u5904\u7406\u9ad8\u9891\u957f\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7b80\u4ecb"}, {"cell_type": "markdown", "id": "839FF4EACE5C4C939F622749B6C8EC67", "metadata": {}, "source": "ACT\u6a21\u578b\u901a\u8fc7\u5728\u6bcf\u65f6\u95f4\u6b65(\u65f6\u95f4\u5e8f\u5217\u8282\u70b9)\u8fdb\u884c\u591a\u6b21\u8ba1\u7b97\u6765\u6a21\u62df\u590d\u6742\u95ee\u9898\u7684\u601d\u8003\u8fc7\u7a0b\u3002\u8fd9\u79cd\u7b97\u6cd5\u5c24\u5176\u6709\u4ef7\u503c\u5728\u4f7f\u7528\u5305\u542b\u5916\u5b58\u50a8\u5668\u7684RNN\u53d8\u79cd\uff08\u5982DNC\u3001NTM\u7b49\uff09\u8ba1\u7b97\u67b6\u6784\u5904\u7406\u957f\u65f6\u95f4\u5e8f\u5217\u7684\u65f6\u5019\u3002\n\n\u4e0b\u9762\u7b80\u5355\u5f62\u8c61\u7684\u4ecb\u7ecd\u4e00\u4e0b\u672c\u6587\u6a21\u578b\uff08\u8fd9\u4e0d\u662f\u5f88\u51c6\u786e\u7684\u63cf\u8ff0\uff0c\u4f46\u662f\u8db3\u591f\u5f62\u8c61\uff09\u3002\u6211\u4eec\u5047\u8bbeVanillaRNN\u6a21\u578b\u4e3a\u4e00\u4e2a\u8003\u751f\uff0c\u5728\u8fdb\u884c\u82f1\u8bed\u542c\u529b\u8003\u8bd5\uff0c\u8fd9\u4e2a\u8003\u751f\u88ab\u8981\u6c42\u201c\u5e72\u542c\u201d\uff0c\u4e5f\u5c31\u662f\u4e0d\u80fd\u4f7f\u7528\u7eb8\u7b14\u8bb0\u5f55\uff0c\u5e76\u4e14\u6240\u6709\u7684\u542c\u529b\u5185\u5bb9\u95ee\u9898\u548c\u7b54\u6848\u5168\u90e8\u90fd\u662f\u4ee5\u53e3\u5934\u5f62\u5f0f\u8fdb\u884c\u7684\u3002\u5f53\u7136\u5728\u6392\u9664\u5929\u7eb5\u5947\u624d\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e2a\u8003\u751f\u7684\u7b54\u6848\u662f\u60e8\u4e0d\u5fcd\u7779\u7684\u3002\n\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/bd1dca24-7dc5-11e7-8389-0242ac140002)\n\n\u4e0b\u9762\u6211\u4eec\u6765\u770b\u7b2c\u4e8c\u4e2a\u540d\u53ebLSTM\u7684\u8003\u751f\uff0c\u8fd9\u4e2a\u8003\u751f\u5728\u8fdb\u884c\u542c\u529b\u8003\u8bd5\u7684\u65f6\u5019\u6709\u4e00\u5757\u78c1\u94c1\u753b\u677f\uff0c\u53ef\u4ee5\u8bb0\u5f55\u4e00\u5b9a\u91cf\u7684\u4fe1\u606f\u3002\u4f46\u662f\u8fd9\u4e2a\u8003\u751f\u5728\u8d85\u8fc730min\u7684\u542c\u529b\u8003\u8bd5\u4e2d\u53ea\u80fd\u4f7f\u7528\u8fd9\u4e2a\u753b\u677f\u8bb0\u5f55\u4e00\u70b9\u70b9\u4fe1\u606f\uff0c\u5982\u679c\u60f3\u8981\u5199\u5165\u65b0\u7684\u4fe1\u606f\u5c31\u5fc5\u987b\u8981\u6e05\u695a\u4e4b\u524d\u7684\u4fe1\u606f\u3002\u4e5f\u5c31\u662f\u8fd9\u4e2a\u53eb\u505aLSTM\u7684\u8003\u751f\u53ef\u4ee5\u8bb0\u5f55\u6709\u9650\u7684\u4fe1\u606f\u3002\u5f53\u7136\u5bf9\u4e8e\u957f\u65f6\u95f4\u7684\u542c\u529b\u8003\u8bd5\u8fd9\u70b9\u7b14\u8bb0\u4e5f\u662f\u676f\u6c34\u8f66\u85aa\u3002\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/be5ae34a-7dc5-11e7-8389-0242ac140002)\n\n\u4e0b\u9762\u6211\u4eec\u6765\u770b\u7b2c\u4e09\u4e2a\u53eb\u505aDNC\u7684\u8003\u751f\uff0c\u8fd9\u4e2a\u8003\u751f\u4f7f\u7528\u4e00\u4e2a\u66f4\u9ad8\u7ea7\u7684\u753b\u677f(E\u4ebaE\u672c)\uff0c\u8fd9\u4e2a\u753b\u677f\u53ef\u4ee5\u8bb0\u5f55\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u751a\u81f3\u53ef\u4ee5\u8bb0\u5f55\u6240\u6709\u7684\u542c\u529b\u4fe1\u606f\uff08\u5f53DNC\u7684\u5916\u5b58\u50a8\u5668\u8db3\u591f\u5927\u7684\u65f6\u5019\uff09\u3002\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/bea5f13c-7dc5-11e7-8389-0242ac140002)\n\n\u4ee5\u4e0a\u7684\u4e09\u4e2a\u8003\u751f\u90fd\u662f\u5728\u52a0\u5f3a\u81ea\u5df1\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u9488\u5bf9\u542c\u529b\u8003\u8bd5\uff08\u957f\u65f6\u95f4\u5e8f\u5217\uff09\u8fdb\u884c\u4fe1\u606f\u5904\u7406\u3002\u8fd9\u4e09\u4e2a\u8003\u751f\u90fd\u662f\u6309\u7167\u542c\u529b\u64ad\u653e\u987a\u5e8f\u8fdb\u884c\u7b54\u9898\uff0c\u867d\u7136\u7b2c\u4e09\u4e2a\u8003\u751f(DNC)\u53ef\u4ee5\u8bb0\u5f55\u51e0\u4e4e\u6240\u6709\u7684\u542c\u529b\u4fe1\u606f\uff0c\u4f46\u662f\u5b83\u5e76\u6ca1\u6709\u989d\u5916\u7684\u65f6\u95f4\u53bb\u7ffb\u9605\u6240\u6709\u8bb0\u5f55\u4e0b\u6765\u7684\u4fe1\u606f\uff0c\u53ea\u80fd\u591f\u6839\u636e\u5927\u8111\uff08\u77ed\u671f\u8bb0\u5fc6\uff09\u4e2d\u7684\u5370\u8c61\u548c\u81ea\u5df1\u5199\u5b57\u987a\u5e8f\u5728\u975e\u5e38\u77ed\u7684\u65f6\u95f4\u5185\u7ffb\u9605\u4e00\u4e0b\u8bb0\u5f55\u672c\u3002\n\n\u4e0b\u9762\u4ecb\u7ecd\u7b2c\u56db\u4e2a\u8003\u751f\uff0c\u4e00\u4e2a\u53eb\u505aPonderDNC\u7684\u5e26\u7740\u65f6\u95f4\u673a\u5668\u4f5c\u5f0a\u5668\u7684\u8003\u751f\u3002PonderDNC\u4e0d\u4f46\u5e26\u7740E\u4ebaE\u672c\uff0c\u800c\u4e14\u8fd8\u5e26\u7740\u65f6\u95f4\u9759\u6b62\u673a\u5668\u3002\u8fd9\u4e5f\u5c31\u662f\u8bf4\u8fd9\u540d\u8003\u751f\u5728\u8fdb\u884c\u542c\u529b\u8003\u8bd5\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u5728\u4efb\u4f55\u81ea\u5df1\u89c9\u5f97\u53ef\u4ee5\u7684\u5730\u65b9\u8ba9\u65f6\u95f4\u9759\u6b62\uff0c\u6ce8\u610f\u4e0d\u80fd\u56de\u6eaf\uff0c\u7136\u540e\u4ece\u5bb9\u7684\u7ffb\u9605\u81ea\u5df1\u5728E\u4ebaE\u672c\u4e0a\u9762\u7684\u8bb0\u5f55\u8fdb\u884c\u7b54\u9898\u3002\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/bec744cc-7dc5-11e7-8389-0242ac140002)\n\u6beb\u65e0\u7591\u95eePonderDNC\u662f\u5728\u7406\u8bba\u4e0a\u9762\u6700\u6709\u53ef\u80fd\u5728\u542c\u529b\u8003\u8bd5\u603b\u83b7\u53d6\u9ad8\u5206\u7684\u8003\u751f\u3002\n\n## PonderDNC\n\n\u901a\u8fc7\u5c06DNC\u8ba1\u7b97\u5355\u5143\u8fdb\u884c\u5d4c\u5165ACT\u8ba1\u7b97\u67b6\u6784\uff0cPonderDNC\u53ef\u4ee5\u5b9e\u73b0\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u591a\u6b21\u8fdb\u884c\u8fd0\u7b97\u3002\u8fd9\u610f\u5473\u7740\u5728\u65f6\u95f4\u6b65t\uff0cDNC\u5728\u83b7\u5f97\u65f6\u523bt\u7684\u5916\u90e8\u8f93\u5165\u4e4b\u540e\u4e0d\u7528\u7acb\u523b\u8f93\u51fa\u4fe1\u606f\uff0c\u800c\u662f\u53ef\u4ee5\u5728\u65f6\u523bt\u53cd\u590d\u8fdb\u884c\u601d\u8003\u8ba1\u7b97\u4e4b\u540e\u518d\u505a\u51fa\u5224\u5b9a\u8f93\u51fa\uff0c\u7136\u540e\u518d\u8fdb\u5165\u4e0b\u4e00\u65f6\u523bt+1\u3002\u5982\u4e0b\u56fe\n![](https://pic2.zhimg.com/v2-39b6d7d4a6a2a000daf210d4b5faaba1_b.png)\n\nACT (Adaptive Computation Time for Recurrent Neural Networks ) \u8ba1\u7b97\u67b6\u6784\u5728\u65f6\u95f4\u6b65t\uff0cRNN\u6a21\u578b\u7ecf\u8fc7\u591a\u6b21\u8ba1\u7b97(\u601d\u8003)\u4e4b\u540e\uff0c\u5f62\u6210\u591a\u4e2a\u8f93\u51fa$y_t^1,...,y_t^N$\u548c\u9690\u85cf\u72b6\u6001$h_t^1,...,h_t^N$ \uff0cRNN\u5728\u65f6\u523bt\u7684\u8f93\u51fa\u4ee5\u53ca\u4f20\u9012\u5230\u4e0b\u4e00\u65f6\u523bt+1\u7684\u9690\u85cf\u72b6\u6001\u901a\u8fc7\u6743\u91cd\u7cfb\u6570\u7d2f\u52a0\u83b7\u5f97\u3002\n![](http://storage-uqer.datayes.com/57b19ec7228e5b79a7759010/bfc1760e-7dc5-11e7-8389-0242ac140002)\n\n\u8fd9\u79cd\u8ba1\u7b97\u67b6\u6784\u901a\u8fc7\u5728\u65f6\u523bt\u8fdb\u884c\u591a\u6b21\u8fd0\u7b97\u53d6\u6743\u91cd\u7cfb\u6570\u7d2f\u52a0\u548c\u7684\u65b9\u5f0f\u6a21\u62df\u601d\u8003\u8fc7\u7a0b\u3002\u8fd9\u5bf9\u4e8e\u4f7f\u7528DNC\u8fd9\u79cd\u5e26\u6709\u5916\u5b58\u50a8\u5668\u7684\u8ba1\u7b97\u5355\u5143\u6765\u5904\u7406\u8d85\u957f\u5e8f\u5217\u5c24\u5176\u6709\u610f\u4e49\u3002\u8fd9\u610f\u5473\u7740\u5bf9\u4e8e\u4e00\u4e2a\u5177\u6709\u8d85\u5927\u5916\u5b58\u50a8\u5668\u8bb0\u5fc6(\u59821K\u884c\u8bb0\u5fc6)\u7684DNC\u800c\u8a00\uff0c\u5728\u65f6\u95f4\u6b65t \u8ba1\u7b97\u5355\u5143DNC\u57fa\u4e8e\u5916\u90e8\u8f93\u5165\u4fe1\u606fx_t \u53ef\u4ee5\u591a\u6b21\u53cd\u590d\u7684\u8bfb\u53d6\u5916\u8bb0\u5fc6\u77e9\u9635\u4fe1\u606f\u8fdb\u884c\u53cd\u590d\u6743\u8861\u601d\u8003\u4e4b\u540e\u518d\u505a\u51fa\u5224\u5b9a\u3002\n\n\u539f\u59cb\u7684DNC\u6a21\u578b\u4e00\u822c\u4f7f\u7528\u4e00\u4e2a500\u884c\u7684\u5916\u8bb0\u5fc6\u77e9\u9635\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65t\u901a\u8fc71~3\u5199\u5934\u63a7\u5236\uff0c2~6\u8bfb\u5934\u63a7\u5236\u4e0e\u5916\u8bb0\u5fc6\u77e9\u9635\u8fdb\u884c\u4fe1\u606f(\u8bb0\u5fc6)\u4ea4\u4e92\u3002\u4e5f\u5c31\u662fDNC\u6a21\u578b\u5728\u65f6\u523bt\u4e00\u822c\u53ea\u4e0e\u4e0d\u8d85\u8fc710\u4e2a\u8bb0\u5fc6\u4f4d\u7f6e\u6709\u4fe1\u606f\u4ea4\u4e92\u3002\u8fd9\u5bf9\u4e8e\u7b80\u5355\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u8bb0\u5fc6\u590d\u73b0\u5177\u6709\u8f83\u597d\u7684\u6548\u679c\uff0c\u4f46\u662f\u5bf9\u4e8e\u90a3\u4e9b\u4e0e\u524d\u9762\u4fe1\u606f\u4ea4\u4e92\u590d\u6742\u800c\u4e14\u65f6\u95f4\u8de8\u5ea6\u5f88\u957f\u7684\u5224\u5b9a\u95ee\u9898\u5219\u6548\u679c\u6b20\u4f73\u3002\u901a\u8fc7\u5c06DNC\u8ba1\u7b97\u5355\u5143\u5d4c\u5165ACT\u8ba1\u7b97\u67b6\u6784\u4e4b\u540e\u5f62\u6210\u7684PonderDNC\u8ba1\u7b97\u5355\u5143\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u53ef\u4ee5\u591a\u6b21\u4e0e\u5916\u8bb0\u5fc6\u77e9\u9635\u8fdb\u884c\u4fe1\u606f\u4ea4\u4e92\u3002\u4f8b\u5982\u4e00\u4e2a\u5177\u67092\u5199\u5934\u63a7\u5236\uff0c4\u8bfb\u5934\u63a7\u5236\uff0c\u5177\u67091000\u884c\u5916\u5b58\u50a8\u8bb0\u5fc6\u7684PonderDNC\u5728\u65f6\u523bt\u8fd0\u7b9750\u6b21\uff0c\u6700\u591a\u53ef\u4ee5\u4e0e\u5916\u8bb0\u5fc6\u77e9\u9635\u7684300\u4e2a\u4f4d\u7f6e\u8fdb\u884c\u4ea4\u4e92\uff0c\u5728\u65f6\u523bt \u6700\u591a\u53ef\u4ee5\u57fa\u4e8e50\u4e2a\u77ed\u671f\u8bb0\u5fc6\u4ee5\u53ca200\u4e2a\u8bfb\u5934\u8bb0\u5fc6\u505a\u51fa\u63a8\u65ad\u8f93\u51fa\u3002\u4e5f\u5c31\u662f\u5728\u65f6\u523bt \u8ba1\u7b97\u5355\u5143PonderDNC\u53ef\u4ee5\u57fa\u4e8e\u524d\u9762\u8bb0\u5fc6\u603b\u91cf\u768420%\u8fdb\u884c\u63a8\u65ad\u5224\u5b9a\u3002\n\n\u901a\u4fd7\u7684\u8bb2\uff0c\u5047\u8bbe\u8fd9\u51e0\u4e2a\u6a21\u578b\u88ab\u770b\u505a\u662f\u4e0d\u540c\u64cd\u76d8\u624b\u7684\u8bdd\uff0cLSTM\u64cd\u76d8\u624b\u5c31\u662f\u57fa\u4e8e\u8fc7\u53bb\u4e24\u5468\u7684K\u7ebf\u56fe\u5bf9\u672a\u6765\u884c\u60c5\u8fdb\u884c\u5224\u65ad\uff0c\u800cPonderDNC\u5219\u662f\u57fa\u4e8e\u8fc7\u53bb\u4e00\u5b63\u5ea6\u7684\u591a\u79cd\u6280\u672f\u5206\u6790\u6307\u6807\u5bf9\u672a\u6765\u8fdb\u884c\u5224\u5b9a\u3002\n\n\u6ce8\uff1aPonderDNC\u6bcf\u4e2a\u65f6\u95f4\u6b65t\u4f20\u9012\u7ed9\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65t+1\u7684\u4fe1\u606f\u4e3a \u7d2f\u52a0\u7684\u63a7\u5236\u5668\u72b6\u6001\u3001\u7d2f\u52a0\u7684\u8bfb\u5934\u8bfb\u53d6\u8bb0\u5fc6\u5411\u91cf\u3001\u4ee5\u53ca\u5728\u65f6\u523bt\u8fdb\u884cN(t)\u6b21\u8fd0\u7b97\u4e4b\u540e\u7684\u5916\u5b58\u50a8\u8bb0\u5fc6\u77e9\u9635(\u8fd9\u4e2a\u4e0d\u662f\u6743\u91cd\u7d2f\u52a0\u548c\u800c\u662f\u6700\u540e\u4e00\u6b21\u8fd0\u7b97\u4e4b\u540e\u7684\u8bb0\u5fc6\u77e9\u9635\u76f4\u63a5\u4f20\u9012\u4e0b\u53bb)\u3002"}, {"cell_type": "markdown", "id": "19A68DE102974E7C9CC406AC9D4B1EB7", "metadata": {}, "source": "### \u5c01\u88c5\u7c7b\u4f7f\u7528\u65b9\u6cd5"}, {"cell_type": "code", "collapsed": false, "id": "19A7D445360141FE88274BED25DED25C", "input": "a = Classifier_DNC_BasicLSTM_L1(train_inputs, train_targets, train_gather_list)\n\n# \u53c2\u6570   \ninputs, # \u8f93\u5165\u8bad\u7ec3\u6570\u636e \ntargets, # \u8f93\u5165\u8bad\u7ec3\u6570\u636e\u5bf9\u5e94\u6807\u7b7e\ngather_list=None, # \u622a\u53d6\u5217\u8868\nbatch_size=1, # \nhidden_size=100, # DNC\u63a7\u5236\u5668\u9690\u85cf\u72b6\u6001\u6570\u91cf \nmemory_size=100, # \u8bb0\u5fc6\u77e9\u9635\u5927\u5c0f\nthreshold=0.99, # ACT\u5728\u65f6\u523bt\u5faa\u73af\u8fd0\u7b97\u622a\u81f3\u9608\u503c\npondering_coefficient = 1e-2, # \u601d\u8003\u635f\u5931\u51fd\u6570\u7cfb\u6570\nnum_reads=3, # DNC\u8bfb\u5934\u63a7\u5236\u6570\u91cf\nnum_writes=1 # DNC\u5199\u5934\u63a7\u5236\u6570\u91cf", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "D2E62C101B5C43798CD48717A7D6D496", "input": "#\u9884\u6d4b\u53c2\u6570\na.fit(5,learning_rate = 1e-1)\n# \u53c2\u6570\ntraining_iters =1e2, # \u8bad\u7ec3\u6b21\u6570            \nlearning_rate = 1e-4, # \u5b66\u4e60\u7387\noptimizer_epsilon = 1e-10, # \u4f18\u5316\u5668\u53c2\u6570\u5efa\u8bae\u4fdd\u6301\u539f\u503c\nmax_gard_norm = 50 # \u4f18\u5316\u5668\u68af\u5ea6\u9632\u8303\u53c2\u6570\u5efa\u8bae\u4fdd\u6301\u539f\u503c", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "6C848E1E859A4CCC8AC45B65B4E6F6A7", "input": "", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "DB1F0D5FF5054BA08C89B39E427F576F", "metadata": {}, "source": "# PonderDNC\u5b9e\u73b0\u4ee3\u7801"}, {"cell_type": "code", "collapsed": false, "folded": true, "id": "0515A0A603294D27A9B5E8233C1D34C6", "input": "from FixDNCore import DNCore_L1\nfrom FixACT import ACTCore\n\nclass Classifier_DNC_BasicLSTM_L1(object):\n    \n    def __init__(self, \n                 inputs, \n                 targets,\n                 gather_list=None,\n                 batch_size=1, \n                 hidden_size=10, \n                 memory_size=10, \n                 threshold=0.99,\n                 pondering_coefficient = 1e-2,\n                 num_reads=3,\n                 num_writes=1):\n        \n        self._tmp_inputs = inputs\n        self._tmp_targets = targets\n        self._in_length = inputs.shape[0]\n        self._in_width = inputs.shape[2]\n        self._out_length = targets.shape[0]\n        self._out_width = targets.shape[2]\n        self._batch_size = batch_size\n        \n        # \n        self._sess = tf.InteractiveSession()\n        \n        self._inputs = tf.placeholder(dtype=tf.float32, \n                                      shape=[self._in_length, self._batch_size, self._in_width], \n                                      name='inputs')\n        self._targets = tf.placeholder(dtype=tf.float32, \n                                       shape=[self._out_length, self._batch_size, self._out_width],\n                                       name='targets')\n        \n        act_core = DNCore_L1( hidden_size=hidden_size, \n                              memory_size=memory_size, \n                              word_size=self._in_width, \n                              num_read_heads=num_reads, \n                              num_write_heads=num_writes)        \n        self._InferenceCell = ACTCore(core=act_core, \n                                      output_size=self._out_width, \n                                      threshold=threshold, \n                                      get_state_for_halting=self._get_hidden_state)\n        \n        self._initial_state = self._InferenceCell.initial_state(self._batch_size)\n        \n        tmp, act_final_cumul_state = \\\n        tf.nn.dynamic_rnn(cell=self._InferenceCell, \n                          inputs=self._inputs, \n                          initial_state=self._initial_state, \n                          time_major=True)\n        act_output, (act_final_iteration, act_final_remainder) = tmp\n        \n        self._pred_outputs = act_output\n        if gather_list is not None:\n            out_sequences = tf.gather(act_output, gather_list)\n        else:\n            out_sequences = act_core\n        \n        pondering_cost = (act_final_iteration + act_final_remainder) * pondering_coefficient\n        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n            labels=self._targets, logits=out_sequences)\n        self._cost = tf.reduce_mean(rnn_cost) + tf.reduce_mean(pondering_cost)\n        \n        self._pred = tf.nn.softmax(out_sequences, dim=2)\n        correct_pred = tf.equal(tf.argmax(self._pred,2), tf.argmax(self._targets,2))\n        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n    def _get_hidden_state(self, state):\n        controller_state = state[0]\n        next_state, next_cell = controller_state\n        return next_state\n        \n    def fit(self, \n            training_iters =1e2,             \n            learning_rate = 1e-4,\n            optimizer_epsilon = 1e-10,\n            max_gard_norm = 50):\n\n        # Set up optimizer with global norm clipping.\n        trainable_variables = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n        global_step = tf.get_variable(\n            name=\"global_step\",\n            shape=[],\n            dtype=tf.int64,\n            initializer=tf.zeros_initializer(),\n            trainable=False,\n            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n        \n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n        self._train_step = optimizer.apply_gradients(\n            zip(grads, trainable_variables), global_step=global_step)  \n        \n        self._sess.run(tf.global_variables_initializer())\n        for scope in range(np.int(training_iters)):\n            _, loss, acc = self._sess.run([self._train_step, self._cost, self._accuracy], \n                                     feed_dict = {self._inputs:self._tmp_inputs, \n                                                  self._targets:self._tmp_targets})\n            print (scope, '  loss--', loss, '  acc--', acc)\n        print (\"Optimization Finished!\") \n            \n            \n    def close(self):\n        self._sess.close()\n        print ('\u7ed3\u675f\u8fdb\u7a0b\uff0c\u6e05\u7406tensorflow\u5185\u5b58/\u663e\u5b58\u5360\u7528')\n        \n        \n    def pred(self, inputs, gather_list=None):\n        \n        output_pred = self._pred_outputs\n        if gather_list is not None:\n            output_pred = tf.gather(output_pred, gather_list)\n        probability = tf.nn.softmax(output_pred)\n        classification = tf.argmax(probability, axis=-1)\n        \n        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})\n    \n    \n    \nclass Classifier_DNC_BasicLSTM_L3(object):\n    \n    def __init__(self, \n                 inputs, \n                 targets,\n                 gather_list=None,\n                 batch_size=1, \n                 hidden_size=10, \n                 memory_size=10, \n                 threshold=0.99,\n                 pondering_coefficient = 1e-2,\n                 num_reads=3,\n                 num_writes=1):\n        \n        self._tmp_inputs = inputs\n        self._tmp_targets = targets\n        self._in_length = None\n        self._in_width = inputs.shape[2]\n        self._out_length = None\n        self._out_width = targets.shape[2]\n        self._batch_size = batch_size\n        \n        # \n        self._sess = tf.InteractiveSession()\n        \n        self._inputs = tf.placeholder(dtype=tf.float32, \n                                      shape=[self._in_length, self._batch_size, self._in_width], \n                                      name='inputs')\n        self._targets = tf.placeholder(dtype=tf.float32, \n                                       shape=[self._out_length, self._batch_size, self._out_width],\n                                       name='targets')\n        \n        act_core = DNCore_L3( hidden_size=hidden_size, \n                              memory_size=memory_size, \n                              word_size=self._in_width, \n                              num_read_heads=num_reads, \n                              num_write_heads=num_writes)        \n        self._InferenceCell = ACTCore(core=act_core, \n                                      output_size=self._out_width, \n                                      threshold=threshold, \n                                      get_state_for_halting=self._get_hidden_state)\n        \n        self._initial_state = self._InferenceCell.initial_state(self._batch_size)\n        \n        tmp, act_final_cumul_state = \\\n        tf.nn.dynamic_rnn(cell=self._InferenceCell, \n                          inputs=self._inputs, \n                          initial_state=self._initial_state, \n                          time_major=True)\n        act_output, (act_final_iteration, act_final_remainder) = tmp\n        \n        self._pred_outputs = act_output\n        if gather_list is not None:\n            out_sequences = tf.gather(act_output, gather_list)\n        else:\n            out_sequences = act_core\n        \n        pondering_cost = (act_final_iteration + act_final_remainder) * pondering_coefficient\n        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n            labels=self._targets, logits=out_sequences)\n        self._cost = tf.reduce_mean(rnn_cost) + tf.reduce_mean(pondering_cost)\n        \n        self._pred = tf.nn.softmax(out_sequences, dim=2)\n        correct_pred = tf.equal(tf.argmax(self._pred,2), tf.argmax(self._targets,2))\n        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        \n    # \u5f85\u5904\u7406\u51fd\u6570\n    def _get_hidden_state(self, state):\n        controller_state, access_state, read_vectors = state\n        layer_1, layer_2, layer_3 = controller_state\n        L1_next_state, L1_next_cell = layer_1\n        L2_next_state, L2_next_cell = layer_2\n        L3_next_state, L3_next_cell = layer_3\n        return tf.concat([L1_next_state, L2_next_state, L3_next_state], axis=-1)\n        \n    def fit(self, \n            training_iters =1e2,             \n            learning_rate = 1e-4,\n            optimizer_epsilon = 1e-10,\n            max_gard_norm = 50):\n\n        # Set up optimizer with global norm clipping.\n        trainable_variables = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(\n            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n        global_step = tf.get_variable(\n            name=\"global_step\",\n            shape=[],\n            dtype=tf.int64,\n            initializer=tf.zeros_initializer(),\n            trainable=False,\n            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n        \n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n        self._train_step = optimizer.apply_gradients(\n            zip(grads, trainable_variables), global_step=global_step)  \n        \n        self._sess.run(tf.global_variables_initializer())\n        for scope in range(np.int(training_iters)):\n            _, loss, acc = self._sess.run([self._train_step, self._cost, self._accuracy], \n                                     feed_dict = {self._inputs:self._tmp_inputs, \n                                                  self._targets:self._tmp_targets})\n            print (scope, '  loss--', loss, '  acc--', acc)\n        print (\"Optimization Finished!\") \n            \n            \n    def close(self):\n        self._sess.close()\n        print ('\u7ed3\u675f\u8fdb\u7a0b\uff0c\u6e05\u7406tensorflow\u5185\u5b58/\u663e\u5b58\u5360\u7528')\n        \n        \n    def pred(self, inputs, gather_list=None):\n        \n        output_pred = self._pred_outputs\n        if gather_list is not None:\n            output_pred = tf.gather(output_pred, gather_list)\n        probability = tf.nn.softmax(output_pred)\n        classification = tf.argmax(probability, axis=-1)\n        \n        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "732DC34DF1D44FA883F4CF03C7664CAC", "metadata": {}, "source": "# DNCore \u6587\u4ef6\u4ee3\u7801"}, {"cell_type": "code", "collapsed": false, "folded": true, "id": "37421A3052C74FD28968D995C34B8A95", "input": "#import collections\nimport tensorflow as tf\nimport numpy as np\n\nfrom sonnet.python.modules.base import AbstractModule\nfrom sonnet.python.modules.basic import BatchApply, Linear, BatchFlatten\nfrom sonnet.python.modules.rnn_core import RNNCore\nfrom sonnet.python.modules.gated_rnn import LSTM\nfrom sonnet.python.modules.basic_rnn import DeepRNN\n\n\n# Content-based addressing\nclass calculate_Content_based_addressing(AbstractModule):\n    \"\"\"\n    Calculates the cosine similarity between a query and each word in memory, then\n    applies a weighted softmax to return a sharp distribution.\n    \"\"\"\n    \n    def __init__(self,\n                num_heads,\n                word_size,\n                epsilon = 1e-6,\n                name='content_based_addressing'):\n        \n        \"\"\"\n        Initializes the module.\n\n        Args:\n          num_heads: number of memory write heads or read heads.\n          word_size: memory word size.\n          name: module name (default 'content_based_addressing')\n        \"\"\"\n        super().__init__(name=name) # \u8c03\u7528\u7236\u7c7b\u521d\u59cb\u5316\n        self._num_heads = num_heads\n        self._word_size = word_size\n        self._epsilon = epsilon\n        \n        \n    def _Clip_L2_norm(self, tensor, axis=2):\n        \"\"\"\n        \u8ba1\u7b97L2\u8303\u6570\uff0c\u4e3a\u4f59\u5f26\u76f8\u4f3c\u6027\u8ba1\u7b97\u516c\u5f0f\u5206\u6bcd\uff0c\u8fd9\u91cc\u8fdb\u884c\u6570\u503c\u5e73\u7a33\u5316\u5904\u7406\n        \"\"\"\n        quadratic_sum = tf.reduce_sum(tf.multiply(tensor, tensor), axis=axis, keep_dims=True)\n        #return tf.max(tf.sqrt(quadratic_sum + self._epsilon), self._epsilon)           \n        return tf.sqrt(quadratic_sum + self._epsilon)\n    \n    \n    def _Calculate_cosine_similarity(self, keys, memory):\n        \n        \"\"\"\n        Args:      \n            memory: A 3-D tensor of shape [batch_size, memory_size, word_size]\n            keys: A 3-D tensor of shape [batch_size, num_heads, word_size]  \n        Returns:\n            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n        \"\"\"\n    \n        matmul = tf.matmul(keys, memory, adjoint_b=True)\n        memory_norm = self._Clip_L2_norm(memory, axis=2)\n        keys_norm = self._Clip_L2_norm(keys, axis=2)\n        cosine_similarity = matmul / (tf.matmul(keys_norm, memory_norm, adjoint_b=True) + self._epsilon)\n        return cosine_similarity\n    \n    \n    def _build(self, memory, keys, strengths):\n        \"\"\"\n        Connects the CosineWeights module into the graph.\n\n        Args:\n            memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n            keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n            strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n\n        Returns:\n            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n            content_weighting: Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n        \"\"\"\n        cosine_similarity = self._Calculate_cosine_similarity(keys=keys, memory=memory)\n        transformed_strengths = tf.expand_dims(strengths, axis=-1)\n        sharp_activations = cosine_similarity * transformed_strengths\n        \n        softmax = BatchApply(module_or_op=tf.nn.softmax)\n        return softmax(sharp_activations)\n    \n#Dynamic_memory_allocation\nclass update_Dynamic_memory_allocation(RNNCore):\n    \"\"\"\n    Memory usage that is increased by writing and decreased by reading.\n\n    This module is a pseudo-RNNCore whose state is a tensor with values in\n    the range [0, 1] indicating the usage of each of `memory_size` memory slots.\n\n    The usage is:\n\n    *   Increased by writing, where usage is increased towards 1 at the write\n      addresses.\n    *   Decreased by reading, where usage is decreased after reading from a\n      location when free_gates is close to 1.\n    \"\"\"  \n    \n    def __init__(self, \n                 memory_size, \n                 epsilon = 1e-6,\n                 name='dynamic_memory_allocation'):\n        \n        \"\"\"Creates a module for dynamic memory allocation.\n\n        Args:\n          memory_size: Number of memory slots.\n          name: Name of the module.\n        \"\"\"\n        super().__init__(name=name)\n        self._memory_size = memory_size\n        self._epsilon = epsilon\n        \n        \n    def _build(self, \n               prev_usage, \n               prev_write_weightings,\n               free_gates, \n               prev_read_weightings,\n               write_gates, \n               num_writes):\n        \n        usage = self._update_usage_vector(prev_usage, \n                                          prev_write_weightings, \n                                          free_gates, \n                                          prev_read_weightings)\n        \n        allocation_weightings = \\\n        self._update_allocation_weightings(usage, \n                                           write_gates, \n                                           num_writes)\n        \n        return usage, allocation_weightings\n        \n\n    def _update_usage_vector(self, \n                             prev_usage, \n                             prev_write_weightings, \n                             free_gates, \n                             prev_read_weightings):\n        \"\"\"\n        The usage is:\n\n        *   Increased by writing, where usage is increased towards 1 at the write\n          addresses.\n        *   Decreased by reading, where usage is decreased after reading from a\n          location when free_gates is close to 1.\n        \n        Args:\n            prev_usage: tensor of shape `[batch_size, memory_size]` giving\n            usage u_{t - 1} at the previous time step, with entries in range [0, 1].\n        \n            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]` \n            giving write weights at previous time step.\n            \n            free_gates: tensor of shape `[batch_size, num_reads]` which indicates\n            which read heads read memory that can now be freed.\n          \n            prev_read_weightings: tensor of shape `[batch_size, num_reads, memory_size]` \n            giving read weights at previous time step.\n          \n        Returns:\n            usage: tensor of shape `[batch_size, memory_size]` representing updated memory usage.\n        \"\"\"\n        prev_write_weightings = tf.stop_gradient(prev_write_weightings)\n        usage = self._Calculate_usage_vector(prev_usage, prev_write_weightings)\n        retention = self._Calculate_retention_vector(free_gates, prev_read_weightings)\n        return usage * retention\n        \n        \n    def _Calculate_usage_vector(self, prev_usage, prev_write_weightings):\n        \"\"\"\n        \u6ce8\u610f\u8fd9\u91ccusage\u66f4\u65b0\u4f7f\u7528\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6570\u636e\u66f4\u65b0\n        \u8fd9\u4e2a\u51fd\u6570\u662f\u7279\u522b\u6dfb\u52a0\u5904\u7406\u591a\u4e2a\u5199\u5934\u7684,\n        \u8fd9\u4e2a\u51fd\u6570\u8ba1\u7b97\u5728\u5199\u5934\u64cd\u4f5c\u4e4b\u540e\u8bb0\u5fc6\u77e9\u9635\u7684\u4f7f\u7528\u60c5\u51b5usage\n        \n        Calcualtes the new usage after writing to memory.\n\n        Args:\n          prev_usage: tensor of shape `[batch_size, memory_size]`.\n          write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n\n        Returns:\n          New usage, a tensor of shape `[batch_size, memory_size]`.\n        \"\"\"\n        with tf.name_scope('usage_after_write'):\n            # Calculate the aggregated effect of all write heads\n            fit_prev_write_weightings = 1 - \\\n            tf.reduce_prod(1 - prev_write_weightings, axis=[1])\n            \n            usage_without_free = \\\n            prev_usage + fit_prev_write_weightings - \\\n            prev_usage * fit_prev_write_weightings\n            \n            return usage_without_free\n\n        \n    def _Calculate_retention_vector(self, free_gates, prev_read_weightings):\n        \"\"\"\n        The memory retention vector phi_t represents by how much each location \n        will not be freed by the gates.\n        \n        Args:\n            free_gates: tensor of shape `[batch_size, num_reads]` with entries in the\n            range [0, 1] indicating the amount that locations read from can be\n            freed.\n            \n            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n        Returns:\n            retention vector: [batch_size, memory_size]\n        \"\"\"\n        with tf.name_scope('usage_after_read'):\n            free_gates = tf.expand_dims(free_gates, axis=-1)\n            \n            retention_vector = tf.reduce_prod(\n                1 - free_gates * prev_read_weightings, \n                axis=[1], name='retention')\n            \n            return retention_vector     \n        \n        \n    def _update_allocation_weightings(self, usage, write_gates, num_writes):\n        \"\"\"\n        Calculates freeness-based locations for writing to.\n\n        This finds unused memory by ranking the memory locations by usage, for each\n        write head. (For more than one write head, we use a \"simulated new usage\"\n        which takes into account the fact that the previous write head will increase\n        the usage in that area of the memory.)\n\n        Args:\n            usage: A tensor of shape `[batch_size, memory_size]` representing\n            current memory usage.\n\n            write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n            the range [0, 1] indicating how much each write head does writing\n            based on the address returned here (and hence how much usage\n            increases).\n\n            num_writes: The number of write heads to calculate write weights for.\n\n        Returns:\n            tensor of shape `[batch_size, num_writes, memory_size]` containing the\n            freeness-based write locations. Note that this isn't scaled by `write_gate`; \n            this scaling must be applied externally.\n        \"\"\"\n        with tf.name_scope('update_allocation'):\n            write_gates = tf.expand_dims(write_gates, axis=-1)\n            allocation_weightings = []\n            for i in range(num_writes):\n                allocation_weightings.append(\n                    self._Calculate_allocation_weighting(usage))\n                # update usage to take into account writing to this new allocation\n                usage += ((1-usage) * write_gates[:,i,:] * allocation_weightings[i])\n            return tf.stack(allocation_weightings, axis=1)\n        \n\n    def _Calculate_allocation_weighting(self, usage):\n        \n        \"\"\"\n        Computes allocation by sorting `usage`.\n\n        This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n\n        Args:\n              usage: tensor of shape `[batch_size, memory_size]` indicating current\n              memory usage. This is equal to u_t in the paper when we only have one\n              write head, but for multiple write heads, one should update the usage\n              while iterating through the write heads to take into account the\n              allocation returned by this function.\n\n        Returns:\n          Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n        \"\"\"\n        with tf.name_scope('allocation'):\n            # Ensure values are not too small prior to cumprod.\n            usage = self._epsilon + (1 - self._epsilon) * usage\n            non_usage = 1 - usage\n            \n            sorted_non_usage, indices = tf.nn.top_k(\n            non_usage, k = self._memory_size, name='sort')\n            \n            sorted_usage = 1 - sorted_non_usage\n            prod_sorted_usage = tf.cumprod(sorted_usage, axis=1, exclusive=True)\n            \n            sorted_allocation_weighting = sorted_non_usage * prod_sorted_usage\n            \n            # This final line \"unsorts\" sorted_allocation, so that the indexing\n            # corresponds to the original indexing of `usage`.\n            inverse_indices = self._batch_invert_permutation(indices)\n            allocation_weighting = self._batch_gather(\n                sorted_allocation_weighting, inverse_indices)\n            \n            return allocation_weighting\n            \n\n    def _batch_invert_permutation(self, permutations):\n        \n        \"\"\"\n        Returns batched `tf.invert_permutation` for every row in `permutations`.\n        \"\"\"\n        \n        with tf.name_scope('batch_invert_permutation', values=[permutations]):\n            unpacked = tf.unstack(permutations, axis=0)\n            \n            inverses = [tf.invert_permutation(permutation) for permutation in unpacked]\n            return tf.stack(inverses, axis=0)\n        \n              \n    def _batch_gather(self, values, indices):\n        \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n        \n        with tf.name_scope('batch_gather', values=[values, indices]):\n            unpacked = zip(tf.unstack(values), tf.unstack(indices))\n            result = [tf.gather(value, index) for value, index in unpacked]\n            return tf.stack(result)   \n        \n    @property\n    def state_size(self):\n        pass\n    \n    @property\n    def output_size(self):\n        pass\n    \n#Temporal_memory_linkage\nclass update_Temporal_memory_linkage(RNNCore):\n    \"\"\"\n    Keeps track of write order for forward and backward addressing.\n\n    This is a pseudo-RNNCore module, whose state is a pair `(link,\n    precedence_weights)`, where `link` is a (collection of) graphs for (possibly\n    multiple) write heads (represented by a tensor with values in the range\n    [0, 1]), and `precedence_weights` records the \"previous write locations\" used\n    to build the link graphs.\n\n    The function `directional_read_weights` computes addresses following the\n    forward and backward directions in the link graphs.\n    \"\"\" \n    def __init__(self, \n                 memory_size, \n                 num_writes, \n                 name='temporal_memory_linkage'):\n        \n        \"\"\"\n        Construct a TemporalLinkage module.\n\n        Args:\n          memory_size: The number of memory slots.\n          num_writes: The number of write heads.\n          name: Name of the module.\n        \"\"\"  \n        super().__init__(name=name)\n        self._memory_size = memory_size\n        self._num_writes = num_writes\n        \n        \n    def _build(self, \n               prev_link, \n               prev_precedence_weightings,\n               prev_read_weightings,\n               write_weightings):\n        \"\"\"\n        Calculate the updated linkage state given the write weights.\n        \n        Args:           \n            prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n            representing the previous link graphs for each write head.\n\n            prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n            containing the previous precedence weights.\n\n            write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n            containing the memory addresses of the different write heads.\n        \n        Returns:\n            link:  A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n            precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n\n        \"\"\"\n        link = self._update_link_matrix(\n            prev_link, prev_precedence_weightings, write_weightings)\n        \n        precedence_weightings = \\\n        self._update_precedence_weightings(\n            prev_precedence_weightings, write_weightings)\n        \n        forward_weightings = \\\n        self._Calculate_directional_read_weightings(\n            link, prev_read_weightings, forward=True)\n        \n        backward_weightings = \\\n        self._Calculate_directional_read_weightings(\n            link, prev_read_weightings, forward=False)\n        \n        return link, precedence_weightings, forward_weightings, backward_weightings  \n    \n    \n    def _update_link_matrix(self, \n                            prev_link, \n                            prev_precedence_weightings, \n                            write_weightings):\n        \"\"\"\n        Calculates the new link graphs.\n\n        For each write head, the link is a directed graph (represented by a matrix\n        with entries in range [0, 1]) whose vertices are the memory locations, and\n        an edge indicates temporal ordering of writes.\n\n        Args:\n          prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n          representing the previous link graphs for each write head.\n\n          prev_precedence_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n          which is the previous \"aggregated\" write weights for each write head.\n\n          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n              containing the new locations in memory written to.\n\n        Returns:\n          A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n          containing the new link graphs for each write head.\n        \"\"\"    \n        \n        with tf.name_scope('link'):\n                   \n            write_weightings_i = tf.expand_dims(write_weightings, axis=3)\n            write_weightings_j = tf.expand_dims(write_weightings, axis=2)\n            prev_link_scale = 1 - write_weightings_i - write_weightings_j\n            remove_old_link = prev_link_scale * prev_link\n            \n            prev_precedence_weightings_j = tf.expand_dims(\n                prev_precedence_weightings, axis=2)\n            add_new_link = write_weightings_i * prev_precedence_weightings_j\n            \n            link = remove_old_link + add_new_link\n            \n            #Return the link with the diagonal set to zero, to remove self-looping edges.\n            batch_size = prev_link.get_shape()[0].value\n            mask = tf.zeros(shape=[batch_size, self._num_writes, self._memory_size], \n                            dtype=prev_link.dtype)\n            \n            fit_link = tf.matrix_set_diag(link, diagonal=mask)\n            return fit_link\n        \n        \n    def _update_precedence_weightings(self, \n                                     prev_precedence_weightings, \n                                     write_weightings):\n        \"\"\"\n        Calculates the new precedence weights given the current write weights.\n\n        The precedence weights are the \"aggregated write weights\" for each write\n        head, where write weights with sum close to zero will leave the precedence\n        weights unchanged, but with sum close to one will replace the precedence\n        weights.   \n\n        Args:\n          prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n          containing the previous precedence weights.\n\n          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n          containing the new write weights.\n\n        Returns:\n          A tensor of shape `[batch_size, num_writes, memory_size]` \n          containing the new precedence weights.  \n        \"\"\"\n        with tf.name_scope('precedence_weightings'):\n            sum_writing = tf.reduce_sum(write_weightings, axis=2, keep_dims=True)\n            \n            precedence_weightings = \\\n            (1 - sum_writing) * prev_precedence_weightings + write_weightings\n            \n            return precedence_weightings\n        \n\n    def _Calculate_directional_read_weightings(self,\n                                               link, \n                                               prev_read_weightings, \n                                               forward):\n        \"\"\"\n        Calculates the forward or the backward read weightings.\n\n        For each read head (at a given address), there are `num_writes` link graphs to follow. \n        Thus this function computes a read address for each of the\n        `num_reads * num_writes` pairs of read and write heads.\n\n        Args:\n            link: tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n            representing the link graphs L_t.\n\n            prev_read_weightsing: tensor of shape `[batch_size, num_reads, memory_size]` \n            containing the previous read weights w_{t-1}^r.\n\n            forward: Boolean indicating whether to follow the \"future\" direction in \n            the link graph (True) or the \"past\" direction (False).\n\n        Returns:\n            tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n\n            Note: We calculate the forward and backward directions for each pair of\n            read and write heads; hence we need to tile the read weights and do a\n            sort of \"outer product\" to get this.\n        \"\"\"\n        with tf.name_scope('directional_read_weightings'):\n            # We calculate the forward and backward directions for each pair of\n            # read and write heads; hence we need to tile the read weights and do a\n            # sort of \"outer product\" to get this.\n            expanded_read_weightings = \\\n            tf.stack([prev_read_weightings] * self._num_writes, axis=1)\n            directional_weightings = tf.matmul(expanded_read_weightings, link, adjoint_b=forward)\n            # Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:\n            return tf.transpose(directional_weightings, perm=[0,2,1,3])    \n        \n# MemoryAccess\nclass MemoryAccess(RNNCore):\n    \"\"\"\n    Access module of the Differentiable Neural Computer.\n\n    This memory module supports multiple read and write heads. It makes use of:\n\n    *   `update_Temporal_memory_linkage` to track the temporal \n    ordering of writes in memory for each write head.\n    \n    *   `update_Dynamic_memory_allocation` for keeping track of \n    memory usage, where usage increase when a memory location is \n    written to, and decreases when memory is read from that \n    the controller says can be freed.\n      \n    Write-address selection is done by an interpolation between content-based\n    lookup and using unused memory.\n    \n    Read-address selection is done by an interpolation of content-based lookup\n    and following the link graph in the forward or backwards read direction.\n    \"\"\"\n    \n    def __init__(self, \n                 memory_size = 128, \n                 word_size = 20, \n                 num_reads = 1, \n                 num_writes = 1, \n                 name='memory_access'):\n        \n        \"\"\"\n        Creates a MemoryAccess module.\n\n        Args:\n            memory_size: The number of memory slots (N in the DNC paper).\n            word_size: The width of each memory slot (W in the DNC paper)\n            num_reads: The number of read heads (R in the DNC paper).\n            num_writes: The number of write heads (fixed at 1 in the paper).\n            name: The name of the module.\n        \"\"\"\n        super().__init__(name=name)\n        self._memory_size = memory_size\n        self._word_size = word_size\n        self._num_reads = num_reads\n        self._num_writes = num_writes\n        \n        self._write_content_mod = calculate_Content_based_addressing(\n            num_heads = self._num_writes, \n            word_size = self._word_size, \n            name = 'write_content_based_addressing')\n        \n        self._read_content_mod = calculate_Content_based_addressing(\n            num_heads = self._num_reads, \n            word_size = self._word_size, \n            name = 'read_content_based_addressing')\n        \n        self._temporal_linkage = update_Temporal_memory_linkage(\n            memory_size = self._memory_size, \n            num_writes = self._num_writes)\n        \n        self._dynamic_allocation = update_Dynamic_memory_allocation(\n            memory_size = self._memory_size)\n        \n        \n    def _build(self, interface_vector, prev_state):\n        \"\"\"\n        Connects the MemoryAccess module into the graph.\n\n        Args:\n            inputs: tensor of shape `[batch_size, input_size]`. \n            This is used to control this access module.\n            \n            prev_state: Instance of `AccessState` containing the previous state.\n\n        Returns:\n            A tuple `(output, next_state)`, where `output` is a tensor of shape\n            `[batch_size, num_reads, word_size]`, and `next_state` is the new\n            `AccessState` named tuple at the current time t.\n        \"\"\"\n        tape = self._Calculate_interface_parameters(interface_vector)\n        \n        prev_memory,\\\n        prev_read_weightings,\\\n        prev_write_weightings,\\\n        prev_precedence_weightings,\\\n        prev_link,\\\n        prev_usage = prev_state\n\n        # \u66f4\u65b0\u5199\u5934\n        write_weightings,\\\n        usage = \\\n        self._update_write_weightings(tape, \n                                      prev_memory, \n                                      prev_usage, \n                                      prev_write_weightings, \n                                      prev_read_weightings)\n        \n        # \u66f4\u65b0\u8bb0\u5fc6\n        memory = self._update_memory(prev_memory, \n                                     write_weightings, \n                                     tape['erase_vectors'], \n                                     tape['write_vectors'])\n        \n        # \u66f4\u65b0\u8bfb\u5934\n        read_weightings,\\\n        link,\\\n        precedence_weightings= \\\n        self._update_read_weightings(tape, \n                                     memory, \n                                     write_weightings,\n                                     prev_read_weightings, \n                                     prev_precedence_weightings,\n                                     prev_link)\n\n        read_vectors = tf.matmul(read_weightings, memory)\n        \n        state = (memory,\n                 read_weightings,\n                 write_weightings,\n                 precedence_weightings,\n                 link,\n                 usage)\n        \n        return read_vectors, state\n        \n\n    def _update_write_weightings(self, \n                                  tape, \n                                  prev_memory, \n                                  prev_usage, \n                                  prev_write_weightings, \n                                  prev_read_weightings):       \n        \"\"\"\n        Calculates the memory locations to write to.\n\n        This uses a combination of content-based lookup and finding an unused\n        location in memory, for each write head.\n\n        Args:\n            tape: Collection of inputs to the access module, including controls for\n            how to chose memory writing, such as the content to look-up and the\n            weighting between content-based and allocation-based addressing.\n            \n            memory: A tensor of shape  `[batch_size, memory_size, word_size]`\n            containing the current memory contents.\n            \n            usage: Current memory usage, which is a tensor of shape \n            `[batch_size, memory_size]`, used for allocation-based addressing.\n\n        Returns:\n            tensor of shape `[batch_size, num_writes, memory_size]` \n            indicating where to write to (if anywhere) for each write head.\n        \"\"\"\n        with tf.name_scope('update_write_weightings', \\\n                           values=[tape, prev_memory, prev_usage]):\n            \n            write_content_weightings = \\\n            self._write_content_mod(\n                prev_memory, \n                tape['write_content_keys'], \n                tape['write_content_strengths'])\n            \n            usage, write_allocation_weightings = \\\n            self._dynamic_allocation(\n                prev_usage, \n                prev_write_weightings, \n                tape['free_gates'], \n                prev_read_weightings, \n                tape['write_gates'], \n                self._num_writes)\n            \n            allocation_gates = tf.expand_dims(tape['allocation_gates'], axis=-1)\n            write_gates = tf.expand_dims(tape['write_gates'], axis=-1)\n            \n            write_weightings = write_gates * \\\n            (allocation_gates * write_allocation_weightings + \\\n             (1 - allocation_gates) * write_content_weightings)\n            \n            return write_weightings, usage\n        \n        \n    def _update_memory(self, \n                       prev_memory, \n                       write_weightings, \n                       erase_vectors, \n                       write_vectors):\n        \"\"\"\n        Args:\n            prev_memory: 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n            write_weightings: 3-D tensor `[batch_size, num_writes, memory_size]`.\n            erase_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n            write_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n\n      Returns:\n            memory: 3-D tensor of shape `[batch_size, num_writes, word_size]`.\n        \"\"\"\n        with tf.name_scope('erase_old_memory', \\\n                           values=[prev_memory, \n                                   write_weightings, \n                                   erase_vectors]):\n\n            expand_write_weightings = \\\n            tf.expand_dims(write_weightings, axis=3)\n\n            expand_erase_vectors = \\\n            tf.expand_dims(erase_vectors, axis=2)\n\n            # \u8fd9\u91cc\u6709\u591a\u4e2a\u5199\u5934\uff0c\u9700\u8981\u4f7f\u7528\u7d2f\u6210\u5904\u7406\u591a\u4e2a\u5199\u5934\n            erase_gates = \\\n            expand_write_weightings * expand_erase_vectors\n\n            retention_gate = \\\n            tf.reduce_prod(1 - erase_gates, axis=[1])\n\n            retention_memory = prev_memory * retention_gate\n\n        with tf.name_scope('additive_new_memory', \\\n                           values=[retention_memory, \n                                   write_weightings, \n                                   write_vectors]):\n\n            memory = retention_memory + \\\n            tf.matmul(write_weightings, write_vectors, adjoint_a=True)\n\n            return memory\n        \n    \n    def _Calculate_interface_parameters(self, interface_vector):\n        \"\"\"\n        Interface parameters. \n        Before being used to parameterize the memory interactions, \n        the individual components are then processed with various \n        functions to ensure that they lie in the correct domain.     \n        \"\"\"\n        # read_keys: [batch_size, num_reads, word_size]\n        read_keys = Linear(\n            output_size= self._num_reads * self._word_size,\n            name='read_keys')(interface_vector)\n        read_keys = tf.reshape(\n            read_keys, shape=[-1, self._num_reads, self._word_size])\n\n        # write_keys: [batch_size, num_writes, word_size]\n        write_keys = Linear(\n            output_size= self._num_writes * self._word_size, \n            name= 'write_keys')(interface_vector)\n        write_keys = tf.reshape(\n            write_keys, shape=[-1, self._num_writes, self._word_size])\n\n\n        # read_strengths: [batch_size, num_reads]\n        read_strengths = Linear(\n            output_size= self._num_reads,\n            name= 'read_strengths')(interface_vector)\n        read_strengths = 1 + tf.nn.softplus(read_strengths)\n\n        # write_strengths: [batch_size, num_writes]\n        write_strengths = Linear(\n            output_size= self._num_writes,\n            name='write_strengths')(interface_vector)\n        write_strengths = 1 + tf.nn.softplus(write_strengths)\n\n\n        # earse_vector: [batch_size, num_writes * word_size]\n        erase_vectors = Linear(\n            output_size= self._num_writes * self._word_size,\n            name='erase_vectors')(interface_vector)\n        erase_vectors = tf.reshape(\n            erase_vectors, shape=[-1, self._num_writes, self._word_size])\n        erase_vectors = tf.nn.sigmoid(erase_vectors)\n\n        # write_vectors: [batch_size, num_writes * word_size]\n        write_vectors = Linear(\n            output_size= self._num_writes * self._word_size,\n            name='write_vectors')(interface_vector)\n        write_vectors = tf.reshape(\n            write_vectors, shape=[-1, self._num_writes, self._word_size])\n\n\n        # free_gates: [batch_size, num_reads]\n        free_gates = Linear(\n            output_size= self._num_reads,\n            name='free_gates')(interface_vector)\n        free_gates = tf.nn.sigmoid(free_gates)\n\n        # allocation_gates: [batch_size, num_writes]\n        allocation_gates = Linear(\n            output_size= self._num_writes,\n            name='allocation_gates')(interface_vector)\n        allocation_gates = tf.nn.sigmoid(allocation_gates)\n\n        # write_gates: [batch_size, num_writes]\n        write_gates = Linear(\n            output_size= self._num_writes,\n            name='write_gates')(interface_vector)\n        write_gates = tf.nn.sigmoid(write_gates)\n\n        # read_modes: [batch_size, (1 + 2 * num_writes) * num_reads]\n        num_read_modes = 1 + 2 * self._num_writes\n        read_modes = Linear(\n            output_size= self._num_reads * num_read_modes,\n            name='read_modes')(interface_vector)\n        read_modes = tf.reshape(\n            read_modes, shape=[-1, self._num_reads, num_read_modes])\n        read_modes = BatchApply(tf.nn.softmax)(read_modes)\n\n        tape = {\n            'read_content_keys': read_keys,\n            'read_content_strengths': read_strengths,\n            'write_content_keys': write_keys,\n            'write_content_strengths': write_strengths,\n            'write_vectors': write_vectors,\n            'erase_vectors': erase_vectors,\n            'free_gates': free_gates,\n            'allocation_gates': allocation_gates,\n            'write_gates': write_gates,\n            'read_modes': read_modes,\n        }\n        return tape        \n\n\n    def _update_read_weightings(self, \n                                tape, \n                                memory, \n                                write_weightings,\n                                prev_read_weightings, \n                                prev_precedence_weightings, \n                                prev_link):\n        \"\"\"\n        Calculates read weights for each read head.\n\n        The read weights are a combination of following the link graphs in the\n        forward or backward directions from the previous read position, and doing\n        content-based lookup. The interpolation between these different modes is\n        done by `inputs['read_mode']`.\n\n        Args:\n            inputs: Controls for this access module. \n            This contains the content-based keys to lookup, \n            and the weightings for the different read modes.\n\n            memory: A tensor of shape `[batch_size, memory_size, word_size]`\n            containing the current memory contents to do content-based lookup.\n\n            prev_read_weights: A tensor of shape `[batch_size, num_reads, memory_size]` \n            containing the previous read locations.\n\n            link: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n            containing the temporal write transition graphs.\n\n        Returns:\n            A tensor of shape `[batch_size, num_reads, memory_size]` \n            containing the read weights for each read head.\n        \"\"\"    \n        with tf.name_scope(\n            'update_read_weightings', \n            values=[tape, \n                    memory, \n                    prev_read_weightings, \n                    prev_precedence_weightings, \n                    prev_link]):\n\n            read_content_weightings = \\\n            self._read_content_mod(\n                memory, \n                tape['read_content_keys'], \n                tape['read_content_strengths'])\n\n            \n            link,\\\n            precedence_weightings,\\\n            forward_weightings,\\\n            backward_weightings = \\\n            self._temporal_linkage(\n                prev_link, \n                prev_precedence_weightings, \n                prev_read_weightings,\n                write_weightings)\n            \n            \n            backward_mode = tape['read_modes'][:, :, :self._num_writes]\n            forward_mode = tape['read_modes'][:, :, self._num_writes:2 * self._num_writes]\n            content_mode = tape['read_modes'][:, :, 2 * self._num_writes]\n            \n            backward_ = tf.expand_dims(backward_mode, axis=3) * backward_weightings\n            backward_ = tf.reduce_sum(backward_, axis=2)\n            \n            forward_ = tf.expand_dims(forward_mode, axis=3) * forward_weightings\n            forward_ = tf.reduce_sum(forward_, axis=2)\n            \n            content_ = tf.expand_dims(content_mode, axis=2) * read_content_weightings\n\n            read_weightings = backward_ + forward_ + content_\n\n            return read_weightings, link, precedence_weightings\n        \n    \n    @property\n    def state_size(self):\n        \"\"\"Returns a tuple of the shape of the state tensors.\"\"\"\n        memory = tf.TensorShape([self._memory_size, self._word_size])\n        read_weightings = tf.TensorShape([self._num_reads, self._memory_size])\n        write_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n        link = tf.TensorShape([self._num_writes, self._memory_size, self._memory_size])\n        precedence_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n        usage = tf.TensorShape([self._memory_size])\n        return (memory, \n                read_weightings, \n                write_weightings,\n                precedence_weightings,\n                link, \n                usage)\n    \n    \n    @property\n    def output_size(self):\n        \"\"\"\n        Returns the output shape.\n        \"\"\"\n        return tf.TensorShape([self._num_reads, self._word_size])\n    \n    \nclass DNCore_L1(RNNCore):\n    \"\"\"\n    DNC core cell\n    Args:\n    controller: \u63a7\u5236\u5668\n    \"\"\"\n    \n    def __init__(self, \n                 hidden_size= 128,\n                 memory_size= 256, \n                 word_size= 128, \n                 num_read_heads= 4, \n                 num_write_heads= 1,\n                 name='DNCore'):\n        \n        super().__init__(name=name) # \u8c03\u7528\u7236\u7c7b\u521d\u59cb\u5316\n        with self._enter_variable_scope():\n            self._controller = LSTM(hidden_size)\n            self._access = MemoryAccess(\n                memory_size= memory_size, \n                word_size= word_size, \n                num_reads= num_read_heads, \n                num_writes= num_write_heads)\n            \n        self._dnc_output_size = \\\n        hidden_size + num_read_heads * word_size\n        \n        self._num_read_heads = num_read_heads\n        self._word_size = word_size\n        \n        \n    def _build(self, inputs, prev_tape):\n        \n        prev_controller_state,\\\n        prev_access_state,\\\n        prev_read_vectors = prev_tape\n        \n        batch_flatten = BatchFlatten()\n        controller_input = tf.concat(\n            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n        \n        # \u63a7\u5236\u5668\u5904\u7406\u6570\u636e\n        controller_output, controller_state = \\\n        self._controller(controller_input, prev_controller_state)\n        \n        # \u5916\u5b58\u50a8\u5668\u4ea4\u4e92\n        read_vectors, access_state = \\\n        self._access(controller_output, prev_access_state)\n        \n        # DNC \u8f93\u51fa\n        dnc_output = tf.concat(\n            [controller_output, batch_flatten(read_vectors)], axis= 1)\n        \n        return dnc_output, (controller_state, access_state, read_vectors)\n    \n    \n    def initial_state(self, batch_size, dtype=tf.float32):\n        controller_state= self._controller.initial_state(batch_size, dtype)\n        access_state= self._access.initial_state(batch_size, dtype)\n        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n        return (controller_state, access_state, read_vectors)\n    \n    \n    @property\n    def state_size(self):\n        controller_state= self._controller.state_size\n        access_state= self._access.state_size\n        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n        return (controller_state, access_state, read_vectors)\n    \n    \n    @property\n    def output_size(self):\n        return tf.TensorShape([self._dnc_output_size])\n    \n    \nclass DNCore_L3(RNNCore):\n    \"\"\"\n    DNC core cell\n    Args:\n    controller: \u63a7\u5236\u5668\n    \"\"\"\n    \n    def __init__(self, \n                 hidden_size= 128,\n                 memory_size= 256, \n                 word_size= 128, \n                 num_read_heads= 3, \n                 num_write_heads= 1,\n                 name='DNCore'):\n        \n        super().__init__(name=name) # \u8c03\u7528\u7236\u7c7b\u521d\u59cb\u5316\n        with self._enter_variable_scope():\n            lstm_1 = LSTM(hidden_size)\n            lstm_2 = LSTM(hidden_size)\n            lstm_3 = LSTM(hidden_size)\n            self._controller = DeepRNN([lstm_1, lstm_2, lstm_3]) \n            self._access = MemoryAccess(\n                memory_size= memory_size, \n                word_size= word_size, \n                num_reads= num_read_heads, \n                num_writes= num_write_heads)\n            \n        self._dnc_output_size = \\\n        hidden_size * 3 + num_read_heads * word_size\n        self._num_read_heads = num_read_heads\n        self._word_size = word_size\n        \n        \n    def _build(self, inputs, prev_tape):\n        \n        prev_controller_state,\\\n        prev_access_state,\\\n        prev_read_vectors = prev_tape\n        \n        batch_flatten = BatchFlatten()\n        controller_input = tf.concat(\n            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n        \n        # \u63a7\u5236\u5668\u5904\u7406\u6570\u636e\n        controller_output, controller_state = \\\n        self._controller(controller_input, prev_controller_state)\n        \n        # \u5916\u5b58\u50a8\u5668\u4ea4\u4e92\n        read_vectors, access_state = \\\n        self._access(controller_output, prev_access_state)\n        \n        # DNC \u8f93\u51fa\n        dnc_output = tf.concat(\n            [controller_output, batch_flatten(read_vectors)], axis= 1)        \n        return dnc_output, (controller_state, access_state, read_vectors)\n    \n    \n    def initial_state(self, batch_size, dtype=tf.float32):\n        controller_state= self._controller.initial_state(batch_size, dtype)\n        access_state= self._access.initial_state(batch_size, dtype)\n        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n        return (controller_state, access_state, read_vectors)\n    \n    \n    @property\n    def state_size(self):\n        controller_state= self._controller.state_size\n        access_state= self._access.state_size\n        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n        return (controller_state, access_state, read_vectors)\n    \n    \n    @property\n    def output_size(self):\n        return tf.TensorShape([self._dnc_output_size])", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "markdown", "id": "14C3D931016341BF85B3FC737BB23C5C", "metadata": {}, "source": "## ACT\u8fed\u4ee3\u8ba1\u7b97\u67b6\u6784\u4ee3\u7801\n<br />\n\n#### \u6ce8\u610f\u6211\u7565\u5fae\u4fee\u6539\u4e86sonnet\u91cc\u9762\u7684\u5173\u4e8eACT\u7684\u4ee3\u7801\uff0c\u4f7f\u5f97DNC\u5d4c\u5165ACT\u8ba1\u7b97\u67b6\u6784\u53ef\u4ee5\u6b63\u5e38\u5316\u3002"}, {"cell_type": "code", "collapsed": false, "folded": true, "id": "DD0B1CE9A79E438288E76A09D20FE2F8", "input": "# Copyright 2017 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n\"\"\"Cores for RNNs with varying number of unrolls.\n\nThis file contains implementations for:\n  * ACT (Adaptive Computation Time)\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nfrom sonnet.python.modules import basic\nfrom sonnet.python.modules import rnn_core\nfrom sonnet.python.ops import nest\nimport tensorflow as tf\n\n\ndef _nested_add(nested_a, nested_b):\n  \"\"\"Add two arbitrarily nested `Tensors`.\"\"\"\n  return nest.map(lambda a, b: a + b, nested_a, nested_b)\n\n\ndef _nested_unary_mul(nested_a, p):\n  \"\"\"Multiply `Tensors` in arbitrarily nested `Tensor` `nested_a` with `p`.\"\"\"\n  return nest.map(lambda a: p * a, nested_a)\n\n\ndef _nested_zeros_like(nested_a):\n  return nest.map(tf.zeros_like, nested_a)\n\n\nclass ACTCore(rnn_core.RNNCore):\n  \"\"\"Adaptive computation time core.\n\n  Implementation of the model described in \"Adaptive Computation Time for\n  Recurrent Neural Networks\" paper, https://arxiv.org/abs/1603.08983.\n\n  The `ACTCore` incorporates the pondering RNN of ACT, with different\n  computation times for each element in the mini batch. Each pondering step is\n  performed by the `core` passed to the constructor of `ACTCore`.\n\n  The output of the `ACTCore` is made of `(act_out, (iteration, remainder)`,\n  where\n\n    * `iteration` counts the number of pondering step in each batch element;\n    * `remainder` is the remainder as defined in the ACT paper;\n    * `act_out` is the weighted average output of all pondering steps (see ACT\n    paper for more info).\n  \"\"\"\n\n  def __init__(self, core, output_size, threshold, get_state_for_halting,\n               name=\"act_core\"):\n    \"\"\"Constructor.\n\n    Args:\n      core: A `sonnet.RNNCore` object. This should only take a single `Tensor`\n          in input, and output only a single flat `Tensor`.\n      output_size: An integer. The size of each output in the sequence.\n      threshold: A float between 0 and 1. Probability to reach for ACT to stop\n          pondering.\n      get_state_for_halting: A callable that can take the `core` state and\n          return the input to the halting function.\n      name: A string. The name of this module.\n\n    Raises:\n      ValueError: if `threshold` is not between 0 and 1.\n      ValueError: if `core` has either nested outputs or outputs that are not\n          one dimensional.\n    \"\"\"\n    super(ACTCore, self).__init__(name=name)\n    self._core = core\n    self._output_size = output_size\n    self._threshold = threshold\n    self._get_state_for_halting = get_state_for_halting\n\n    if not isinstance(self._core.output_size, tf.TensorShape):\n      raise ValueError(\"Output of core should be single Tensor.\")\n    if self._core.output_size.ndims != 1:\n      raise ValueError(\"Output of core should be 1D.\")\n\n    if not 0 <= self._threshold <= 1:\n      raise ValueError(\"Threshold should be between 0 and 1, but found {}\".\n                       format(self._threshold))\n\n  def initial_state(self, *args, **kwargs):\n    return self._core.initial_state(*args, **kwargs)\n\n  @property\n  def output_size(self):\n    return tf.TensorShape([self._output_size]), (tf.TensorShape([1]),\n                                                 tf.TensorShape([1]))\n\n  @property\n  def state_size(self):\n    return self._core.state_size\n\n  @property\n  def batch_size(self):\n    self._ensure_is_connected()\n    return self._batch_size\n\n  @property\n  def dtype(self):\n    self._ensure_is_connected()\n    return self._dtype\n\n  def _cond(self, unused_x, unused_cumul_out, unused_prev_state,\n            unused_cumul_state, cumul_halting, unused_iteration,\n            unused_remainder):\n    \"\"\"The `cond` of the `tf.while_loop`.\"\"\"\n    return tf.reduce_any(cumul_halting < 1)\n\n  def _body(self, x, cumul_out, prev_state, cumul_state,\n            cumul_halting, iteration, remainder, halting_linear, x_ones):\n    \"\"\"The `body` of `tf.while_loop`.\"\"\"\n    # Increase iteration count only for those elements that are still running.\n    all_ones = tf.constant(1, shape=(self._batch_size, 1), dtype=self._dtype)\n    is_iteration_over = tf.equal(cumul_halting, all_ones)\n    next_iteration = tf.where(is_iteration_over, iteration, iteration + 1)\n    out, next_state = self._core(x, prev_state)\n    # Get part of state used to compute halting values.\n    halting_input = halting_linear(self._get_state_for_halting(next_state))\n    halting = tf.sigmoid(halting_input, name=\"halting\")\n    next_cumul_halting_raw = cumul_halting + halting\n    over_threshold = next_cumul_halting_raw > self._threshold\n    next_cumul_halting = tf.where(over_threshold, all_ones,\n                                  next_cumul_halting_raw)\n    next_remainder = tf.where(over_threshold, remainder,\n                              1 - next_cumul_halting_raw)\n    p = next_cumul_halting - cumul_halting\n    \n    next_cumul_state = _nested_add(cumul_state,\n                                   _nested_unary_mul(next_state, p))\n    \n    next_cumul_out = cumul_out + p * out\n\n    return (x_ones, next_cumul_out, next_state, next_cumul_state,\n            next_cumul_halting, next_iteration, next_remainder)\n\n  def _build(self, x, prev_state):\n    \"\"\"Connects the core to the graph.\n\n    Args:\n      x: Input `Tensor` of shape `(batch_size, input_size)`.\n      prev_state: Previous state. This could be a `Tensor`, or a tuple of\n          `Tensor`s.\n\n    Returns:\n      The tuple `(output, state)` for this core.\n\n    Raises:\n      ValueError: if the `Tensor` `x` does not have rank 2.\n    \"\"\"\n    x.get_shape().with_rank(2)\n    self._batch_size = x.get_shape().as_list()[0]\n    self._dtype = x.dtype\n\n    x_zeros = tf.concat(\n        [x, tf.zeros(\n            shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n    x_ones = tf.concat(\n        [x, tf.ones(\n            shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n    # Weights for the halting signal\n    halting_linear = basic.Linear(name=\"halting_linear\", output_size=1)\n\n    body = functools.partial(\n        self._body, halting_linear=halting_linear, x_ones=x_ones)\n    cumul_halting_init = tf.zeros(shape=(self._batch_size, 1),\n                                  dtype=self._dtype)\n    iteration_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n    core_output_size = [x.value for x in self._core.output_size]\n    out_init = tf.zeros(shape=(self._batch_size,) + tuple(core_output_size),\n                        dtype=self._dtype)\n    cumul_state_init = _nested_zeros_like(prev_state)\n    remainder_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n    (unused_final_x, final_out, unused_final_state, final_cumul_state,\n     unused_final_halting, final_iteration, final_remainder) = tf.while_loop(\n         self._cond, body, [x_zeros, out_init, prev_state, cumul_state_init,\n                            cumul_halting_init, iteration_init, remainder_init])\n\n    act_output = basic.Linear(\n        name=\"act_output_linear\", output_size=self._output_size)(final_out)\n    \n    # \u4fee\u6539\uff0c\u63a7\u5236\u5668state\u548c\u8bfb\u5411\u91cf\u4f7f\u7528 pondering \u7d2f\u52a0\u6743\u91cd\u7cfb\u6570\u65b9\u5f0f\uff0c\n    # \u8bb0\u5fc6\u77e9\u9635\u4e0d\u4f7f\u7528\uff0c\u8bb0\u5fc6\u77e9\u9635\u4fdd\u6301\u5c55\u5f00\u65f6\u95f4\u8fde\u7eed\u6027\n    controller_state, access_state, read_vectors = final_cumul_state\n    final_cumul_state = (controller_state, unused_final_state[1], read_vectors)\n\n    return (act_output, (final_iteration, final_remainder)), final_cumul_state\n", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}