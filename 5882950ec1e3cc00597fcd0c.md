# 【增强学习】简单的Qlearn选股模型

## 之前说好了要发一篇增强学习的策略，agent初版完成。写了个Qlearn的代码，初步版本完成，未调参，非线性reward判定。纯手撸机器学习的class果然酸爽，就Qlearn这么简单的增强学习机……熬了好几个夜。

* 为了赶在春节前写完（免得春节发出来没人看）所以先把agent写了以后，人为设定了几个坎判断涨跌幅度，以此来返回reward
* 本模型的调参方式有以下三种方式：
	* 1，调alpha值（学习比率），epsilon值（探索率）。这俩值直接在create_agent()这个函数里调就行了，不需要进入类里面去改。
	* 2、调reward判定阈，reward判定阈，也就是对state的判断，这里分为两部分，对大盘的判断和对个股的判断。大盘的判定调整函数show_pan()就可以了。个股的判定阈得进入类里面调make_stock_information（）这个函数……其实这个函数可以放在类外面的，不过写都写完了下次吧……
	* 3、调整reset()函数里的“自信控制”，也就是epsilon随时间和奖励的变化方式。以降低后期因为探索产生的损失。

### 缺陷：
* 1、第一个最明显的缺陷，速度慢。因为对每个股票都要new一个对……不对python不叫new，反正对每个个股都创建了一个对象，脚踏几百条船，学习起来自然很慢……个人机子回测一次半小时，这还只是对于HS300而言。如果股票池子再多点，再加上线性reward函数，中间再织几个神经网络什么的……估计等你算出今天的买buy_list，已经是后天了……土豪可以通过多配置几个内存条，高配GPU运算等方式解决这个问题。
* 2、由于初始的时候依然设定了交易（而此时很明显agent还未从任何信息中学到任何东西）所以基本上是在随机操作，不过这个问题很容易解决，设定探索率衰减，探索提示，当探索时（以及不自信时）不进行交易即可……我已经在代理人类的reset()函数里写了这个的草版，自己调调参即可。
* 3、由于是日线回测，增强学习获得今天的信息已经是昨天的事了，相当于用昨天的收盘信息来判断今天买否。一旦判定跌了，就卖出，判断涨就买入，这就是个问题了。如果昨天恰好最后一天涨，今天恰好最后一天跌怎么办呢？解决方案很多，比如就是提高频率……引入分钟线，秒钟线什么的（高频）……也可以扩大信息量，判断一段时间，而不是一天的信息（屁股频）。
* 4、由于是初版的，很多东西都没写，包括用一些高速的库而不是for循环……不过这不是我要说的最后一个缺陷，最后一个缺陷就是，增强学习策略严格遵从以往的经验，也就是说使用增强学习策略就意味着你认为市场是强有效市场。这时候公开信息包含了所有的信息。但是万一……当前市场是内幕消息市场呢？弱式市场当前的增强学习函数是行不通的。


## 改进方式：
* 1、使用线性reward判定，这个好说，写个复杂点的learns就可以了。还省了人工reward判定的事。
* 2、对reward判定和learns机制放在一起，使用卷积层之类的深度学习方式动态学习，因为简单的线性reward判定很可能存在严重的偏差。
* 3、调整因子。这次只用了价格，交易量，涨跌幅等因子，对于反应股市来说太单薄了。但是因子如果选取太多，又会存在另一个问题。理论上来说，假设选大盘因子3个，每个3种状态，股指因子3个，每个3种状态，那么就是(3*3)^2 == 81种状态，假设平均一个状态需要10天时间确认好，那么81种状态完全确认就是810天，这就快3年了。那么，假设大盘因子10个，每个3种状态，个股因子也是10个，每个3种状态,每个状态需要10天确认呢？呢？就是(3*10)^2*10 == 9000天……24年多，我们已有的回测参数够不够24年都是个问题。不过这个问题可以通过对因子溯源的方法，让神经网络自己在隐藏层里生成因子去……
* 4、将交易与否与探索率绑定起来：假设目前随机到探索，则不进行交易。假设探索率过高（意味着不够自信）那么不进行交易。关于探索率的改变我已经在类里写了reset()函数，包括对于奖励的判定也在里面，调用即可。

就写到这啦~祝大家新年愉快。下一个增强学习的模型，就在这个的基础上加线性reward判定机制和learns的函数吧~


	* 注：第一个回测框架内的是没使用epsilon衰减（自信指数）的，最后一个回测框架内是使用了探索率衰减的。另，由于初期的探索是随机的，所以一个回测并不一定能复现，有可能初期的探索一直在赔钱，后来仅仅能够回本。