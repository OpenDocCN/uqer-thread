{"metadata": {"signature": "sha256:a352716004a9a8f85e27bd7434fe2d1f81dc5f25e6b5aae42009adf9f96fa154"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "9032C521573B4EA6A66CAC79063A015A", "metadata": {}, "source": "# 4.\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5-\u673a\u5668\u5b66\u4e60\u5165\u95e8"}, {"cell_type": "code", "collapsed": false, "id": "729DBA4802B646A684BAFE5F8D0AF9CD", "input": "import numpy as np\n\ndef tanh(x):  \n    return np.tanh(x)\n\ndef tanh_deriv(x):  \n    return 1.0 - np.tanh(x)*np.tanh(x)\n\ndef logistic(x):  \n    return 1/(1 + np.exp(-x))\n\ndef logistic_derivative(x):  \n    return logistic(x)*(1-logistic(x))\n\n\n\nclass NeuralNetwork:\n    def __init__(self, layers, activation='tanh'):  \n        \"\"\"  \n        :param layers: A list containing the number of units in each layer.\n        Should be at least two values  \n        :param activation: The activation function to be used. Can be\n        \"logistic\" or \"tanh\"  \n        \"\"\"  \n        if activation == 'logistic':  \n            self.activation = logistic  \n            self.activation_deriv = logistic_derivative  \n        elif activation == 'tanh':  \n            self.activation = tanh  \n            self.activation_deriv = tanh_deriv\n    \n        self.weights = []  \n        for i in range(1, len(layers) - 1):  \n            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)  \n            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n            \n            \n    def fit(self, X, y, learning_rate=0.2, epochs=10000):         \n        X = np.atleast_2d(X)         \n        temp = np.ones([X.shape[0], X.shape[1]+1])         \n        temp[:, 0:-1] = X  # adding the bias unit to the input layer         \n        X = temp         \n        y = np.array(y)\n    \n        for k in range(epochs):  \n            i = np.random.randint(X.shape[0])  \n            a = [X[i]]\n    \n            for l in range(len(self.weights)):  #going forward network, for each layer\n                a.append(self.activation(np.dot(a[l], self.weights[l])))  #Computer the node value for each layer (O_i) using activation function\n            error = y[i] - a[-1]  #Computer the error at the top layer\n            deltas = [error * self.activation_deriv(a[-1])] #For output layer, Err calculation (delta is updated error)\n            \n            #Staring backprobagation\n            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer \n                #Compute the updated error (i,e, deltas) for each node going from top layer to input layer \n                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))  \n            deltas.reverse()  \n            for i in range(len(self.weights)):  \n                layer = np.atleast_2d(a[i])  \n                delta = np.atleast_2d(deltas[i])  \n                self.weights[i] += learning_rate * layer.T.dot(delta)\n                \n                \n    def predict(self, x):         \n        x = np.array(x)         \n        temp = np.ones(x.shape[0]+1)         \n        temp[0:-1] = x         \n        a = temp         \n        for l in range(0, len(self.weights)):             \n            a = self.activation(np.dot(a, self.weights[l]))         \n        return a\n", "language": "python", "metadata": {}, "outputs": [], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "EC2B4AED1B22486F803A97F6F311397F", "input": "nn = NeuralNetwork([2,2,1], 'tanh')     \nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])     \ny = np.array([0, 1, 1, 0])     \nnn.fit(X, y)     \nfor i in [[0, 0], [0, 1], [1, 0], [1,1]]:    \n    print(i, nn.predict(i))", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "([0, 0], array([ 0.01028705]))"}, {"output_type": "stream", "stream": "stdout", "text": "\n([0, 1], array([ 0.9984179]))\n([1, 0], array([ 0.99841446]))\n([1, 1], array([ 0.01749611]))\n"}], "trusted": true}, {"cell_type": "code", "collapsed": false, "id": "0C4664316C9E47429B76FDF2E0CB6B38", "input": "import numpy as np \nfrom sklearn.datasets import load_digits \nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom sklearn.preprocessing import LabelBinarizer \nfrom sklearn.cross_validation import train_test_split\n\n\ndigits = load_digits()  \nX = digits.data  \ny = digits.target  \nX -= X.min() # normalize the values to bring them into the range 0-1  \nX /= X.max()\n\nnn = NeuralNetwork([64,100,10],'logistic')  \nX_train, X_test, y_train, y_test = train_test_split(X, y)  \nlabels_train = LabelBinarizer().fit_transform(y_train)  \nlabels_test = LabelBinarizer().fit_transform(y_test)\nprint \"start fitting\"\nnn.fit(X_train,labels_train,epochs=3000)  \npredictions = []  \nfor i in range(X_test.shape[0]):  \n    o = nn.predict(X_test[i] )  \n    predictions.append(np.argmax(o))  \nprint confusion_matrix(y_test,predictions)  \nprint classification_report(y_test,predictions)", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "start fitting"}, {"output_type": "stream", "stream": "stdout", "text": "\n[[42  0  0  0  1  0  0  0  0  0]\n [ 0 45  0  0  0  0  1  0  0  1]\n [ 0  1 37  0  0  0  0  0  0  0]\n [ 0  1  0 50  0  0  0  1  1  0]\n [ 0  1  0  0 45  0  0  0  1  0]\n [ 0  1  0  0  0 40  0  0  0  0]\n [ 0  2  0  0  0  0 30  0  0  0]\n [ 0  0  0  0  1  1  0 46  0  0]\n [ 0  9  1  0  0  2  0  1 41  0]\n [ 0  2  0  0  1  0  0  1  0 43]]"}, {"output_type": "stream", "stream": "stdout", "text": "\n             precision    recall  f1-score   support\n\n          0       1.00      0.98      0.99        43\n          1       0.73      0.96      0.83        47\n          2       0.97      0.97      0.97        38\n          3       1.00      0.94      0.97        53\n          4       0.94      0.96      0.95        47\n          5       0.93      0.98      0.95        41\n          6       0.97      0.94      0.95        32\n          7       0.94      0.96      0.95        48\n          8       0.95      0.76      0.85        54\n          9       0.98      0.91      0.95        47\n\navg / total       0.94      0.93      0.93       450\n\n"}], "trusted": true}], "metadata": {}}]}