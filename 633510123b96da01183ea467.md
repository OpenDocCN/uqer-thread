# 机器学习之“深度学习”

1958 年感知机的诞生以及1986 年反向传播算法的出现，为深度学习奠定了基础。

1989 年，卷积神经网络（CNN）首次被提出，共用卷积核的方式很大程度上减少了模型中需要被训练的参数，在图像识别等方面有更好表现。

2000 年，一类非常重要的循环神经网络（RNN），长短期记忆神经网络（LSTM）被提出，在一定程度上缓解了梯度消失和梯度爆炸等问题。

2009 年，深度信念网络（DBN）与深度玻尔兹曼机（DBM）先后被提出，其中 DBM是多个受限玻尔兹曼机（RBM）相连构成的无向图，而 DBN 是在最远离可视层处为 RBM，其余层为贝叶斯信念网络的混合模型。

同年，图神经网络（GNN）出现，通过顶点、边和全局的信息汇聚对属性做变换，但不改变图的结构，用于关系预测、顶点分类等问题。

2012 年，AlexNet 的出现才使得神经网络重新进入人们的视野，并逐渐成为机器学习算法的热点。

2013 年，一种深度生成模型，变分自编码器（VAE）被提出。

2014 年，生成对抗网络（GAN）被提出，通过判别器与生成器对抗学习，在各种生成式任务上发挥出强大的威力。另外，一类循环神经网络 GRU 被提出，解决长期记忆和反向传播中的梯度等问题。

同年，在自然语言处理领域一类重要的模型架构 seq2seq 被提出。

2015 年，ResNet 出现，基于 CNN 的网络架构加入残差连接，很大程度上缓解了由于网络架构的加深而导致的模型性能下降以及难以训练的问题。

2017 年，具有里程碑意义的模型 Transformer 被提出，在自然语言处理方面有着非常好的表现。

2018 年，BERT 被提出，一类双向的基于 Transformer 块的序列预测模型，可利用两侧的信息来预测中间的信息，应用于自然语言处理问题，模型表现非常好。

![图片注释](http://storage-uqer.datayes.com/6245aa787bf0370166768fd0/49c586d6-3fa6-11ed-98b7-0242ac140002)