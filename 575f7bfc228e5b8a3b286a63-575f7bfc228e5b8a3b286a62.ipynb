{"metadata": {"signature": "sha256:f03aeafbf502dbbc4ef80f24a43f4be8694c9bfc36a2c5553af797abba89cfd8"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "18C456E8F2B6441C95158221A8F47CE4", "metadata": {}, "source": "# "}, {"cell_type": "markdown", "id": "F8A2EFC053714CD7869F5252732B87D9", "metadata": {}, "source": "\u505a\u5b66\u4e60\u8bb0\u5f55\u7528\uff0c\u4e0d\u559c\u52a1\u55b7"}, {"cell_type": "markdown", "id": "600AAB4139CF4007BF61D23C68DDFDD9", "metadata": {}, "source": "\n\u5168\u79f0 Monte Carlo Tree Search\uff0c\u662f\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u95ee\u9898\u4e2d\u505a\u51fa\u6700\u4f18\u51b3\u7b56\u7684\u65b9\u6cd5\uff0c\u4e00\u822c\u662f\u5728\u7ec4\u5408\u535a\u5f08\u4e2d\u7684\u884c\u52a8\uff08move\uff09\u89c4\u5212\u5f62\u5f0f\u3002\u5b83\u7ed3\u5408\u4e86\u968f\u673a\u6a21\u62df\u7684\u4e00\u822c\u6027\u548c\u6811\u641c\u7d22\u7684\u51c6\u786e\u6027\u3002\n\nMCTS \u53d7\u5230\u5feb\u901f\u5173\u6ce8\u4e3b\u8981\u662f\u7531\u8ba1\u7b97\u673a\u56f4\u68cb\u7a0b\u5e8f\u7684\u6210\u529f\u4ee5\u53ca\u5176\u6f5c\u5728\u7684\u5728\u4f17\u591a\u96be\u9898\u4e0a\u7684\u5e94\u7528\u6240\u81f4\u3002\u8d85\u8d8a\u535a\u5f08\u6e38\u620f\u672c\u8eab\uff0cMCTS \u7406\u8bba\u4e0a\u53ef\u4ee5\u88ab\u7528\u5728\u4ee5 {\u72b6\u6001 state\uff0c\u884c\u52a8 action} \u5bf9\u5b9a\u4e49\u548c\u7528\u6a21\u62df\u8fdb\u884c\u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u7684\u4efb\u4f55\u9886\u57df\u3002\n\n\u9009\u62e9 Selection\uff1a\u4ece\u6839\u8282\u70b9 R \u5f00\u59cb\uff0c\u9012\u5f52\u9009\u62e9\u6700\u4f18\u7684\u5b50\u8282\u70b9\uff08\u540e\u9762\u4f1a\u89e3\u91ca\uff09\u76f4\u5230\u8fbe\u5230\u53f6\u5b50\u8282\u70b9 L\u3002\n\u6269\u5c55 Expansion\uff1a\u5982\u679c L \u4e0d\u662f\u4e00\u4e2a\u7ec8\u6b62\u8282\u70b9\uff08\u4e5f\u5c31\u662f\uff0c\u4e0d\u4f1a\u5bfc\u81f4\u535a\u5f08\u6e38\u620f\u7ec8\u6b62\uff09\u90a3\u4e48\u5c31\u521b\u5efa\u4e00\u4e2a\u6216\u8005\u66f4\u591a\u7684\u5b57\u5b50\u8282\u70b9\uff0c\u9009\u62e9\u5176\u4e2d\u4e00\u4e2a C\u3002\n\u6a21\u62df Simulation\uff1a\u4ece C \u5f00\u59cb\u8fd0\u884c\u4e00\u4e2a\u6a21\u62df\u7684\u8f93\u51fa\uff0c\u76f4\u5230\u535a\u5f08\u6e38\u620f\u7ed3\u675f\u3002\n\u53cd\u5411\u4f20\u64ad Backpropagation\uff1a\u7528\u6a21\u62df\u7684\u7ed3\u679c\u8f93\u51fa\u66f4\u65b0\u5f53\u524d\u884c\u52a8\u5e8f\u5217\u3002\n"}, {"cell_type": "markdown", "id": "30C34DF3EF4B47818C3A2A786870DACA", "metadata": {}, "source": "\u4ee5\u4e0b\u662f\u662f\u641c\u7d22\u6811\u4ee3\u7801\uff0c\u53ef\u53d6\u540d\u4e3amcts"}, {"cell_type": "code", "collapsed": false, "id": "FC54325EA5024BDB8B8270E4DCAB5EF6", "input": "import numpy as np\n\n\nclass TreeNode(object):\n\t\"\"\"Tree Representation of MCTS that covers Selection, Expansion, Evaluation, and backUp (aka 'update()')\n\t\"\"\"\n\tdef __init__(self, parent, prior_p):\n\t\tself.parent = parent\n\t\tself.nVisits = 0\n\t\tself.Q_value = 0\n\t\tself.u_value = prior_p\n\t\tself.children = {}\n\t\tself.P = prior_p\n\n\tdef expansion(self, actions):\n\t\t\"\"\"Expand subtree - a dictionary with a tuple of (x,y) position as keys, TreeNode object as values\n\n\t\tKeyword arguments:\n\t\tOutput from policy function - a list of tuples of (x, y) position and prior probability\n\n\t\tReturns:\n\t\tNone\n\t\t\"\"\"\n\t\tfor action, prob in actions:\n\t\t\tif action not in self.children:\n\t\t\t\tself.children[action] = TreeNode(self, prob)\n\n\tdef selection(self):\n\t\t\"\"\"Select among subtree to get the position that gives maximum action value Q plus bonus u(P)\n\n\t\tKeyword arguments:\n\t\tNone.\n\n\t\tReturns:\n\t\ta tuple of (action, next_node)\n\t\t\"\"\"\n\t\treturn max(self.children.iteritems(), key=lambda (a, n): n.toValue())\n\n\tdef isLeaf(self):\n\t\t\"\"\"Check if leaf node (i.e. no nodes below this have been expanded)\n\t\t\"\"\"\n\t\treturn self.children == {}\n\n\tdef update(self, leaf_value, c_puct):\n\t\t\"\"\"Update node values from leaf evaluation\n\n\t\tArguments:\n\t\tvalue of traversed subtree evaluation\n\n\t\tReturns:\n\t\tNone\n\t\t\"\"\"\n\t\t# count visit\n\t\tself.nVisits += 1\n\t\t# update Q\n\t\tmean_V = self.Q_value * (self.nVisits - 1)\n\t\tself.Q_value = (mean_V + leaf_value) / self.nVisits\n\t\t# update u (note that u is not normalized to be a distribution)\n\t\tself.u_value = c_puct * self.P * np.sqrt(self.parent.nVisits) / (1 + self.nVisits)\n\n\tdef toValue(self):\n\t\t\"\"\"Return action value Q plus bonus u(P)\n\t\t\"\"\"\n\t\treturn self.Q_value + self.u_value\n\n\nclass MCTS(object):\n\t\"\"\"Monte Carlo tree search, takes an input of game state, value network function, policy network function,\n\trollout policy function. get_move outputs an action after lookahead search is complete.\n\n\tThe value function should take in a state and output a number in [-1, 1]\n\tThe policy and rollout functions should take in a state and output a list of (action,prob) tuples where\n\t\taction is an (x,y) tuple\n\n\tlmbda and c_puct are hyperparameters. 0 <= lmbda <= 1 controls the relative weight of the value network and fast\n\trollouts in determining the value of a leaf node. 0 < c_puct < inf controls how quickly exploration converges\n\tto the maximum-value policy\n\t\"\"\"\n\n\tdef __init__(self, state, value_network, policy_network, rollout_policy, lmbda=0.5, c_puct=5, rollout_limit=500, playout_depth=20, n_search=10000):\n\t\tself.root = TreeNode(None, 1.0)\n\t\tself._value = value_network\n\t\tself._policy = policy_network\n\t\tself._rollout = rollout_policy\n\t\tself._lmbda = lmbda\n\t\tself._c_puct = c_puct\n\t\tself._rollout_limit = rollout_limit\n\t\tself._L = playout_depth\n\t\tself._n_search = n_search\n\n\tdef _DFS(self, nDepth, treenode, state):\n\t\t\"\"\"Monte Carlo tree search over a certain depth per simulation, at the end of simulation,\n\t\tthe action values and visits of counts of traversed treenode are updated.\n\n\t\tKeyword arguments:\n\t\tInitial GameState object\n\t\tInitial TreeNode object\n\t\tSearch Depth\n\n\t\tReturns:\n\t\tNone\n\t\t\"\"\"\n\n\t\tvisited = [None] * nDepth\n\n\t\t# Playout to nDepth moves using the full policy network\n\t\tfor index in xrange(nDepth):\n\t\t\taction_probs = self._policy(state)\n\t\t\t# check for end of game\n\t\t\tif len(action_probs) == 0:\n\t\t\t\tbreak\n\t\t\ttreenode.expansion(action_probs)\n\t\t\taction, treenode = treenode.selection()\n\t\t\tstate.do_move(action)\n\t\t\tvisited[index] = treenode\n\n\t\t# leaf evaluation\n\t\tv = self._value(state)\n\t\tz = self._evaluate_rollout(state, self._rollout_limit)\n\t\tleaf_value = (1 - self._lmbda) * v + self._lmbda * z\n\n\t\t# update value and visit count of nodes in this traversal\n\t\t# Note: it is important that this happens from the root downward\n\t\t# so that 'parent' visit counts are correct\n\t\tfor node in visited:\n\t\t\tnode.update(leaf_value, self._c_puct)\n\n\tdef _evaluate_rollout(self, state, limit):\n\t\t\"\"\"Use the rollout policy to play until the end of the game, get the winner (or 0 if tie)\n\t\t\"\"\"\n\t\tfor i in xrange(limit):\n\t\t\taction_probs = self._rollout(state)\n\t\t\tif len(action_probs) == 0:\n\t\t\t\tbreak\n\t\t\tmax_action = max(action_probs, key=lambda (a, p): p)[0]\n\t\t\tstate.do_move(max_action)\n\t\telse:\n\t\t\t# if no break from the loop\n\t\t\tprint \"WARNING: rollout reached move limit\"\n\t\treturn state.get_winner()\n\n\tdef get_move(self, state):\n\t\t\"\"\"After running simulations for a certain number of times, when the search is complete, an action is selected\n\t\tfrom root state\n\n\t\tKeyword arguments:\n\t\tNumber of Simulations\n\n\t\tReturns:\n\t\taction -- a tuple of (x, y)\n\t\t\"\"\"\n\t\taction_probs = self._policy(state)\n\t\tself.root.expansion(action_probs)\n\n\t\tfor n in xrange(0, self._n_search):\n\t\t\tstate_copy = state.copy()\n\t\t\tself._DFS(self._L, self.root, state_copy)\n\n\t\t# chosen action is the *most visited child*, not the highest-value\n\t\t# (note that they are the same as self._n_search gets large)\n\t\treturn max(self.root.children.iteritems(), key=lambda (a, n): n.nVisits)[0]\n\n\tdef update_with_move(self, last_move):\n\t\t\"\"\"step forward in the tree and discard everything that isn't still reachable\n\t\t\"\"\"\n\t\tif last_move in self.root.children:\n\t\t\tself.root = self.root.children[last_move]\n\t\t\tself.root.parent = None\n\t\t\t# siblings of root will be garbage-collected because they are no longer reachable\n\t\telse:\n\t\t\tself.root = TreeNode(None, 1.0)\n\n\nclass ParallelMCTS(MCTS):\n\tpass\n", "language": "python", "metadata": {}, "outputs": [], "trusted": true}], "metadata": {}}]}