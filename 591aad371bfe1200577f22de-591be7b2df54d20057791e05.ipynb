{"metadata": {"signature": "sha256:60287524bb8fe09400962a30ed046f908d0e08e3d4aaf20c66e2e45b8bc880d2"}, "nbformat": 3, "nbformat_minor": 0, "worksheets": [{"cells": [{"cell_type": "markdown", "id": "D609991E7D4344ACAB663F63722D47B4", "metadata": {}, "source": "# Notebook"}, {"cell_type": "markdown", "id": "B69C59325D0444D2847D63B5AB6BCCE9", "metadata": {}, "source": "# \u673a\u5668\u5b66\u4e60\u7b14\u8bb0\n\n- [\u611f\u77e5\u673a\u539f\u59cb\u4e0e\u5bf9\u5076\u5f62\u5f0f\u5b9e\u73b0](#perceptron)\n- [\u7528\u71b5\u6765\u51b3\u5b9a\u6570\u636e\u5e93\u4f18\u5148\u67e5\u8be2\u6761\u4ef6](#entropy)\n\n\n## Perceptron\n\u4ee3\u7801\u5b9e\u73b0\u674e\u822a\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u4e2d\u7684\u4f8b2.1\uff0c 2.2\n\u6b64\u5904\u611f\u77e5\u673a\u4e3a\u4e24\u7ef4\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u4e00\u4e2a\u7ebf\u6027\u51fd\u6570\u8868\u793a\u4e00\u4e2a\u611f\u77e5\u5668\uff0c\u6240\u4ee5\u4e0e\u6570\u636e\u65e0\u5173\uff0c\u5728\u6784\u9020 Perception \u7c7b\u65f6\u53ea\u9700\u7ebf\u6027\u51fd\u6570\u76f8\u5173\u53c2\u6570\uff0c\u4e0d\u9700\u8981\u6837\u672c\u6570\u636e\uff0cfit Perceptron \u65f6\u4f7f\u7528\u6837\u672c\u6570\u636e"}, {"cell_type": "code", "collapsed": false, "id": "500E02D24C064E7592CDE48D6ABB2CDD", "input": "import numpy as np\nimport logging\nimport pandas as pd\n\ndef sign(x):\n    \"\"\"Return 1 or -1 based on x __ge__(0) or lower than0\"\"\"\n    return 1 if x>=0 else -1\n\ndef gram_matrix(X):\n    \"\"\"Gram matix, each element is two column vectors dot product.\"\"\"\n    return np.dot(X.transpose(),X)\n\n\nclass PerceptronBase(object):\n    \"\"\"Linear classifier(2D) using sign function as activation function.\"\"\"\n\n    def __init__(self,w=(0,0),b=0, learning_rate=1.0):\n        \"\"\"Initialize the linear function w*x+b=0.\n            w   slope vector\n            b   intercept scalar\n            eta constant learning rate, default to 1\n        \"\"\"\n        self.w=np.asarray(w)\n        self.b=b\n        self.eta=learning_rate\n\n    def predict(self,x):\n        \"\"\"Return classification of entry x.\"\"\"\n        x=np.asarray(x)\n        return sign(np.dot(self.w, x)+self.b)\n\n\nclass Perceptron(PerceptronBase):\n    def __init__(self,w=(0,0),b=0, learning_rate=1.0):\n        super(Perceptron,self).__init__(w=w,b=b, learning_rate=learning_rate)\n\n    def next(self, x, lable):\n        \"\"\"Return new  w, b using one entry.\"\"\"\n        logging.debug('------ update %s'% str((self.w, self.b)))\n        x=np.asarray(x)\n        self.w+=self.eta*lable*x\n        self.b+=self.eta*lable\n        return self.w, self.b\n\n    def sign_wrong(self,x, lable):\n        \"\"\"Return True if entry is on the wrong side of the linear function.\"\"\"\n        x=np.asarray(x)\n        return not lable*(np.dot(self.w, x)+self.b)>0\n\n    def fit(self,x,lable, n_iter=30):\n        \"\"\"Correct w, b with labled entry.\n            x       2d data point\n            lable   -1, 1. Two sides of the linear function\n        \"\"\"\n        n=0\n        x1,x2=x\n        correct=0\n        #update w, b until reach upper limit of iterator times or all entry classyfied correctly\n        while n<n_iter and correct==0:\n            correct=1 # default all correct\n            for i in range(len(x1)): # fit perceptron with every entry \n                logging.debug('------ %d fit data %d: %d %d %d'%(n,i,x1[i],x2[i],y[i]))\n                if self.sign_wrong((x1[i],x2[i]),y[i]):\n                    print(self.next((x1[i],x2[i]),y[i])) # x=(3,3) y=1\n                    correct=0 # wrong classified entry exists\n                n+=1\n        logging.info('------ final w, b %s %s' %(self.w,self.b))\n\n\nclass PerceptronDualForm(PerceptronBase):\n    def __init__(self,b=0, learning_rate=1.0):\n        super(PerceptronDualForm,self).__init__(b=b,learning_rate=learning_rate)\n\n    def next(self,a, lable,i):\n        \"\"\"Return new  a, b with one misclassified entry.\n            a   1d array, each element is ith entry accumulated learning_rate\n        \"\"\"\n        logging.debug('------ update a,b for ith data %s'% str((a, self.b, i)))\n        a[i]+=self.eta\n        self.b+=self.eta*lable\n        return a, self.b\n\n    def sign_wrong(self,a, y,i):\n        \"\"\"Return True if ith entry is on the wrong side of the linear function.\"\"\"\n        s=0\n        for j in range(len(a)):\n            s+=a[j]*y[j]*g[j,i]\n        return not y[i]*(s+self.b)>0\n\n    def fit(self,X,y, n_iter=30):\n        \"\"\"Correct a, b with labled entry.\n            X   2d array, one entry as a column vector, N columns for N entries\n            y   -1, 1. Two sides of the linear function\n        \"\"\"\n        n=0\n        dcounts=X.shape[1] # total entries number\n        a= np.zeros(dcounts)\n        correct=0\n        #update w, b until reach upper limit of iterator times or all entry classyfied correctly\n        while n<n_iter and correct==0:\n            correct=1 # default all correct\n            for i in range(dcounts): # fit perceptron with every entry \n                logging.debug('------ %d fit data %d: %d '%(n,i,y[i]))\n                if self.sign_wrong(a,y,i):\n                    print(self.next(a,y[i],i)) # x=(3,3) y=1\n                    correct=0 # wrong classified entry exists\n                n+=1\n        self.w=np.dot(X,a*y) # do not leave y...\n        logging.info('------ final w, b %s %s' %(self.w,self.b))\n\n\nif __name__=='__main__':\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s')\n\n    df= pd.read_csv('perceptron.csv')\n\n    # \u539f\u59cb\u5f62\u5f0f\n    x1=df['x1'] # first atrribute\n    x2=df['x2'] # second atrribute\n    y=df['y']\n    p=Perceptron()\n    p.fit((x1,x2),y)\n    print(p.predict((3,3)), p.predict((1,1)),p.predict((4,3)),p.predict((4,1)))\n\n    # \u5bf9\u5076\u5f62\u5f0f\n    X=np.array((df['x1'], df['x2'])) # X=(x1,x2,x3) x1 column vector\n    g=gram_matrix(X)\n    print(X)\n    print(g)\n    p=PerceptronDualForm()\n    p.fit(X,y)\n    print(p.predict((3,3)), p.predict((1,1)),p.predict((4,3)),p.predict((4,1)))\n", "language": "python", "metadata": {}, "outputs": [{"output_type": "stream", "stream": "stdout", "text": "\u6a21\u5757:logging\u4e0d\u652f\u6301, \u5982\u679c\u60a8\u9700\u8981\u6dfb\u52a0, \u8bf7\u8054\u7cfb\u6211\u4eec."}, {"output_type": "stream", "stream": "stdout", "text": "\n"}], "trusted": true}, {"cell_type": "markdown", "id": "E8F1D145086443968ACFC99F3411E4EF", "metadata": {}, "source": "\n## \u5229\u7528\u71b5\u51b3\u5b9a\u51b3\u7b56\u6811\u4f18\u9009\u8282\u70b9\n\u7528\u4e8e\u51b3\u5b9a\u6570\u636e\u5e93\u4f18\u5148\u67e5\u8be2\u6761\u4ef6\n\n#### Entropy \n\u53c2\u8003 Tom M.Mitchell \u300a\u673a\u5668\u5b66\u4e60\u300b\u4e2d\u7684\u51b3\u7b56\u6811\u4f8b\u5b50\uff0c\u4f4d\u4e8e\u7b2c 3 \u7ae0 P42,\u6216\u76f4\u63a5\u770b\u8bfe\u4ef6\u53c2\u8003\u8d44\u65992\u3002\n\n####\u5b9e\u4f8b\uff1a\n\u6570\u636e\u5e93\u4e2d\u6709 $1400\\cdot8$ \u6761\u6570\u636e, count_date 8 \u4e2a\u503c, site 5 \u4e2a\u503c\uff0c\u54ea\u4e2a\u7b5b\u9009\u6761\u4ef6\u66f4\u597d?\u67e5\u8be2\u7ed3\u679c\u4e3a\u67d0\u4e00\u4e2a site \u67d0\u4e2a count_date \u4e0b\u7684\u6570\u636e\u3002 \n$E(X|count\\_date)=(-\\frac{1400}{1400\\cdot8}\\log_2\\frac{1400}{1400\\cdot8}-\\frac{1400\\cdot7}{1400\\cdot8}\\log_2\\frac{1400\\cdot7}{1400\\cdot8})1=0.543$\n$E(X|site)=(-\\frac{1}{5}\\log_2\\frac{1}{5}-\\frac{4}{5}\\log_2\\frac{4}{5})1=0.722$\nE(X|count\\_date) < E(X|site), count_date \u4fe1\u606f\u589e\u76ca\u6700\u5927\uff0c\u56e0\u6b64\u4f5c\u4e3a\u4f18\u5148\u7b5b\u9009\u6761\u4ef6\n\u7ed3\u8bba\uff1a\u7b5b\u9009\u6761\u4ef6\u4e2d\u53d6\u503c\u6700\u591a(count_date > site)\u7684\u6761\u4ef6\u4f5c\u4e3a\u4f18\u5148\u7b5b\u9009\u6761\u4ef6\n\n\n## \u53c2\u8003\u8d44\u6599\n1. \u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a \n2. [Decision tree learning slides at CMU](http://www.cs.cmu.edu/~ninamf/courses/601sp15/slides/01_DTreesAndOverfitting-1-12-2015.pdf)\n\n"}], "metadata": {}}]}